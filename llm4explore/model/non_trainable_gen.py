"""
Generate ideas with non-trainable generator models.

Supported generator models:
- `zero-shot-prompting`
- `cot-prompting`
- `interpolate-embedding`
- `weighted-interpolate-embedding`
- `plagiarism`
"""

import json
import os
from typing import Any, Dict, List, Tuple

import numpy as np
import torch
import vec2text

from llm4explore.model.base import IdeaGenerator
from llm4explore.model.common import KNNSampler
from llm4explore.utils.api import process_chat_requests


def references_to_dict(
    data_old: List[str],
    indices: List[int],
    dists: List[float],
) -> List[Dict[str, str]]:
    return [
        {
            "reference": data_old[i],
            "distance": round(d, 4),
        }
        for i, d in zip(indices, dists)
    ]


class PlagiarismGenerator(IdeaGenerator):
    """Plagiarism-based idea generator.

    This generator uses a nearest neighbor search to find the most similar idea
    in the training data and returns it as the generated idea.
    """

    def __init__(
        self,
        n_dims: int,
        data_old: List[str],
        low_dim_embeddings_old: np.ndarray,
        sampler_kwargs: Dict[str, Any] = None,
    ):
        super().__init__(n_dims, data_old, low_dim_embeddings_old)
        sampler_kwargs = sampler_kwargs or {}
        self.sampler = KNNSampler(low_dim_embeddings_old, **sampler_kwargs)

    def decode(self, low_dim_embedding: np.ndarray) -> Tuple[str, Any]:
        indices, dists = self.sampler.sample(low_dim_embedding)
        return self.data_old[indices[0]], references_to_dict(
            self.data_old, indices, dists
        )


class EmbeddingBasedGenerator(IdeaGenerator):
    """Embedding-based idea generator.

    This generator uses a (weighted) interpolation of the high-dimensional embeddings
    of the training data to generate new ideas.

    Note: This generator requires the high-dimensional embeddings to be
    generated by text-embedding-ada-002 as it depends on vec2text for decoding.
    """

    def __init__(
        self,
        n_dims: int,
        data_old: List[str],
        low_dim_embeddings_old: np.ndarray,
        high_dim_embeddings_old: np.ndarray,
        weighted: bool = False,
        sampler_kwargs: Dict[str, Any] = None,
        vec2text_kwargs: Dict[str, Any] = None,
    ):
        assert high_dim_embeddings_old.shape[1] == 1536, (
            "High-dimensional embeddings must have 1536 dimensions. "
            "Please use text-embedding-ada-002 to generate the embeddings."
        )
        super().__init__(n_dims, data_old, low_dim_embeddings_old)
        sampler_kwargs = sampler_kwargs or {}
        vec2text_kwargs = vec2text_kwargs or {}
        self.weighted = weighted
        self.high_dim_embeddings_old = high_dim_embeddings_old
        self.sampler = KNNSampler(low_dim_embeddings_old, **sampler_kwargs)
        self.vec2text_kwargs = vec2text_kwargs
        self.vec2text_corrector = vec2text.load_corrector("text-embedding-ada-002")

    def decode(self, low_dim_embedding: np.ndarray) -> Tuple[str, Any]:
        # Sample nearest neighbors and interpolate high-dimensional embeddings.
        indices, dists = self.sampler.sample(low_dim_embedding)
        high_dim_embeddings = self.high_dim_embeddings_old[indices]
        dists = np.array(dists)
        weights = 1 / (dists + 1e-6) if self.weighted else np.ones_like(dists)
        high_dim_embedding = np.average(high_dim_embeddings, axis=0, weights=weights)
        # Generate text from the interpolated high-dimensional embedding using vec2text.
        if high_dim_embedding.ndim == 1:
            high_dim_embedding = high_dim_embedding[None, :]
        return vec2text.invert_embeddings(
            torch.tensor(
                high_dim_embedding,
                dtype=self.vec2text_corrector.model.dtype,
                device=self.vec2text_corrector.model.device,
            ),
            self.vec2text_corrector,
            **self.vec2text_kwargs,
        ), references_to_dict(self.data_old, indices, dists)


class PromptingBasedGenerator(IdeaGenerator):
    """Prompting-based idea generator.

    This generator uses a prompting-based model to generate new ideas based
    on semantically similar ideas in the training data.
    """

    def __init__(
        self,
        model_name: str,
        prompt_type: str,
        n_dims: int,
        data_old: List[str],
        low_dim_embeddings_old: np.ndarray,
        sampler_kwargs: Dict[str, Any] = None,
        api_kwargs: Dict[str, Any] = None,
    ):
        super().__init__(n_dims, data_old, low_dim_embeddings_old)
        self.model_name = model_name
        self.prompt_type = prompt_type
        sampler_kwargs = sampler_kwargs or {}
        api_kwargs = api_kwargs or {}
        self.sampler = KNNSampler(low_dim_embeddings_old, **sampler_kwargs)
        self.api_kwargs = api_kwargs
        prompt_path = os.path.join(
            os.path.dirname(__file__), "prompts", prompt_type + ".json"
        )
        with open(prompt_path, "r") as f:
            self.prompt_messages = json.load(f)

    def get_prompt(self, references: List[str]) -> List[Dict[str, str]]:
        return self.prompt_messages + [
            {"role": "user", "content": "\n".join(references)}
        ]

    def decode(self, low_dim_embedding: np.ndarray) -> Tuple[str, Any]:
        raise NotImplementedError(
            "Prompting-based generator does not support decoding single embeddings."
        )

    def decode_all(self, low_dim_embeddings: np.ndarray) -> List[Tuple[str, Any]]:
        messages = []
        references = []
        for low_dim_embedding in low_dim_embeddings:
            indices, dists = self.sampler.sample(low_dim_embedding)
            ref = [self.data_old[i] for i in indices]
            messages.append(self.get_prompt(ref))
            references.append(ref)
        preds = process_chat_requests(
            self.model_name,
            messages,
            **self.api_kwargs,
        )
        return list(zip(preds, references))
