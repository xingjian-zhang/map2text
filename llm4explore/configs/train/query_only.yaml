model: google/flan-t5-base
tokenizer: google/flan-t5-base
data:
  path: data/massw_llm4explore.tsv
  target_col: key_idea
  time_col: year
  time_train: [1970, 2023]
  time_val: [2022, 2023]
  time_test: [2023, 2024]
  use_sampler: False
  sampler_kwargs: null
  input_kwargs: null
training_args:
  output_dir: ./
  evaluation_strategy: steps
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  predict_with_generate: True
  logging_steps: 500
  save_steps: 10000
  eval_steps: 1000
  warmup_steps: 2000
metrics: [bleu, rouge, meteor, cosine, bleurt, bertscore]