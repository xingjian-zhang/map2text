[
    {
        "target": "Current Graph Neural Networks (GNNs) rely heavily on the homophily assumption and perform poorly when it does not hold true. Although GNNs specifically designed for heterophilic graphs exist, they lack generality and are limited to specific models.",
        "prediction": "Most Graph Neural Networks (GNNs) depend on the homophily assumption and fail to generalize to graphs with heterophily where dissimilar nodes connect. Traditional concepts of homophily or heterophily are global measurements of the whole graph and fail to describe the local connectivity of a node.",
        "queries": [
            -9.219607,
            -17.18536
        ],
        "log": [
            {
                "reference": "Most Graph Neural Networks (GNNs) depend on the homophily assumption and fail to generalize to graphs with heterophily where dissimilar nodes connect. Traditional concepts of homophily or heterophily are global measurements of the whole graph and fail to describe the local connectivity of a node.",
                "distance": 0.0133
            },
            {
                "reference": "Graph neural networks (GNNs) have emerged as a neural architecture to learn vector representations of nodes and graphs. However, GNNs have only been evaluated empirically, not theoretically, and have not been compared to traditional methods such as the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL).",
                "distance": 0.0203
            },
            {
                "reference": "Despite the recent success of Message-passing Graph Neural Networks (MP-GNNs), their strong inductive bias of homophily limits their ability to generalize to heterophilic graphs and leads to the over-smoothing problem. Current works attempt to mitigate this issue by emphasizing the contribution from similar neighbors and reducing those from dissimilar ones when performing aggregation, but these techniques fail to optimally utilize dissimilarities.",
                "distance": 0.0221
            },
            {
                "reference": "Question answering on complex tables is a challenge due to the relationships between tables and columns, which can be modeled as graph. Current Graph Neural Networks (GNNs) ignore the relationships of sibling nodes and use summation as aggregation function to model parent-child nodes. This often sees nodes with less degree, like column nodes in schema graph, obtaining little information.",
                "distance": 0.0293
            },
            {
                "reference": "Existing Graph Neural Networks (GNNs) are largely modeled by the Uniform Message Passing framework, which simplifies homophily and heterophily to the node-level property and ignores attribute differences. Unfortunately, this approach doesn't account for the diverse characteristics of different attributes.",
                "distance": 0.0352
            },
            {
                "reference": "In the fraud graph, fraudsters often interact with a large number of benign entities to hide themselves, thereby forming both homophilic and heterophilic connections. Existing GNN-based fraud detection methods enhance the homophily in fraud graph and use the low-pass filter to retain the commonality of node features among neighbors, disregarding the heterophilic connections.",
                "distance": 0.0353
            },
            {
                "reference": "Graph Neural Networks (GNNs) are key models in many applications, but the majority of these models assume homophily among nodes in a graph. This assumption overlooks the case of heterophily, where most connected nodes belong to different classes.",
                "distance": 0.0379
            },
            {
                "reference": "Graph neural networks (GNNs) and message passing neural networks (MPNNs) have been shown to be effective for subgraph structures, but existing message passing mechanics are not well-designed theoretically, especially when it comes to targeting explicit edge modeling applications like subgraph isomorphism counting and matching.",
                "distance": 0.038
            },
            {
                "reference": "Graph Neural Networks (GNNs) use the same aggregation methods and parameters across all nodes in a graph, enabling them to handle homophily relational data. However, difficulties arise when not all graphs are homophilic and when distributions within the same graph vary significantly. Moreover, existing GNNs integrate node features and structure identically, which limits their expressive power.",
                "distance": 0.0442
            },
            {
                "reference": "Graph Neural Networks (GNNs) like GCN and GPRGNN, while effective in graph machine learning, are vulnerable to homophily changes on test graphs and have limited capacity to generalize to graphs with different homophily levels. Existing methods to improve GNN robustness, which learn new graph structures or calculate edge attentions, are mostly spatial and involve complex defense mechanisms.",
                "distance": 0.0463
            },
            {
                "reference": "Graph kernels, widely used for graph classification tasks, suffer from limited performance due to hand-crafted combinatorial features of graphs. Graph neural networks (GNNs), which have achieved state-of-the-art performance in downstream graph-related tasks, have some limitations and can't exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism tests.",
                "distance": 0.0506
            },
            {
                "reference": "The computational superiority of Graph Neural Networks (GNNs) entails a iterative aggregation scheme, which introduces redundant message flows and hinders the propagation of long-length path information and learning of graph similarities.",
                "distance": 0.051
            },
            {
                "reference": "Graph Neural Networks (GNNs) are known for their ability to distinguish the isomorphism class of their inputs. However, the degree of difficulty to accomplish this task in the message-passing model (MPNN), which represents most GNNs in use today, is not well understood.",
                "distance": 0.0535
            },
            {
                "reference": "Friend recommendation services are essential for the growth of online social networks, and Graph Neural Networks (GNNs), such as LightGCN and PPRGo, have shown superior performance due to their ability to encode neighborhood context. However, many GNN variants use a static, pre-defined normalizer which doesn't fully leverage the potential of GNN and leads to scale distortion.",
                "distance": 0.0556
            },
            {
                "reference": "Graph Neural Networks (GNNs) struggle with limited expressive power, long-range interactions, and lack a principled way to model higher-order structures. The recently proposed Message Passing Simplicial Networks attempt to address these issues but are constrained by the rigid combinatorial structure of Simplicial Complexes (SCs).",
                "distance": 0.0567
            },
            {
                "reference": "Graph Neural Networks (GNNs) have been widely used for learning on graph-structured data. They typically rely on message passing mechanisms for propagation, but little effort has been made to discover and analyze the relationships among different propagation mechanisms.",
                "distance": 0.061
            },
            {
                "reference": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). However, GNNs have been found performing unsatisfactorily on a non-trivial set of datasets when compared to NNs, a fact attributed to heterophily.",
                "distance": 0.0619
            },
            {
                "reference": "Most graph neural network (GNN) architectures work by message-passing node vector embeddings over the adjacency matrix and it is assumed that they are able to capture the graph topology efficiently.",
                "distance": 0.063
            },
            {
                "reference": "Graph Neural Networks (GNNs), regardless of their specific structure, should be either invariant (the ordering of the input graph's nodes does not affect the output) or equivariant (a permutation of the input permutes the output). A recent work by Maron et al. (2019) proved the universality of invariant GNNs when higher-order tensorization is allowed inside the network.",
                "distance": 0.0636
            },
            {
                "reference": "The most popular design of Graph Neural Networks (GNNs) is 1-hop message passing, where information is aggregated from 1-hop neighbors. Although 1-hop message passing's expressive power is bounded by the Weisfeiler-Lehman (1-WL) test, researchers extended it to K-hop message passing, which has remained unanalyzed.",
                "distance": 0.0647
            }
        ]
    },
    {
        "target": "Large language models (LLMs) are now widely used, interacting with users with critical information needs across various services like healthcare, education, and customer service. Ensuring the safety of these systems is of prime importance, and understanding their capabilities and limitations is necessary.",
        "prediction": "Large pre-trained language models (LMs) are known for their impressive capabilities in generating long, fluent text. Despite this, there hasn't been substantial analysis on their ability to maintain entity coherence and consistency in narrative generation.",
        "queries": [
            -2.766855,
            14.641213
        ],
        "log": [
            {
                "reference": "Large pre-trained language models (LMs) are known for their impressive capabilities in generating long, fluent text. Despite this, there hasn't been substantial analysis on their ability to maintain entity coherence and consistency in narrative generation.",
                "distance": 0.0241
            },
            {
                "reference": "Language models (LMs) often generate incoherent outputs that refer to events and entity states incompatible with the state of the world described in their inputs, posing a problem in their practical application.",
                "distance": 0.0586
            },
            {
                "reference": "Large language models (LLMs) have shown human-level performance on many natural language tasks, yet it is unexplored how they can better assimilate knowledge from structured data, such as a knowledge graph, versus from text.",
                "distance": 0.0638
            },
            {
                "reference": "Large language models (LLMs) have demonstrated excellent understanding and generation capabilities, largely due to the massive amounts of world knowledge they internalize during pretraining. However, how the model's world knowledge interacts with factual information presented in the context remains under explored.",
                "distance": 0.0639
            },
            {
                "reference": "Pretrained large language models (LLMs) are commonly used in numerous sub-fields of natural language processing (NLP) and are known for their excellent few-shot learning ability with task-specific exemplars. The Chain of Thought (CoT) prompting method has previously been utilized to achieve state-of-the-art performance in difficult system-2 tasks such as arithmetic and symbolic reasoning.",
                "distance": 0.0668
            },
            {
                "reference": "Investigations into how models of event implications predict entity state-changes have shown that Large Language Models (LLMs), despite being exposed to procedural knowledge about object interactions, fail to reason about the world. Moreover, current benchmarking methods often misrepresent the abilities of LLMs due to improper task encodings.",
                "distance": 0.0671
            },
            {
                "reference": "Pre-trained language models have gained prominence in language understanding tasks, especially when they are pre-trained on the same data as the downstream task. However, there is a privacy concern with this approach as it may lead to information leakage from individuals mentioned in the training data.",
                "distance": 0.0761
            }
        ]
    },
    {
        "target": "The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained named entities in low-context situations and noisy scenarios like the presence of spelling mistakes and typos across multiple languages. This poses significant challenges due to the scarcity of contextual information and the high granularity of the entities and the interference of noisy data.",
        "prediction": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
        "queries": [
            -1.633257,
            5.450421
        ],
        "log": [
            {
                "reference": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
                "distance": 0.3655
            },
            {
                "reference": "The task of Japanese Word Sense Disambiguation (WSD) is being addressed by participating systems in SemEval-2.",
                "distance": 0.4256
            }
        ]
    },
    {
        "target": "Link Prediction on Hyper-relational Knowledge Graphs (HKG) is an important task in representation learning. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting their representation.",
        "prediction": "Existing evaluation protocols of knowledge graph completion methods for link prediction rely on training with positive and negative triples and testing by ranking positive triples against negative counterparts. However, these protocols suffer from several shortcomings including imbalances in the number of negative counterparts, anomalies in benchmarking datasets, inappropriate splitting of datasets, and aggregation of ranks into a single metric.",
        "queries": [
            -6.716893,
            -16.411486
        ],
        "log": [
            {
                "reference": "Existing evaluation protocols of knowledge graph completion methods for link prediction rely on training with positive and negative triples and testing by ranking positive triples against negative counterparts. However, these protocols suffer from several shortcomings including imbalances in the number of negative counterparts, anomalies in benchmarking datasets, inappropriate splitting of datasets, and aggregation of ranks into a single metric.",
                "distance": 0.0468
            },
            {
                "reference": "Existing Knowledge Graph Embedding (KGE) models can extrapolate to unseen triples, predicting missing entities or relations. However, most research has focused on designing complex triple modeling functions without extensively studying why these models can extrapolate to unseen data, and what the critical factors are for successful extrapolation.",
                "distance": 0.053
            },
            {
                "reference": "Representation learning of knowledge graphs is a key research topic in machine learning and AI, however, reimplementing Knowledge Graph Embedding (KGE) methods can be laborious, especially for methods originally written in non-python programming languages.",
                "distance": 0.0537
            },
            {
                "reference": "A large number of Knowledge graph embedding (KGE) techniques for multi-relational link prediction have been proposed recently, often with differing model architectures, training strategies, and approaches to hyperparameter optimization.",
                "distance": 0.0584
            },
            {
                "reference": "Knowledge Graph Embeddings (KGEs) map entities and relations from a knowledge graph into a geometric space, showing promising performance on link prediction tasks. However, many KGEs use Euclidean geometry, which limits their ability to preserve complex structures and often leads to incorrect inferences by the models.",
                "distance": 0.0647
            },
            {
                "reference": "Existing bi-encoder architectures for distantly supervised relation extraction, which use text and knowledge graphs (KG), either do not share any information between the text and KG encoders, or, if a KG-to-text attention model is used, only share information in one direction.",
                "distance": 0.0668
            },
            {
                "reference": "While embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, embedding models for uncertain KGs, which typically model the inherent uncertainty of relations facts with a confidence score, represent an unresolved challenge.",
                "distance": 0.0692
            },
            {
                "reference": "Conventional knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors, resulting in large model sizes on real-world graphs with millions of entities and limiting their utility in multi-stage pipelines.",
                "distance": 0.0693
            },
            {
                "reference": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity.",
                "distance": 0.0695
            },
            {
                "reference": "Conventional knowledge graph embedding (KGE) suffers from limited knowledge representation, leading to a degradation in performance particularly for low-resource problems.",
                "distance": 0.0712
            },
            {
                "reference": "Continuous representations of knowledge graph (KG) components, such as entities, types and relations, are widely used for entity mention disambiguation, relation inference, and deep question answering. Existing models that use Gaussian, holographic, and complex embeddings don't directly enforce transitivity inherent in is-instance-of and is-subtype-of relations. A recent proposal, order embedding (OE), has attempted to enforce these transitive relations but has some limitations.",
                "distance": 0.0716
            },
            {
                "reference": "Most Knowledge Graph (KG) embedding models are trained based on negative sampling, which helps to reduce the time complexity of model learning. However, this may fail to deliver stable model performance due to the uncertainty in the sampling procedure.",
                "distance": 0.074
            },
            {
                "reference": "Knowledge graph completion aims to predict missing relations between entities in a knowledge graph. Existing methods primarily rely on entity embedding.",
                "distance": 0.074
            },
            {
                "reference": "Despite neural link predictors' efficiency in identifying missing edges in large scale Knowledge Graphs, their use in answering complex queries, such as those using logical conjunctions, disjunctions, and existential quantifiers, especially while considering missing edges, is unclear.",
                "distance": 0.0742
            },
            {
                "reference": "Existing methods for knowledge graph embedding (KGE) primarily focus on entity-centric world knowledge and structured triple facts, mostly ignoring multi-source and heterogeneous knowledge, like event-centric world knowledge, commonsense knowledge, linguistic knowledge, and unstructured information like text descriptions, node types, and temporal information.",
                "distance": 0.0777
            },
            {
                "reference": "Existing knowledge graph embedding (KGE) based methods represent individual entities and links in knowledge graphs (KGs) as vectors in a low-dimension space. However, they mainly focus on link prediction of individual entities, and neglect that between group entities, which are widely present in real-world KGs.",
                "distance": 0.0827
            },
            {
                "reference": "In KG embeddings, entities and relations are embedded into a single geometric space such as Euclidean, hyperbolic, or hyperspherical space to maintain their geometric structures like chain, hierarchy and ring structures. Yet, the topology of KGs is complex and may contain multiple geometric structures at the same time. Existing methods are insufficient in capturing such complex structures.",
                "distance": 0.0854
            },
            {
                "reference": "Relation prediction on knowledge graphs (KGs) has been deeply studied, but most studies are limited to the transductive setting, which cannot handle emerging entities. The inductive setting, allowing entities in the testing phase to be unseen during training, is closer to real-life scenarios but requires entity-independent relation modeling and discrete logical reasoning for interoperability.",
                "distance": 0.087
            },
            {
                "reference": "Recently, transition-based knowledge graph embedding (KGE) methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector.",
                "distance": 0.0889
            },
            {
                "reference": "Knowledge Graph Embedding (KGE) models are commonly used for the task of link prediction in knowledge graphs. However, they are susceptible to data poisoning attacks due to their inductive abilities captured through relationship patterns like symmetry, inversion, and composition.",
                "distance": 0.0909
            }
        ]
    },
    {
        "target": "Prior work on Classical languages like Ancient Greek and Latin predominantly uses BERT, and there is a need for exploration of more versatile language models suitable for Classical philology tasks.",
        "prediction": "While language model-based pre-trained representations such as BERT significantly improve neural models on a variety of tasks, their effectiveness in handling non-canonical text and performing lexical normalization in a resource-scarce scenario is unclear.",
        "queries": [
            -1.088337,
            15.013219
        ],
        "log": [
            {
                "reference": "While language model-based pre-trained representations such as BERT significantly improve neural models on a variety of tasks, their effectiveness in handling non-canonical text and performing lexical normalization in a resource-scarce scenario is unclear.",
                "distance": 0.0512
            },
            {
                "reference": "BERT pre-trained models are generally used for various domains but fall short when it comes to catering to a specific domain with a new additive vocabulary, especially under constrained resources such as computation and data.",
                "distance": 0.0706
            },
            {
                "reference": "Online search latency is a major bottleneck in deploying large-scale pre-trained language models, such as BERT, in retrieval applications.",
                "distance": 0.0751
            },
            {
                "reference": "Pre-trained models have been successful in many natural language processing tasks. However, in the field of Automated Essay Scoring (AES), pre-trained models like BERT have not proven superior to deep learning models such as LSTM.",
                "distance": 0.0789
            },
            {
                "reference": "Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation.",
                "distance": 0.0818
            },
            {
                "reference": "Current pretraining architectures like BERT, GPT, and T5 perform well in specific areas but not across all tasks of natural language understanding, unconditional generation, and conditional generation.",
                "distance": 0.0897
            },
            {
                "reference": "BERT has been successful in a variety of NLP tasks, with studies showing it can capture lexico-semantic clues from words in the context. However, it is not clear to what extent BERT understands the transitive nature of certain lexical relations like the IS-A relation.",
                "distance": 0.09
            },
            {
                "reference": "The content on the web is continuously evolving with new entities, issues, and ideas emerging and semantics of existing topics shifting. Pre-trained language models like BERT have significantly improved tasks related to content understanding, but the performance of these models tends to deteriorate when directly applied to data from subsequent years.",
                "distance": 0.0942
            },
            {
                "reference": "Domain adaptation or transfer learning using pre-trained language models such as BERT has been an effective approach for many natural language processing tasks.",
                "distance": 0.0977
            },
            {
                "reference": "Very large pre-trained models deliver top performance in various natural language processing (NLP) tasks, but their size can be a hindrance in resource-constrained environments. Compression techniques can reduce model size and inference time with minimal impact on main metrics, but can hide performance drops in underrepresented features or result in encoding biases.",
                "distance": 0.0997
            }
        ]
    },
    {
        "target": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs.",
        "prediction": "Pre-trained Language Models (PLMs) are widely available and used for multi-task fine-tuning across domains. However, they may memorize nontransferable knowledge when applied to tasks from distant domains with different class label sets, leading to negative transfer.",
        "queries": [
            -2.865197,
            15.804994
        ],
        "log": [
            {
                "reference": "Pre-trained Language Models (PLMs) are widely available and used for multi-task fine-tuning across domains. However, they may memorize nontransferable knowledge when applied to tasks from distant domains with different class label sets, leading to negative transfer.",
                "distance": 0.0057
            },
            {
                "reference": "Pre-trained language models (PLMs) have been effectively used in news recommendation, but they often suffer from the domain shift problem between the pre-training corpus and downstream news texts. Additionally, PLMs are computationally heavy, impacting their usability in low-latency online services.",
                "distance": 0.0152
            },
            {
                "reference": "Predominantly, pretrained language models (PLMs) are combined with textual patterns to enhance performance in both the zero and few-shot learning settings. The patterns used either directly resemble the text seen during the PLM's pretraining (for zero-shot tasks) or they are adapted more flexibly to the PLM's unique characteristics (for supervised training tasks).",
                "distance": 0.0189
            },
            {
                "reference": "Latency and efficiency issues are often overlooked in the evaluation of Information Retrieval (IR) models based on Pretrained Language Models (PLMs) across various hardware and software testing scenarios. The SPLADE model, which has achieved state-of-the-art zero-shot performance and competitive results on TREC collections, has efficiency controlled via a regularization factor which has proven not to be efficient enough.",
                "distance": 0.0222
            },
            {
                "reference": "Domain-specific Pre-trained Language Models (PLMs) have been proposed to enhance task performance within specific domains. However, this Domain Adaptive Pre-training (DAPT) tends to forget the previous general knowledge acquired by general PLMs, which leads to the phenomenon of catastrophic forgetting and sub-optimal performance.",
                "distance": 0.0286
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have proven effective in various Natural Language Processing (NLP) tasks, but they require large amounts of computational resources due to their high number of parameters. Model pruning is a common method for compressing large-scale PLMs; however, most current methods are focused on task-specific knowledge and tend to neglect the essential task-agnostic knowledge during pruning, leading to the catastrophic forgetting problem and poor generalization ability.",
                "distance": 0.0331
            },
            {
                "reference": "Pre-trained language models (PLMs) improve the generalization of natural language understanding models, but the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods.",
                "distance": 0.0337
            },
            {
                "reference": "Pre-trained language models (PLMs) have shown impressive performance by self-supervised training on text, yet they lack visual semantics or commonsense related to properties such as sizes, shapes, and colors of commonplace objects. Existing solutions usually depend on explicit images, which are time-consuming to retrieve or generate, and apply augmentation to the whole input text indiscriminately.",
                "distance": 0.0338
            },
            {
                "reference": "Pre-trained language models (PLMs) achieve remarkable performance on many downstream tasks, but may fail in giving reliable estimates of their predictive uncertainty. There's a lack of comprehensive understanding of PLMs' calibration, a topic that has not been thoroughly explored.",
                "distance": 0.0387
            },
            {
                "reference": "Pretrained language models (PLMs) typically use a fixed number of steps for inference, leading to inefficiencies in certain situations and potential overthinking which can hinder performance.",
                "distance": 0.0391
            },
            {
                "reference": "Large-scale Protein Language Models (PLMs) have been successful in various protein prediction tasks and Evoformer, a PLM used in AlphaFold, has remained largely unexplored beyond structure prediction.",
                "distance": 0.0481
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have been effectively used in various Natural Language Processing (NLP) tasks with a denoising autoencoder framework being the most successful one, where the models learn to recover the original text from a noise-corrupted version. However, existing studies mainly focus on injecting noises into the input.",
                "distance": 0.0482
            },
            {
                "reference": "Pre-trained language models (PLMs) show notable prediction performance in various natural language processing (NLP) tasks. However, they often lack reliable indication of the trustworthiness of their predictions, especially in safety-critical applications. Prior studies have only partially addressed this issue without providing a comprehensive analysis on how to optimize a well-calibrated PLM-based prediction pipeline.",
                "distance": 0.0502
            },
            {
                "reference": "Applying pre-trained language models (PLMs) for search ranking often requires more nuances and training signals. There exist discrepancies in the training objectives and model architectures, as well as the knowledge needed in ranking and that learned during pre-training.",
                "distance": 0.0537
            },
            {
                "reference": "Although pre-trained language models (PLMs) are commonly used in NLP and provide generic initialization for finding high-performance minima, little is understood about the connections between the different minima reached under various adaptation configurations.",
                "distance": 0.0551
            },
            {
                "reference": "Pre-trained language models (PLMs) have improved performances of various Chinese natural language processing (NLP) tasks, however, they mostly use a vocabulary provided by Google Chinese BERT which is based on Chinese characters and the masked language model pre-training is based on a single vocabulary, thus limiting its downstream task performances.",
                "distance": 0.0555
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have established state-of-the-art ranking effectiveness for search engines, but the challenges of expensive computations, the discrepancy between ranking-agnostic pre-training objectives and ad-hoc retrieval scenarios, as well as the compatibility of individually fine-tuned ranking models in a cooperative system, prevent their direct application in large-scale web search systems.",
                "distance": 0.0557
            },
            {
                "reference": "Pre-trained language models (PLM) are used to extract high-quality item encodings for content-based collaborative filtering (CCF). However, fine-tuning PLMs in an end-to-end (E2E) manner for CCF is resource-intensive due to its multi-modal nature: optimization involves redundant content encoding for interactions from users.",
                "distance": 0.0557
            },
            {
                "reference": "Pretrained language models (PLMs) have significantly improved text generation, but can produce unfaithful or inappropriate content. On the other hand, template-based systems offer faithfulness, but lack fluency.",
                "distance": 0.0561
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have been widely used in passage re-ranking tasks due to their natural language understanding capabilities. However, these models often suffer from issues such as vocabulary mismatch and lack of domain-specific knowledge.",
                "distance": 0.0574
            }
        ]
    },
    {
        "target": "Attribute mining on e-commerce products typically requires extensive human intervention.",
        "prediction": "Currently, product descriptions do not have structured attribute-value pairs, making tasks like product recommendations, product comparison, and demand forecasting challenging.",
        "queries": [
            -14.904341,
            3.084192
        ],
        "log": [
            {
                "reference": "Currently, product descriptions do not have structured attribute-value pairs, making tasks like product recommendations, product comparison, and demand forecasting challenging.",
                "distance": 0.0089
            },
            {
                "reference": "There is a need for an efficient model that can classify and retrieve e-commerce products from a database, but the creation and use of such a model is challenging due to the need for manual labeling of datasets.",
                "distance": 0.0314
            },
            {
                "reference": "Quality product understanding is crucial in many business applications, but existing datasets may lack sufficient quality or proper taxonomy. The improvement of such understanding could be achieved through product representation learning.",
                "distance": 0.0368
            },
            {
                "reference": "Analyzing commercial pages to infer the products or services offered by a web-based business is important for multiple e-commerce tasks. Challenges in this task arise due to the presence of two types of e-commerce product pages: single-product (SP) and multi-product (MP) pages.",
                "distance": 0.0375
            },
            {
                "reference": "The rise of cross-border e-commerce motivates the demand for intelligent solutions to link local product offerings with global customers. In particular, a challenge exists in connecting product attribute sets in one language to persuasive product descriptions in a different language.",
                "distance": 0.0455
            },
            {
                "reference": "For e-commerce companies, identifying a family of product variants is crucial for customer experiences and brand image. Current approaches, such as using basic classification models, fall short in dealing with unstructured product information and delivering satisfactory results.",
                "distance": 0.0484
            },
            {
                "reference": "Many e-Commerce sellers target cross-country and cross-lingual markets, which can be challenging due to the erroneous or incomplete seller descriptions. A key issue that arises is the alignment of items across multilingual e-commerce catalogues.",
                "distance": 0.0495
            },
            {
                "reference": "With billions of database-generated pages on the Web where consumers can readily add priced product offerings to their virtual shopping cart, several opportunities become possible once we can automatically recognize what exactly is being offered for sale on each page.",
                "distance": 0.0535
            },
            {
                "reference": "The substantial increase in online e-commerce products has made manual annotation of tags, which assist users in finding these products, infeasible.",
                "distance": 0.0676
            },
            {
                "reference": "With eBay\u2019s expansion into international countries and the growing complexity of the site, requests for icon creation have increased exponentially. This created challenges in maintaining consistency amid the abundance of icons and their various functions.",
                "distance": 0.0679
            },
            {
                "reference": "Online shopping platforms like Amazon, eBay, Google, Yahoo! and Walmart organize products into different product taxonomies, making it challenging and time-consuming for sellers to categorize goods for each shopping platform.",
                "distance": 0.072
            },
            {
                "reference": "Knowledge extraction from product profiles is crucial for various applications in e-Commerce, but current models are not flexible and were designed for a single category of product, which is not applicable for complex e-Commerce scenarios with multiple diverse categories.",
                "distance": 0.0795
            },
            {
                "reference": "Retrieving online product information in e-commerce websites is often difficult due to different descriptions for the same product.",
                "distance": 0.0803
            },
            {
                "reference": "Classifying products into categories precisely and efficiently is a significant challenge in e-commerce, due to the influx of new products daily and the dynamic nature of categories that necessitates the need for machine learning models to reduce the time and cost for human editors.",
                "distance": 0.0835
            },
            {
                "reference": "In e-commerce, there are motivations for e-shoppers, sellers and manufacturers to require an automated approach for matching product offers from various online sources referring to the same or a similar real-world product. Current methods for matching identical and similar product offers do not provide adequate information for further calculations and analyses.",
                "distance": 0.0837
            },
            {
                "reference": "In e-commerce, long-tail product pricing presents significant technical issues due to data scarcity, which prevents the use of mainstream methods like deep learning that require abundant data.",
                "distance": 0.0845
            },
            {
                "reference": "In large e-commerce enterprises, sellers enter millions of items daily. While some sellers provide structured descriptions of their items, a majority provides unstructured natural language descriptions.",
                "distance": 0.089
            },
            {
                "reference": "An e-commerce catalog typically consists of millions of product specifications, and the search engine receives millions of offers from thousands of independent merchants that need to be matched to the right products. There are significant challenges in matching unstructured offers to structured product descriptions.",
                "distance": 0.0979
            }
        ]
    },
    {
        "target": "Open-domain dialogue systems in commercial settings face the 'WHAT, WHEN, and HOW' (WWH) problem, where there's a challenge in generating natural and personalized dialogue responses while also frequently reverting back to casual conversation turns.",
        "prediction": "Most Spoken Dialog Systems are based on speech grammars and frame/slot semantics with input utterances usually defined in an ad-hoc manner, lacking the ability to generalize beyond the target application domain or learn from annotated corpora.",
        "queries": [
            -7.029365,
            13.213564
        ],
        "log": [
            {
                "reference": "Most Spoken Dialog Systems are based on speech grammars and frame/slot semantics with input utterances usually defined in an ad-hoc manner, lacking the ability to generalize beyond the target application domain or learn from annotated corpora.",
                "distance": 0.0196
            },
            {
                "reference": "A significant challenge in developing dialog systems is acquiring realistic data for training in specific domains.",
                "distance": 0.0293
            },
            {
                "reference": "Many open-domain dialogue models that are pre-trained with social media comments can generate coherent replies but struggle to produce engaging responses when interacting with real users, possibly due to the lack of annotated human-human conversations and a misalignment with human preference.",
                "distance": 0.0326
            },
            {
                "reference": "Dialog topic tracking, which analyzes and maintains topic transitions in ongoing dialogs, is an area where improvements can be made.",
                "distance": 0.0451
            },
            {
                "reference": "The automatic evaluation of open-domain dialogues remains a challenge and requires human judges to evaluate dialogue quality, a process that is usually expensive and not easily scalable.",
                "distance": 0.0556
            },
            {
                "reference": "Evaluation of open-domain dialogue systems is challenging and recently has been abandoned in some competitions due to unreliability. Current automatic metrics do not provide a good indication of conversation quality.",
                "distance": 0.0577
            },
            {
                "reference": "To establish sophisticated dialogue systems, text planning needs to manage with congruent as well as incongruent interests between interlocutors. Much of the previous work has not focused on dialogues where the interlocutors' interests are not fully aligned.",
                "distance": 0.0589
            },
            {
                "reference": "Designing utterance generation modules that are fast, flexible and general, yet produce high-quality output in particular domains is a challenging problem for spoken dialog systems.",
                "distance": 0.0625
            },
            {
                "reference": "Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data, but there's a lack of comprehensive evaluation benchmarks for such systems, especially in Chinese language.",
                "distance": 0.0691
            },
            {
                "reference": "Automatically extracting social meaning and intention from spoken dialogue is an important task for dialogue systems and social computing, but effective methods for this have yet to be established.",
                "distance": 0.0694
            },
            {
                "reference": "State-of-the-art open-domain dialogue approaches involve training end-to-end deep-learning models to learn various conversational features. While these models have achieved reasonable results, they do not model the cognitive processes that humans use when conversing with each other, which might improve the quality of the dialogue agent's responses.",
                "distance": 0.0717
            },
            {
                "reference": "Traditional training of dialogue systems involves empirical methods which typically require large amounts of data and updates happen after many dialogues.",
                "distance": 0.0816
            },
            {
                "reference": "Current dialogue systems lack the ability to smoothly transition a conversation from a dialogue context toward a target sentence. This presents a challenge for designing dialogue systems that direct a conversation toward specific goals.",
                "distance": 0.0828
            },
            {
                "reference": "Automatic evaluation of dialogue systems for unstructured domains is highly challenging. ADEM was introduced by Lowe et al. in 2017 as an innovative solution, formulating the evaluation as a learning problem and yielding results highly correlated with human judgments, surpassing word-overlap metrics such as BLEU.",
                "distance": 0.0831
            },
            {
                "reference": "Existing multidomain Spoken Dialogue Systems require complex re-training processes for domain selection when new components are added.",
                "distance": 0.0833
            },
            {
                "reference": "Open-domain dialogue models often perform poorly in long-term human-bot conversations, possibly due to their lack of capacity to understand and memorize long-term dialogue history information.",
                "distance": 0.0858
            },
            {
                "reference": "Existing model-based, reference-free metrics for open-domain dialogue evaluation have promising correlations with human judgment. However, they perform either turn-level evaluation or assess only a single dialogue quality dimension.",
                "distance": 0.0871
            },
            {
                "reference": "In expert-consultation dialogues, agents often lack sufficient information to determine whether to accept or reject a proposal. This necessitates the generation of information-sharing subdialogues to form shared beliefs for effective proposal evaluation.",
                "distance": 0.0933
            },
            {
                "reference": "Recent open-domain dialogue models have brought numerous breakthroughs, but they often require a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. It is also difficult for these systems to maintain consistent roles while conversing naturally with humans.",
                "distance": 0.0946
            },
            {
                "reference": "Creating a high-quality multi-domain dialogue system is challenging due to the complicated and entangled dialogue state space among each domain, a factor that seriously impacts dialogue policy quality and the generated response.",
                "distance": 0.0957
            }
        ]
    },
    {
        "target": "Continuity or discontinuity (maintaining or shifting deictic centres across discourse segments) is an important property of discourse relations, but the two are correlated in greatly varying ways.",
        "prediction": "Discourse dependency trees, converted from Rhetorical Structure Theory (RST) trees, are useful for certain downstream Natural Language Processing (NLP) tasks. Improving the accuracy of the upper parts of these RST trees is key to deriving better quality discourse dependency trees.",
        "queries": [
            -3.320584,
            8.605453
        ],
        "log": [
            {
                "reference": "Discourse dependency trees, converted from Rhetorical Structure Theory (RST) trees, are useful for certain downstream Natural Language Processing (NLP) tasks. Improving the accuracy of the upper parts of these RST trees is key to deriving better quality discourse dependency trees.",
                "distance": 0.0063
            },
            {
                "reference": "In discourse relation studies, the continuity hypothesis categorizes discourse relations as continuous or discontinuous (Murray, 1997), suggesting that discontinuous relations might have more unambiguous connectives. However, prior work (Asr and Demberg, 2013) tested this prediction using a markedness measure restricted only to relations with explicit connectives.",
                "distance": 0.0071
            },
            {
                "reference": "Text-level discourse parsing is a challenging task because delineating discourse relations requires nuanced semantic judgments that aren't readily captured using standard features.",
                "distance": 0.0099
            },
            {
                "reference": "The current state of discourse analysis implies the need for more precise tools for understanding rhetorical structures.",
                "distance": 0.019
            },
            {
                "reference": "Discourse signals are often implicit and require interpreters to use their own assumptions and beliefs to draw inferences, leading to multiple valid interpretations. However, current discourse data and frameworks ignore this social aspect and expect single ground truth.",
                "distance": 0.031
            },
            {
                "reference": "Existing theories of discourse structure suggest that the segments comprising the discourse are linked through inferred relations such as causality and temporal contiguity, and the resulting discourse is represented hierarchically.",
                "distance": 0.035
            },
            {
                "reference": "It is difficult to apply the strategy of hierarchically constructing micro (intra-sentence or inter-sentence) discourse structure trees using explicit boundaries to document-level macro (inter-paragraph) discourse parsing due to the lack of explicit boundaries at the higher level.",
                "distance": 0.0402
            },
            {
                "reference": "Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning requiring an annotated corpus of sufficient size and quality. However, the benchmark English corpus for RST parsing, the RST Discourse Treebank (RST-DT), is small due to costly annotation activities resulting in poor performance, especially in relation labeling.",
                "distance": 0.0455
            },
            {
                "reference": "Discourse analysis can benefit from separately modeling intra- and inter-sentential levels, where proper representations for text units of different granularities are needed to capture both the text units' meaning and their relationships to the context.",
                "distance": 0.0493
            },
            {
                "reference": "Discourse segmentation and sentence-level discourse parsing are crucial for various NLP tasks. Despite advances in both tasks, there are still challenges due to the scarcity of labeled data.",
                "distance": 0.0598
            },
            {
                "reference": "The traditional view is that discourse structure carries the entire responsibility of conveying discourse relations, which can be complex and cumbersome.",
                "distance": 0.0631
            },
            {
                "reference": "Several studies suggest advancements in RST discourse parsing over the years, reporting a relative error reduction of 24 to 51% on all metrics. These improvements were attributed to the introduction of distributed representations of discourse units.",
                "distance": 0.0659
            },
            {
                "reference": "Most generation systems communicate information by explicitly uttering it. Rhetorical Structure Theory (RST), frequently used for text planning in natural language generation, only uses one prominent relation between clauses and doesn't support implicit rhetorical relations. In formal systems, the underlying information is very detailed, which for the sake of naturalness and comprehensibility, requires easily inferable parts to be left implicit.",
                "distance": 0.0659
            },
            {
                "reference": "Segmented Discourse Representation Theory (SDRT) is used to explain discourse grammars with complex propositional structure. However, there is a question of whether SDRT captures ambiguities such as in sentence (1).",
                "distance": 0.0707
            },
            {
                "reference": "Rhetorical Structure Theory (RST) has been extensively used for understanding discourse by portraying it as a hierarchically semantic structure.",
                "distance": 0.0708
            },
            {
                "reference": "While it is generally accepted that many discourse markers (DMs) can express discourse relations (DRs) which exist independently, the specific contribution of discourse markers is not clear.",
                "distance": 0.0717
            },
            {
                "reference": "The current state-of-the-art method in relation assignment for text-level discourse parsing has an accuracy of 55.73% and is computationally intense, resulting in its impracticality for real world application.",
                "distance": 0.0821
            },
            {
                "reference": "Most discourse grammars, such as SDRT and RST, are descriptive theories of written discourse which presuppose a coherent structure. This structure is the outcome of a goal directed planning process by the producer.",
                "distance": 0.083
            },
            {
                "reference": "There have been efforts to model discourse structure for assimilating text into a data structure but a comprehensive solution is needed.",
                "distance": 0.086
            },
            {
                "reference": "Discourse markers play a role in argumentative discourse, and their discriminative nature for claims and premises is not conclusively understood.",
                "distance": 0.0888
            }
        ]
    },
    {
        "target": "Learning network embeddings with vector-based node representation is widely used but may incur privacy issues when node attributes are included. Moreover, bottleneck problems arise when communication between classical and quantum devices is involved.",
        "prediction": "Textual network embedding processes leverage rich text information associated with a network to learn low-dimensional vector representations of vertices. Current models exploit the relationships of texts on the same edge to embed the text but ignore the complete connectivity level between any two texts in the graph.",
        "queries": [
            -7.758502,
            -15.352807
        ],
        "log": [
            {
                "reference": "Textual network embedding processes leverage rich text information associated with a network to learn low-dimensional vector representations of vertices. Current models exploit the relationships of texts on the same edge to embed the text but ignore the complete connectivity level between any two texts in the graph.",
                "distance": 0.0223
            },
            {
                "reference": "Network embedding transforms complex network data into a low-dimensional vector space, showing high performance in scenarios like link prediction, node classification, and similarity search. Many methods focus on node representation learning, but there has been little focus on embedding techniques for bipartite attributed networks, which model nodes from two different partitions.",
                "distance": 0.0243
            },
            {
                "reference": "Current Heterogeneous Network Embedding (HNE) models are generally relation-aware or metapath-aware but they either fail to represent non-pairwise relations in a heterogeneous graph, or can only capture local information around the target node.",
                "distance": 0.0292
            },
            {
                "reference": "Network embedding models are widely used to map nodes in a network into continuous vector-space representations, which are then used for tasks like classification and link prediction. Currently, these models integrate all information of each node into a single embedding vector, which limits their ability to represent nodes with multiple, possibly non-correlated, facets or characteristics.",
                "distance": 0.037
            },
            {
                "reference": "Node representation learning for directed graphs is essential for many graph mining tasks. Existing methods learn two embedding vectors for each node - a source vector and a target vector - to represent directed edges. However, these methods learn source and target vectors separately, which can lead to ineffective learning, especially for nodes with very low indegree or outdegree.",
                "distance": 0.038
            },
            {
                "reference": "Network embedding (NE) aims to learn low-dimensional representations for nodes in networks. Previous NE methods often separate content and structure representations of nodes, which requires a post-processing combination step. Existing methods also generally consider only short and fixed neighborhood scopes, which poses issues when dealing with complex networks.",
                "distance": 0.046
            },
            {
                "reference": "Most network embedding algorithms rely on measuring co-occurrences of nodes via random walks and then learning the embeddings using Skip-Gram with Negative Sampling. Despite it being an effective method, it does have critical time-complexity issues and its alternatives, such as GloVe, have not been thoroughly explored for network embedding.",
                "distance": 0.0461
            },
            {
                "reference": "Network embedding has been increasingly utilized for network analytics applications to transform complex networks into low-dimensional vector representations. However, these network embedding results remain difficult to interpret and understand, particularly in terms of understanding the factors that influence the embeddings and how instances are distributed in the embedding space.",
                "distance": 0.0474
            },
            {
                "reference": "Network embedding, which aims to embed a network into a vector space while maintaining its inherent structural properties, is an area of interest. However, existing methods embed nodes as point vectors, making the formation of the edge deterministic and only determined by node positions. This approach does not properly account for the uncertainties typical in the formation and evolution of real-world networks.",
                "distance": 0.053
            },
            {
                "reference": "Network embedding learns the vector representations of nodes. Most real world networks are heterogeneous and evolve over time. However, there have been no network embedding approaches designed for dynamic heterogeneous networks.",
                "distance": 0.06
            },
            {
                "reference": "Many Network Representation Learning (NRL) methods have been proposed to learn vector representations for vertices in a network, and these can generally be categorized into a two-step framework, consisting of proximity matrix construction and dimension reduction.",
                "distance": 0.0615
            },
            {
                "reference": "Learning continuous representations of nodes is used in a variety of applications but existing node embedding algorithms and systems can only process networks with hundreds of thousands to a few millions of nodes, and scaling them to networks with tens or hundreds of millions nodes remains a challenge.",
                "distance": 0.0632
            },
            {
                "reference": "Large-scale network embedding aims to learn a latent representation for each node in an unsupervised manner. Existing methods often use a contrastive objective to train an encoder, which requires careful selection of positive and negative samples. While methods for drawing negative samples are satisfying, the selection of positive examples remains a challenging issue.",
                "distance": 0.0672
            },
            {
                "reference": "Bipartite network embedding (BNE) maps each node in a bipartite graph to compact embedding vectors to facilitate downstream tasks. Existing BNE solutions have challenges in scalability to massive bipartite graphs and often produce low-quality results, as effective BNE should preserve not only the direct connections between nodes but also the multi-hop relationships.",
                "distance": 0.0698
            },
            {
                "reference": "Embedding a web-scale information network into a low-dimensional vector space is useful for tasks like link prediction, classification, and visualization. Past research has attempted to extract such embeddings by adopting methods from words to graphs, without defining a clear graph-related objective. These past works implicitly utilize similarity measures among graph nodes.",
                "distance": 0.0722
            },
            {
                "reference": "Existing network embedding methods like DeepWalk, Node2Vec, and LINE capture node representations by assigning context nodes to the reference nodes and pushing away the negative context nodes in the low-dimensional vector space using a predefined sampling distribution based on node popularity. However, this approach often fails to capture the real informativeness of each node and doesn't reflect the training state.",
                "distance": 0.0754
            },
            {
                "reference": "Network embedding seeks to find low-dimensional vector representations for network nodes while preserving the network structure. These embeddings are typically represented as continuous vectors, which pose challenges in storage and computation costs in large-scale applications.",
                "distance": 0.0767
            },
            {
                "reference": "Network embedding (NE) has become increasingly successful in creating low dimensional node representations for various network analytic tasks. There is, however, a challenge in applying NE to content-rich networks, where each node is associated with substantive content data, due to the difficulty in reconciling complex structural dependencies with the node content.",
                "distance": 0.0769
            },
            {
                "reference": "Textual network embedding methods have been studied to learn low-dimensional representations of text-annotated nodes in a graph. However, prior work mainly focuses on fixed graph structures, which is not quite suitable for real-world dynamic networks.",
                "distance": 0.0797
            },
            {
                "reference": "Network embedding (NE) algorithms, which embed network nodes into a vector space, contain a large number of hyperparameters that must be effectively tuned. Automated machine learning (AutoML) has been applied successfully to other types of data but poses challenges for network data due to its massive scale and complex node relationships.",
                "distance": 0.0799
            }
        ]
    },
    {
        "target": "Graph Contrastive Learning (GCL) has attracted considerable interest for its node representation learning capability. Despite the wide application of GCL techniques, its security vulnerabilities, especially in the presence of malicious backdoor adversaries, have not been extensively studied.",
        "prediction": "Graph contrastive learning, which may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties, has been a focus area in node representation. The recent COLES method incorporates traditional graph embedding and negative sampling into one framework and minimizes the trace difference between the within-class scatter matrix and the total scatter matrix.",
        "queries": [
            26.893036,
            5.029032
        ],
        "log": [
            {
                "reference": "Graph contrastive learning, which may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties, has been a focus area in node representation. The recent COLES method incorporates traditional graph embedding and negative sampling into one framework and minimizes the trace difference between the within-class scatter matrix and the total scatter matrix.",
                "distance": 0.0137
            },
            {
                "reference": "Graph Contrastive Learning (GCL) is a new paradigm for learning graph representations without human annotations, but the success behind GCL remains unclear due to several critical design considerations.",
                "distance": 0.0174
            },
            {
                "reference": "Graph Contrastive Learning (GCL) has been an emerging technique to learn generalizable representations from contrastive views. However, it is limited by factors such as altering the graph structure through data augmentation, which may interfere with the message passing scheme and lose intrinsic graph structural information, and by its conventional use of predefined contrastive views and handpicked parameters, failing to fully utilize the contrastive information.",
                "distance": 0.0182
            },
            {
                "reference": "Graph contrastive learning (GCL) has proven a successful technique for graph representation learning where mutual information between paired graph augmentations is maximized. Existing data augmentation methods in GCL are manual trial-and-error, costly domain knowledge, or cumbersome searching, each limit efficiency and broad applicability.",
                "distance": 0.0202
            },
            {
                "reference": "Graph contrastive learning (GCL) is a self-supervised learning approach to graph representation learning (GRL) that maximizes mutual information for similar instances. However, GCL is inefficient in both time and memory consumption and requires a large number of training epochs to be well-trained on large-scale datasets.",
                "distance": 0.0218
            },
            {
                "reference": "Graph contrastive learning attracts/disperses node representations for similar/dissimilar node pairs under some notion of similarity. It may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. However, current contrastive graph embedding models may fail under disjoint positive and negative distributions, which may naturally emerge during sampling in the contrastive setting.",
                "distance": 0.0245
            },
            {
                "reference": "Despite the attraction that Graph Contrastive Learning (GCL) has received, there are some unclear aspects, such as the information encoded into learned representations and any general graph augmentation rules behind different augmentations.",
                "distance": 0.0275
            },
            {
                "reference": "Graph representation learning is vital for real-world applications, but the effective learning of representations without human labelling remains a challenge. Graph contrastive learning (GCL) addresses this problem, but current GCL methods primarily sample node pairs based on node-level proximity, rarely considering community structures. As a result, two nodes from the same community might be sampled as a negative pair.",
                "distance": 0.0295
            },
            {
                "reference": "Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. However, the effectiveness of GraphCL relies on manual selection of data augmentations per dataset, which limits its general applicability due to the diverse nature of graph data.",
                "distance": 0.0382
            },
            {
                "reference": "Graph contrastive learning (GCL) has led to state-of-the-art results on various downstream tasks. However, the graph augmentation step, a vital part of GCL, has not been thoroughly studied. In practice, the node embedding via graph augmentations is highly biased, somewhat restricting contrastive models' ability to learn discriminative features.",
                "distance": 0.0386
            },
            {
                "reference": "Graph classification has wide applications and in many situations, labels are scarce or hard to get. Unsupervised learning is hence a natural paradigm but its performance often lags behind that of supervised learning. Recently, contrastive learning (CL) in computer vision models is seen to perform comparably to supervised models. Many theoretical and empirical works analyzing visual CL frameworks find that leveraging large datasets and task relevant augmentations is essential for CL framework success. However, graph CL frameworks report high performance while using much smaller data, and employing domain-agnostic graph augmentations (DAGAs) that can also remove essential information.",
                "distance": 0.0413
            },
            {
                "reference": "Contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning, where stochastic augmentation on the input graph is performed to obtain two graph views and maximize the agreement of representations. However, the design of graph augmentation schemes\u2014a crucial element in CL\u2014has often oversimplified by using uniform data augmentation schemes, leading to suboptimal performance.",
                "distance": 0.0416
            },
            {
                "reference": "Graph contrastive learning (GCL), which uses graph augmentations to create different views for training graph neural networks (GNNs), has been successfully applied to graph benchmark datasets. However, existing GCL methods face challenges with real-world data, such as the need for manual graph augmentation and difficulty in handling class-imbalanced distribution.",
                "distance": 0.0434
            },
            {
                "reference": "Graph contrastive learning is the state-of-the-art unsupervised graph representation learning framework. However, its robustness to adversarial attacks is untested because most current graph adversarial attacks are based on supervised models, which heavily rely on labels and are only useful in specific scenarios.",
                "distance": 0.0469
            },
            {
                "reference": "Contrastive Learning (CL) has emerged as an effective technique for unsupervised representation learning. While CL can benefit from hard negatives, the application of existing hard negative mining techniques to Graph Contrastive Learning (GCL) has seen limited success.",
                "distance": 0.0568
            },
            {
                "reference": "Current graph contrastive learning methods rely on the vector inner product based similarity metric and manually constructed positive/negative samples. However, these methods may fail to capture the intrinsic local structures of the graph, and the chosen similarity metric may not fully exploit the local structures to characterize the graph difference effectively.",
                "distance": 0.0592
            },
            {
                "reference": "Unsupervised graph representation learning, particularly graph contrastive learning, has garnered attention for its promising performance in various tasks. However, current methods largely focus on augmenting or perturbing graph structures to generate contrasting pairs, but this can introduce noise and limit the application scope of graph contrastive learning.",
                "distance": 0.0621
            },
            {
                "reference": "Graph contrastive learning has seen significant progress but existing works have not explored non-aligned node-node contrasting for node-level representation learning.",
                "distance": 0.0657
            },
            {
                "reference": "Contrastive learning is an effective unsupervised method in graph representation learning, and has been recently extended from images to graphs. However, most of these works are simply adaptations of models designed for images, which make data augmentation on graphs less intuitive and difficult to provide high-quality contrastive samples.",
                "distance": 0.0697
            },
            {
                "reference": "Supervised contrastive (SupCon) learning has been recently proposed for classification tasks, but it could be difficult to handle datasets with large intra-class variances and high inter-class similarities. This issue is even more challenging when it comes to graph structures.",
                "distance": 0.0742
            }
        ]
    },
    {
        "target": "Current object segmentation requires curated datasets, which is a difficult task. The quality and ease of use of conditional image generation has been enhanced with large-scale pre-trained generative models, but the process still requires segmentation labels.",
        "prediction": "Semantic segmentation is a challenging problem in satellite imagery processing due to the complex environment, and automatic categorization and segmentation of land cover can help in urban planning, environmental engineering or natural landscape monitoring.",
        "queries": [
            14.354095,
            -6.644534
        ],
        "log": [
            {
                "reference": "Semantic segmentation is a challenging problem in satellite imagery processing due to the complex environment, and automatic categorization and segmentation of land cover can help in urban planning, environmental engineering or natural landscape monitoring.",
                "distance": 0.0121
            },
            {
                "reference": "Semantic segmentation has been traditionally formulated as a per-pixel classification task, while instance-level segmentation is handled differently, using mask classification.",
                "distance": 0.0326
            },
            {
                "reference": "The problem of multi-label, supervised image segmentation is considered when an initial labeling of some pixels is given. Existing algorithms mainly focus on inter-label discrimination.",
                "distance": 0.0421
            },
            {
                "reference": "Semantic volume segmentation requires voxel-wise annotated ground-truth data, which takes significant effort to obtain.",
                "distance": 0.0461
            },
            {
                "reference": "Semantic segmentation algorithms for object identification are crucial for applications such as autonomous driving. However, existing algorithms treat each image in isolation, which can be suboptimal since autonomous vehicles often revisit the same locations.",
                "distance": 0.0521
            },
            {
                "reference": "Scene segmentation, which requires labeling every pixel in an image, is a challenging task. Current methods struggle to fully exploit discriminative context and aggregate multi-scale features for optimal segmentation.",
                "distance": 0.066
            },
            {
                "reference": "Image segmentation, a common pre-process for high-level vision tasks, requires expensive mask annotation for supervised training. Existing weakly-supervised and unsupervised methods depend on comparisons of image pairs and have limited applicability.",
                "distance": 0.0728
            },
            {
                "reference": "Semantic segmentation, a task of assigning a label to every pixel in an image, poses challenges due to complex layering, large deformations, non-convex objects, and overlapping instances.",
                "distance": 0.0811
            }
        ]
    },
    {
        "target": "While deep neural networks have seen significant advancements and increased deployment in edge device software, they traditionally require the download of all parameters during software updates, negatively affecting user experience.",
        "prediction": "In AI applications, deep neural networks (DNNs) are increasingly being executed on edge devices, and there is a practical need to update these DNN models on the edge device post-deployment for reasons such as refining the model, experiencing concept drift, or outright change in the learning task. However, updates must often be made under bandwidth constraints.",
        "queries": [
            7.514573,
            -8.857841
        ],
        "log": [
            {
                "reference": "In AI applications, deep neural networks (DNNs) are increasingly being executed on edge devices, and there is a practical need to update these DNN models on the edge device post-deployment for reasons such as refining the model, experiencing concept drift, or outright change in the learning task. However, updates must often be made under bandwidth constraints.",
                "distance": 0.012
            },
            {
                "reference": "Machine learning is becoming vital for many web applications, and edge computing is becoming an essential part of 5G networks. In particular, edge artificial intelligence (AI) performs machine learning model training and inference at the network edge on edge servers. Although edge AI enables low-latency machine learning inference, which is critical for delay-sensitive applications, previous work has not fully explored its unique ability to process data with context-awareness.",
                "distance": 0.0211
            },
            {
                "reference": "Current systems for analytics at tactical edge rely on deep neural networks (DNNs) that make use of floating-point Multiply-ACcumulate operations (MACs) which can be computationally expensive and inefficient for certain applications.",
                "distance": 0.0329
            },
            {
                "reference": "Mobile devices and other resource-limited systems often struggle to host deep learning models due to the high computational requirements of such models.",
                "distance": 0.0447
            },
            {
                "reference": "Deploying deep neural networks (DNNs) on head-mounted systems poses challenges due to hardware resource constraints and the need for high inference accuracy and low latency.",
                "distance": 0.0494
            },
            {
                "reference": "In industry scale applications, large machine learning models reside in cloud servers while large amounts of input data are collected at the edge of the cloud. Inferencing these models with high accuracy and low latency on low-powered edge devices is a challenge.",
                "distance": 0.0515
            },
            {
                "reference": "With the development of deep learning technologies, the demand for deep learning-based services on personal devices, such as mobile phones, has increased rapidly. Conventional personalization methods require additional fine-tuning with personal data.",
                "distance": 0.0526
            },
            {
                "reference": "Deep neural networks for computer vision and speech recognition commonly perform inference either locally on edge devices (edge-based), or transmit the data to the cloud for remote inference (cloud-based). However, edge devices often lack sufficient hardware resources to support large models, and cloud services may pose privacy concerns if they are not trustworthy.",
                "distance": 0.0562
            },
            {
                "reference": "Traditional hardware and software ecosystems for Deep Learning (DL) models, including those dealing with face recognition tasks, tend to consume high power due to their operational mechanisms.",
                "distance": 0.0573
            },
            {
                "reference": "The need to integrate machine learning capabilities into Internet-of-Things devices is growing, but device resource constraints often hinder the effectiveness of this functionality. Traditional hand-crafted optimizers or existing meta optimizers are not designed to meet the challenges of these resource-limited devices, as the former require exhaustive hyper-parameter tuning and the latter can be costly in terms of meta algorithm overhead.",
                "distance": 0.0591
            },
            {
                "reference": "There has been an increase in demand to construct compact deep architecture to improve computational efficiency by removing redundancy. Nevertheless, most current models focus on reducing redundancy by eliminating unneeded weight parameters, making them incompatible with multiple devices with different resources and requiring a new network to be understood and trained from scratch when met with new device or circumstantial conditions.",
                "distance": 0.069
            },
            {
                "reference": "A pressing issue in the deployment of distributed deep learning inference is the high-resource demand in terms of memory footprint, energy consumption, and latency.",
                "distance": 0.0719
            },
            {
                "reference": "EfficientNets used a giant formula for simultaneously enlarging the resolution, depth and width of deep neural architectures to optimize performance, yet their method may not be suitable for tiny networks where minimization of model sizes and computational costs is required.",
                "distance": 0.0745
            },
            {
                "reference": "While deep neural networks (DNNs) have improved the accuracy of video data analytics, current video analytic systems still lack support for more advanced operations beyond selection and aggregation queries, like the important Top-K analytical operation.",
                "distance": 0.0759
            },
            {
                "reference": "Building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices poses a major challenge.",
                "distance": 0.0764
            },
            {
                "reference": "Smart portable applications increasingly rely on edge computing which has major challenges: heavily resource-constrained hardware and dynamic application conditions. Probabilistic models present a potential solution to these challenges but the current notions of tractability are often limited to model complexity, disregarding the hardware specifications and constraints.",
                "distance": 0.0769
            },
            {
                "reference": "The increasing demand for deep learning services on mobile devices with limited capacity calls for efficient deployment of deep neural networks (DNNs). The cloud-based solution is promising but disclosing data to the cloud leads to potential privacy risks.",
                "distance": 0.0806
            },
            {
                "reference": "Training deep neural networks for deployment on resource-constrained embedded hardware usually involves representing each learned weight parameter ideally using a single bit, but this often leads to increased error rates.",
                "distance": 0.0843
            },
            {
                "reference": "Successful deployment of deep learning in real-world environments necessitates compact, accurate, and robust models. However, efforts to simultaneously meet these requirements have been largely unsuccessful, leading to a question about the feasibility.",
                "distance": 0.0873
            },
            {
                "reference": "Existing methods to serialize and deserialize gradient-boosted trees and random forests don't account for scenarios when these models are deployed on low-resource devices in the Internet of Things or as cloud microservices where resources are allocated on demand. This results in bottlenecks for model inference as they often cannot be loaded fully into memory.",
                "distance": 0.0896
            }
        ]
    },
    {
        "target": "Generalized Additive Models (GAMs) often use pairwise interactions to maintain accuracy, flexibility, and interpretability, but this creates a computational challenge as the number of terms increases quadratically. Previous approaches that consider sparse pairwise interactions don\u2019t scale well, particularly with added structural interpretability constraints.",
        "prediction": "Standard generalized additive models (GAMs) typically model the dependent variable as a sum of univariate models. Although these models can be interpreted by users, they lack the accuracy of more complex models that permit interactions.",
        "queries": [
            3.296636,
            -13.424284
        ],
        "log": [
            {
                "reference": "Standard generalized additive models (GAMs) typically model the dependent variable as a sum of univariate models. Although these models can be interpreted by users, they lack the accuracy of more complex models that permit interactions.",
                "distance": 0.0341
            },
            {
                "reference": "Generalized additive models (GAMs) are favored in many regression and binary classification problems for their abilities to fit complex nonlinear functions while maintaining interpretability. However, the natural interpretability of GAMs breaks down when applied to multi-class scenarios, leading to potentially misleading conclusions.",
                "distance": 0.0383
            },
            {
                "reference": "Generalized additive models (GAMs) are prominent in interpretable machine learning, but varying algorithms for training GAMs can result in a range of models with potentially conflicting interpretations, all while maintaining equivalent accuracy.",
                "distance": 0.0437
            },
            {
                "reference": "In high-dimensional statistics, the challenge lies in achieving outlier-robust estimation under sparsity constraints, specifically for tasks such as robust sparse mean estimation and robust sparse PCA, which has previously been carried out under more restrictive distributional assumptions.",
                "distance": 0.0442
            },
            {
                "reference": "Generalized Additive Models (GAM) are popular for forecasting electricity demand, but the residuals of the fitted GAM are typically heteroscedastic and leptokurtic due to the nature of energy data.",
                "distance": 0.0533
            },
            {
                "reference": "Generalized Additive Models (GAMs) are the leading choice for inherently interpretable machine learning, but lack expressive power and scalability, making them less suitable for real-world tasks.",
                "distance": 0.0537
            },
            {
                "reference": "The context of this research paper is robust regression in the list-decodable setting where an adversary can corrupt a greater than 1/2 fraction of examples which has not been resolved by any polynomial-time algorithm.",
                "distance": 0.0633
            },
            {
                "reference": "The problem under study is high-dimensional robust mean estimation in the presence of a constant fraction of adversarial outliers. Sophisticated polynomial-time algorithms with dimension-independent error guarantees for several natural distribution families have been proposed in the existing literature.",
                "distance": 0.0804
            },
            {
                "reference": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) are powerful generalizations of linear regression, often involving non-convex estimation procedures. Kalai and Sastry (2009) introduced the Isotron algorithm for learning SIMs and GLMs under certain assumptions and constraints, but their approach requires a fresh sample every iteration and involves an isotonic regression step with quadratic complexity.",
                "distance": 0.0816
            },
            {
                "reference": "Neyman Orthogonality and generalized random forests are two powerful tools used to deal with the problem of high-dimensional sparsity in statistical estimation, but combining them into a coherent model to improve this estimation process was a research gap.",
                "distance": 0.0913
            },
            {
                "reference": "Current methods of signal estimation from nonlinear observations following the semi-parametric single index model (SIM) are typically iterative and assume that the signal is sparse, an assumption which may not always hold.",
                "distance": 0.094
            }
        ]
    },
    {
        "target": "Existing methods for segmenting objects are supervised and require training.",
        "prediction": "Recent advances in semantic segmentation have failed to adequately address the problem of identifying object instances.",
        "queries": [
            14.677112,
            -5.709468
        ],
        "log": [
            {
                "reference": "Recent advances in semantic segmentation have failed to adequately address the problem of identifying object instances.",
                "distance": 0.0305
            },
            {
                "reference": "Existing methods for instance-level semantic segmentation typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each proposal. However, they are not robust to errors in the object candidate generation process, such as too small or shifted boxes.",
                "distance": 0.0867
            },
            {
                "reference": "The task of semantic part segmentation for animals is challenging due to multiple reasons. The standard object detection, object segmentation, and pose estimation tasks involve semantic parts of animals that often share similar appearances and highly varying shapes.",
                "distance": 0.0919
            }
        ]
    },
    {
        "target": "Conjunctive queries with predicates in the form of comparisons spanning multiple relations have regained recent interest due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, shows limited efficacy on such comparisons while a technique by Willard is efficient for short comparisons that are adjacent in the join tree.",
        "prediction": "The optimization of disjunctive queries with expensive predicates, particularly selection predicates with terms of drastically varying evaluation costs, such as nested subqueries, invocations of user-defined functions, and more expensive than basic attribute access, is challenging in object-oriented or extended relational models.",
        "queries": [
            -11.329066,
            -6.660494
        ],
        "log": [
            {
                "reference": "The optimization of disjunctive queries with expensive predicates, particularly selection predicates with terms of drastically varying evaluation costs, such as nested subqueries, invocations of user-defined functions, and more expensive than basic attribute access, is challenging in object-oriented or extended relational models.",
                "distance": 0.0272
            },
            {
                "reference": "Production systems recently enabled users to specify an appropriate ordering or a clustering of join operations, and various efficiency heuristics have been used to manually optimize production rules.",
                "distance": 0.0502
            },
            {
                "reference": "Existing methods for query performance prediction focus on a single query. They struggle to effectively predict the performance for concurrent queries due to the complexity in capturing correlations between different queries such as lock conflict and buffer sharing.",
                "distance": 0.0632
            },
            {
                "reference": "Modern query optimizers need to take into account the performance of expensive user-defined predicates. Existing research has shown how to incorporate such predicates in a traditional cost-based query optimizer, but not with the optimization of the expensive predicates themselves.",
                "distance": 0.0632
            },
            {
                "reference": "Query optimization for relational database systems is a combinatorial optimization problem. With the growth of query size, exhaustive search becomes unacceptable. The common alternatives are randomized algorithms such as Simulated Annealing (SA) and Iterative Improvement (II).",
                "distance": 0.0739
            },
            {
                "reference": "Query optimizers employ various sources of information including runtime statistics and integrity constraints to optimize queries. Recently, optimizers began exploiting other types of integrity constraints for more accurate query optimization. However, many potential 'constraints' useful for optimization are often not explicitly available for the optimizer.",
                "distance": 0.0823
            },
            {
                "reference": "In a distributed database environment, finding the optimal strategy that fully reduces all relations referenced by a general tree query may take exponential time. Furthermore, the problem of minimizing the total transmission cost, even with only one join attribute, is NP-hard.",
                "distance": 0.084
            },
            {
                "reference": "Estimating resource consumption of SQL queries is crucial for many tasks in a database system. Current practices employ manually constructed cost models for query optimization or use statistical techniques. However, these models are often not robust and may not generalize well to queries different from the training set, leading to estimation errors.",
                "distance": 0.092
            },
            {
                "reference": "Recent work in query optimization has been tackling the issue of deciding where to place expensive predicates in a query plan.",
                "distance": 0.0985
            },
            {
                "reference": "In a generalized distributed database system with decentralized controls and heterogeneous and pre-existing nodes, query processing can be complex, especially for data integration purposes. The issue of selecting optimal strategies for complex queries is NP-complete, making it unfeasible to evaluate every strategy. Many existing query decomposition algorithms produce optimal or near-optimal strategies only under highly restrictive assumptions that apply to a particular implementation.",
                "distance": 0.0998
            }
        ]
    },
    {
        "target": "Efficient detectors for edge devices are often optimized for metrics like parameters or speed counts, which have a weak correlation with the energy of detectors. Some vision applications of convolutional neural networks (CNNs), such as always-on surveillance cameras, are critical for energy constraints.",
        "prediction": "Small, specialized convolutional neural networks (CNNs) are widely used in datacenter vision systems for high-throughput inference on specific tasks. However, these networks underutilize accelerators due to low arithmetic intensity, leading to suboptimal throughput and poor investment returns. Increasing the batch size was the only known way to boost throughput and accelerator utilization, but it yields diminishing returns, especially for specialized CNNs.",
        "queries": [
            9.648402,
            -8.151662
        ],
        "log": [
            {
                "reference": "Small, specialized convolutional neural networks (CNNs) are widely used in datacenter vision systems for high-throughput inference on specific tasks. However, these networks underutilize accelerators due to low arithmetic intensity, leading to suboptimal throughput and poor investment returns. Increasing the batch size was the only known way to boost throughput and accelerator utilization, but it yields diminishing returns, especially for specialized CNNs.",
                "distance": 0.0136
            },
            {
                "reference": "Training convolutional neural networks (CNNs) is time-consuming and prior work has attempted to reduce the computational demands by eliminating gradients with relatively small magnitude.",
                "distance": 0.0152
            },
            {
                "reference": "Optimizing convolutional neural networks (CNNs) for GPUs remains challenging and most researchers rely on libraries provided by vendors. High performance computing in fields such as computer vision, natural language processing, and artificial intelligence is in need of more efficient computational methods.",
                "distance": 0.0158
            },
            {
                "reference": "Current architectures for landmark localization using Convolutional Neural Networks (CNNs) are highly performant but not suitable for applications with limited computational resources.",
                "distance": 0.0257
            },
            {
                "reference": "Deep convolutional neural networks (CNNs) accuracy generally improves with high resolution images, however, this increases computational cost and memory footprint.",
                "distance": 0.0376
            },
            {
                "reference": "Convolutional Neural Networks (CNN) have significantly improved computer vision performances over the recent years, with applications deployed on general purpose hardware like CPUs, GPUs or FPGAs. However, the considerations for mobile and embedded applications include factors like power consumption, speed, accuracy, memory footprint, and die size.",
                "distance": 0.0436
            },
            {
                "reference": "Deep convolutional neural networks (CNNs) are commonly large in size and computationally intensive, which makes them difficult to deploy on devices with limited resources. Existing methods for network pruning focus on reducing less-important layers in a network, but this approach generally changes the original network structure.",
                "distance": 0.048
            },
            {
                "reference": "Deep convolutional neural networks (CNNs) are resource-intensive due to their complex design and a large number of learnable parameters, making them expensive to deploy on mobile devices. Recent works aim to reduce this cost by finding redundancy in pre-defined architectures, but the input resolution used by CNNs is typically fixed and hasn't been fully investigated for potential redundancy.",
                "distance": 0.0531
            },
            {
                "reference": "Convolutional Neural Networks (CNN) are ideal for safety-critical applications but ensuring reliability in the presence of possible memory faults is a challenge. Traditional methods such as error correction codes (ECC) and Triple Modular Redundancy (TMR) are not specifically designed for CNNs and lead to significant memory overhead and energy costs.",
                "distance": 0.0542
            },
            {
                "reference": "Current convolutional neural networks (CNNs) usually utilize full-precision networks and preserve floating-point information using a set of discrete values, often referred to as value approximation. Such models are not ideal for mobile devices with limited power capacity and computation resources.",
                "distance": 0.0564
            },
            {
                "reference": "Training Convolutional Neural Networks (ConvNets) for large datasets like Imagenet can be time-consuming. Previous works have attempted to improve across multiple GPUs performance by improving the underlying optimization algorithm.",
                "distance": 0.0572
            },
            {
                "reference": "Convolutional Neural Networks (CNNs) are computationally intensive, limiting their applications on mobile devices due to high energy usage from multiple convolutions. Sparse Winograd methods can reduce this operation count but cannot be directly combined with network pruning, as applying the Winograd transform end up filling the sparsity in both the weights and the activations.",
                "distance": 0.0588
            },
            {
                "reference": "Current convolutional neural network (CNN) architectures for semantic segmentation under resource constraints are inefficient in terms of speed and performance.",
                "distance": 0.0623
            },
            {
                "reference": "Various applications for autonomous driving rely on optimized convolutional neural networks (CNNs) for processing camera data. However, spending redundant GPU hours during compression makes it challenging to quickly utilize newly learned features.",
                "distance": 0.0646
            },
            {
                "reference": "Very deep and wide Convolutional Neural Networks (CNNs) are effective in various computer vision tasks but the significant storage requirement of such networks impedes their deployment on computationally limited devices.",
                "distance": 0.0721
            },
            {
                "reference": "While convolutional neural networks (CNN) based object detectors have improved substantially in terms of accuracy, they are often too computationally intensive for real-time applications. Model compression methods can reduce these computation requirements, but they normally result in significant accuracy losses.",
                "distance": 0.0727
            },
            {
                "reference": "Convolutional neural networks (CNNs) excel in various computer vision related tasks but are extremely computationally intensive and power hungry to run on mobile and embedded devices. Recent pruning techniques can reduce the computation and memory requirements of CNNs, but a costly retraining step is needed to restore the classification accuracy of the pruned model.",
                "distance": 0.0746
            },
            {
                "reference": "Densely Connected Convolutional Networks (DenseNet) are suffering from redundancy due to the need to integrate all previous outputs, leading to inefficient feature fusion and increased complexity.",
                "distance": 0.0755
            },
            {
                "reference": "Deep CNN models have become state-of-the-art techniques in many applications like face recognition, speaker recognition, and image classification. Many studies focus on the speedup or compression of individual models, but very few studies focus on co-compressing and unifying models from different modalities.",
                "distance": 0.0792
            },
            {
                "reference": "While deep convolutional neural networks (CNNs) have contributed to groundbreaking results in machine learning and computer vision, vector processing hardware such as GPUs has played a vital role in modern CNN implementations. However, little research has been dedicated to vectorization for scaling up CNNs.",
                "distance": 0.0801
            }
        ]
    },
    {
        "target": "Contrastive loss is a popular method for learning representations from multiple modalities with the aim to match modalities precisely in the latent space. However, the impact of this modality alignment on downstream task performance is unclear.",
        "prediction": "Contrastive learning models have achieved success in unsupervised visual representation learning, relying on maximizing the similarities between feature representations of different views of the same image, while minimizing the similarities between feature representations of views of different images.",
        "queries": [
            26.755856,
            4.984269
        ],
        "log": [
            {
                "reference": "Contrastive learning models have achieved success in unsupervised visual representation learning, relying on maximizing the similarities between feature representations of different views of the same image, while minimizing the similarities between feature representations of views of different images.",
                "distance": 0.0186
            },
            {
                "reference": "Existing graph self-supervised learning methods can be divided into predictive learning and contrastive learning, with contrastive learning attracting more attention due to better performance.",
                "distance": 0.0245
            },
            {
                "reference": "Instance-wise contrastive learning has been a popular method for unsupervised representation learning, but it has some fundamental limitations, such as reliance on low-level cues, which PCL seeks to address.",
                "distance": 0.0289
            },
            {
                "reference": "Supervised contrastive learning (SupCon) is a promising method for training accurate models, but its tendency for class collapse\u2014when all points in a class map to the same representation\u2014limits its transferability and robustness.",
                "distance": 0.029
            },
            {
                "reference": "While contrastive learning has become a successful technique in the self-supervised learning domain, the approach usually requires large, computationally expensive networks to achieve high accuracy. Additionally, applying dynamic pruning algorithms with auxiliary salience predictors from supervised learning to contrastive learning proves challenging.",
                "distance": 0.0373
            },
            {
                "reference": "Contrastive representation learning, which uses unlabeled data, has been efficiently used in the vision domain with the help of specifically crafted data augmentations using domain knowledge.",
                "distance": 0.0449
            },
            {
                "reference": "Contrastive self-supervised learning has been successfully integrated into deep reinforcement learning (RL), showing efficient policy learning across various applications. However, the theoretical understanding of contrastive learning for RL remains unclear.",
                "distance": 0.0528
            },
            {
                "reference": "Contrastive learning between multiple views of the data has recently achieved state-of-the-art performance in the field of self-supervised representation learning. However, the influence of different view choices in this process is less studied.",
                "distance": 0.0553
            },
            {
                "reference": "Self-supervised contrastive learning methods have made notable strides in visual representation learning. Supervised contrastive learning has also been shown to perform better than its cross-entropy counterparts by utilising labels for contrast selection. Nonetheless, studies exploring the transfer capability of contrastive learning to different domains have been scant.",
                "distance": 0.0575
            },
            {
                "reference": "Existing symmetric contrastive learning methods suffer from collapses (complete and dimensional) or quadratic complexity of objectives. These methods typically aim to maximize the mutual information of two generated views, along either instance or feature dimension.",
                "distance": 0.0585
            },
            {
                "reference": "Unsupervised learning methods based on contrastive learning have drawn increasing attention due to their promising results. Most of these methods aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance.",
                "distance": 0.0585
            },
            {
                "reference": "Contrastive self-supervised learning is a promising way to learn unsupervised visual representation learning that focuses on global image-level representations. However, there is a need for dense pixel-level representations in many visual understanding tasks.",
                "distance": 0.0588
            },
            {
                "reference": "Contrastive self-supervised learning has become a key component for learning visual representations in many computer vision tasks. However, its application in the context of domain adaptation remains largely underexplored.",
                "distance": 0.059
            },
            {
                "reference": "Contrastive Learning has emerged as a powerful representation learning method. However, how to construct efficient contrastive samples through data augmentation is key to its success and has not been investigated sufficiently in language tasks.",
                "distance": 0.0604
            },
            {
                "reference": "Contrastive learning between different views of the data in the field of self-supervised representation learning successfully obtains the minimal sufficient representation, which contains only the shared information and eliminates the non-shared information between views.",
                "distance": 0.0621
            },
            {
                "reference": "Self-supervised learning, specifically contrastive learning, has been effectively used to learn discriminative features, and has gained popularity in the Natural Language Processing (NLP) community. However, its application in Transformer models for addressing the Winograd Schema Challenge is not explored.",
                "distance": 0.0626
            },
            {
                "reference": "Unsupervised pretraining based on contrastive learning has shown comparable or even superior transfer learning performance to traditional supervised pretraining on various tasks.",
                "distance": 0.0629
            },
            {
                "reference": "Contrastive learning based methods, while effective for self-supervised learning, suffer from slow convergence and difficulty in optimization, particularly when using small-scale models which see a significant performance drop compared to their supervised counterparts.",
                "distance": 0.0633
            },
            {
                "reference": "Contrastive self-supervised learning (CSL) has matched or surpassed the performance of supervised learning in image and video classification. However, it is largely unknown whether the types of representations learned by both paradigms are similar.",
                "distance": 0.0636
            },
            {
                "reference": "Contrastive representation learning has achieved significant success but it relies on carefully designed data augmentations using domain-specific knowledge. This is a challenge in natural language processing, due to the discrete nature of natural language, and no general rules exist for data augmentation.",
                "distance": 0.0637
            }
        ]
    },
    {
        "target": "The data scarcity problem in end-to-end speech translation (E2E ST) is currently addressed through pre-training and fine-tuning methods. Issues like modality gap and capacity gap among high and low resource tasks lead to sub-optimal model performance.",
        "prediction": "End-to-end speech translation (ST) has been treated as an independent task, not benefiting from the advancements in text machine translation (MT) due to a modality gap between text and audio inputs that makes MT data and models incompatible with their ST equivalents.",
        "queries": [
            4.234341,
            13.589175
        ],
        "log": [
            {
                "reference": "End-to-end speech translation (ST) has been treated as an independent task, not benefiting from the advancements in text machine translation (MT) due to a modality gap between text and audio inputs that makes MT data and models incompatible with their ST equivalents.",
                "distance": 0.0062
            },
            {
                "reference": "End-to-end Speech Translation (ST) traditionally requires parallel ST data for training, which is challenging and costly to obtain. Existing zero-shot methods fail to align the two modalities of speech and text into a shared semantic space, causing them to underperform compared to supervised ST methods.",
                "distance": 0.0284
            },
            {
                "reference": "Speech-to-text translation (ST), which directly translates the source language speech to target language text, has recently gained attention. The combination of speech recognition and machine translation in a single model poses a heavy burden on direct cross-modal cross-lingual mapping.",
                "distance": 0.0285
            },
            {
                "reference": "In the field of speech translation (ST), multitask learning has been used to improve ST performance. This typically involves a recognition decoder that generates the source language text and a translation decoder that forms translations based on this output.",
                "distance": 0.0355
            },
            {
                "reference": "End-to-end Speech Translation (E2E ST) traditionally struggles with degraded performance when only limited ST data are available, and performance of these models often correlates with embedding similarity between speech and transcript.",
                "distance": 0.0433
            },
            {
                "reference": "End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in real-time, is useful in many scenarios, but has not been fully researched yet.",
                "distance": 0.0499
            },
            {
                "reference": "End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. Conventional methods typically adopt fixed policies, such as segmenting the source speech with a fixed length and generating translation, but suffer from low translation quality due to the lack of consideration for contextual information.",
                "distance": 0.0608
            },
            {
                "reference": "Direct speech-to-speech translation (S2ST), which all components can be optimized jointly, is known to offer faster inference and a simplified pipeline, in comparison to cascaded approaches.",
                "distance": 0.0695
            },
            {
                "reference": "End-to-end speech-to-text translation (ST) methods interpret audio in a source language and output text in a target language. However, these methods are limited by the amount of available parallel corpus.",
                "distance": 0.0745
            },
            {
                "reference": "The effectiveness of neural-based approaches to End-to-End Speech Translation (E2E-ST) is severely limited by the available training corpus, especially for domain adaptation where in-domain triplet training data is scarce or nonexistent.",
                "distance": 0.0795
            },
            {
                "reference": "Direct approaches to speech translation (ST) have been improving and competing with traditional cascade solutions over the past five years.",
                "distance": 0.0847
            },
            {
                "reference": "End-to-end speech-to-text translation (E2E-ST) is becoming popular due to its potential for lower error propagation, latency, and fewer parameters. Current high-quality E2E-ST systems leverage the \u3008speech, transcription\u3009 pair to pre-train the model and then the \u3008speech, translation\u3009 pair to further optimize it. However, this process only involves two-tuple data at each stage and doesn't fully use the association between triplet data.",
                "distance": 0.0944
            },
            {
                "reference": "The use of a non-autoregressive speech-to-text translation model based on connectionist temporal classification (CTC) presents challenges, partly due to CTC's monotonicity assumption.",
                "distance": 0.0947
            }
        ]
    },
    {
        "target": "Machine learning-based forecasting models used in Intelligent Transportation Systems (ITS) are prone to adversarial attacks, leading to inaccurate traffic predictions with potential negative impacts such as congestion and delays.",
        "prediction": "Urban flow monitoring systems are integral to smart city efforts, but the wide-scale deployment of monitoring devices, such as CCTVs, results in significant cost in maintenance and operation. Existing solutions do not sufficiently reduce the number of required devices while maintaining data accuracy and granularity.",
        "queries": [
            9.422734,
            1.766268
        ],
        "log": [
            {
                "reference": "Urban flow monitoring systems are integral to smart city efforts, but the wide-scale deployment of monitoring devices, such as CCTVs, results in significant cost in maintenance and operation. Existing solutions do not sufficiently reduce the number of required devices while maintaining data accuracy and granularity.",
                "distance": 0.0107
            },
            {
                "reference": "Traffic state estimation (TSE) is important for traffic managements and traditional TSE approaches mainly bifurcate into two categories: model-driven and data-driven, each having their own shortcomings. Hybrid TSE methods which combine both model-driven and data-driven approaches are starting to be seen as promising solutions to these issues.",
                "distance": 0.0278
            },
            {
                "reference": "Previous works on real-time traffic accident forecasting often perform on hour levels, utilizing existing neural networks with static region-wise correlations. This approach can lead to challenges when forecasting granularity improves due to the highly dynamic nature of the road network and inherent rareness of accident records in a single training sample.",
                "distance": 0.0283
            },
            {
                "reference": "Traffic congestion problems are rooted deeply in transportation infrastructure, and people's hesitance to carpool with unknown drivers is one of the issues making these problems difficult to solve.",
                "distance": 0.0299
            },
            {
                "reference": "Prior work has not addressed the problem of predicting travel time of vehicles on arterial roads, which is affected by complex spatio-temporal dependencies between different road links.",
                "distance": 0.0354
            },
            {
                "reference": "Urban traffic control tasks traditionally rely on deep reasoning mechanisms. The challenges lie in the temporal and spatial computational costs associated with explaining urban traffic behavior.",
                "distance": 0.0398
            },
            {
                "reference": "Traffic jam is a prevalent issue in urban areas, and city-wide traffic modeling, visualization, analysis, and prediction are still challenging.",
                "distance": 0.0456
            },
            {
                "reference": "Traffic congestion is a significant problem in large-scale crowd activities negatively impacting the efficiency of individual visitors and the whole system.",
                "distance": 0.0462
            },
            {
                "reference": "Discovering congestion patterns from table-formed traffic reports is critical for traffic bottleneck analysis, but patterns mined by existing algorithms often do not satisfy user requirements and are not actionable for traffic management.",
                "distance": 0.0593
            },
            {
                "reference": "Traffic forecasting, a critical aspect of Intelligent Traffic Systems (ITS), presents unique challenges due to its dependence on road networks (which render data as traffic graphs), strong spatial dependence (resulting in complex graph node relationships), and pronounced temporal dependence.",
                "distance": 0.0608
            },
            {
                "reference": "Road traffic congestion is a growing global problem that leads to significant air pollution, loss of time and money, and requires efficient management. There is a need for intelligent traffic management that can efficiently estimate and classify the traffic congestion state of different road segments.",
                "distance": 0.0637
            },
            {
                "reference": "Internet traffic routing has been a challenge and previously, reinforcement learning (RL) algorithms have been used in an attempt to optimize it.",
                "distance": 0.0664
            },
            {
                "reference": "Public accidents caused by urban logistics have been a significant issue, and existing models may not effectively analyze their causes.",
                "distance": 0.072
            },
            {
                "reference": "Existing methods of predicting travel times in an automobile, which may involve predicting speeds at different stages along a route, have provided unsatisfactory results.",
                "distance": 0.0738
            },
            {
                "reference": "Traffic forecasting has become increasingly important with the rise of online navigation services, ridesharing, and smart city projects. However, due to the non-stationary nature of road traffic, the forecasting accuracy is hindered by the lack of contextual information.",
                "distance": 0.0751
            },
            {
                "reference": "Traffic flow prediction is a challenging task in machine learning, due to the complex dynamic spatial-temporal dependencies within a road network. Typically, traditional methods have used predefined static graphs which do not effectively capture these complexities.",
                "distance": 0.0754
            },
            {
                "reference": "Carpooling schemes have been around for a while, but the prevalence of driving alone, particularly during the COVID-19 pandemic, the increase in car ownership, and the difficulty in finding travelers with matching schedules and routes continue to keep car occupancy low.",
                "distance": 0.0802
            },
            {
                "reference": "The bus system is essential for sustainable urban transportation, but it is often unstable due to uncertainties in passenger demand and traffic conditions. One common problem is bus bunching, which undermines service reliability and efficiency. Despite advances in multi-agent reinforcement learning (MARL) in traffic control, little work has been done on bus fleet control due to the asynchronous nature of bus arrivals at stops.",
                "distance": 0.0828
            },
            {
                "reference": "Traffic prediction, which is highly affected by factors such as spatial dependency of complex road networks and temporal dynamics, is a challenging task. Previous research has utilized the combination of convolutional neural networks (CNNs) for spatial dependency and recurrent neural networks (RNNs) for temporal dynamics; however, these combinations fail to capture the connectivity and globality of traffic networks.",
                "distance": 0.0887
            },
            {
                "reference": "Traffic congestion significantly impacts quality of life and the economy.",
                "distance": 0.0889
            }
        ]
    },
    {
        "target": "Despite significant progress in question answering on tabular data (Table QA), it remains unclear how and to what extent existing Table QA models are robust to task-specific perturbations like, replacing key question entities or shuffling table columns.",
        "prediction": "Traditional framework for question answering systems struggles in handling questions with hidden assumptions and implicatures.",
        "queries": [
            -7.354535,
            10.718641
        ],
        "log": [
            {
                "reference": "Traditional framework for question answering systems struggles in handling questions with hidden assumptions and implicatures.",
                "distance": 0.0413
            },
            {
                "reference": "Current methods for generating poll questions tackle formally-written texts, primarily focusing on sources like exam papers, and often struggle with short, informal texts like social media posts due to data sparsity.",
                "distance": 0.0453
            },
            {
                "reference": "The meaning of replies to yes/no questions, particularly those involving scalar modifiers, is often indirect and unclear. Traditional methods struggle to interpret and ground the meanings of scalar adjectives.",
                "distance": 0.0552
            },
            {
                "reference": "The task of generating yes or no questions based on given passages exists but the solutions already available are mentioned as lacking.",
                "distance": 0.0598
            },
            {
                "reference": "Despite models achieving superhuman performance on popular question answering (QA) datasets such as SQuAD, they have not yet surpassed humans at the task of question answering itself.",
                "distance": 0.0608
            },
            {
                "reference": "Traditional question answering models struggle with variations of questions and often fail to generate natural and correct answers.",
                "distance": 0.0664
            },
            {
                "reference": "The task of generating questions based on answers and relevant contexts has been largely focused on the quality of a single generated question, despite it being a one-to-many problem where different questions with varied focus and expressions can be generated from the same context.",
                "distance": 0.0674
            },
            {
                "reference": "The current methods of question-knowledge matching in chatbot systems like AliMe require improvements, and the high Query Per Second (QPS) requirement makes it impractical to use deep learning models.",
                "distance": 0.0757
            },
            {
                "reference": "Question generation has been utilized to customize question answering (QA) systems to new domains without the need for manually annotated training data. However, current methods require large synthetic datasets and significant computational resources, making them inaccessible in cases where the text corpora are limited. Many domains depend on small text corpora, constraining the quantity of synthetic data that can be generated.",
                "distance": 0.078
            },
            {
                "reference": "The existing system for the Question Generation Shared Task Evaluation Challenge (QGSTEC) developed at the University of Pennsylvania in 2010 involves generating and ranking questions, which has rooms for improvement.",
                "distance": 0.0949
            }
        ]
    },
    {
        "target": "Existing methods for knowledge graph embeddings (KGE) have not considered the fact that knowledge graphs typically contain two different views: high-level ontological concepts and fine-grained instance entities. They usually represent all nodes as vectors in a single latent space, which overlooks the structural differences between the two views and misses the probabilistic semantics of the granularity of the concepts.",
        "prediction": "A large number of Knowledge graph embedding (KGE) techniques for multi-relational link prediction have been proposed recently, often with differing model architectures, training strategies, and approaches to hyperparameter optimization.",
        "queries": [
            -6.656665,
            -16.395514
        ],
        "log": [
            {
                "reference": "A large number of Knowledge graph embedding (KGE) techniques for multi-relational link prediction have been proposed recently, often with differing model architectures, training strategies, and approaches to hyperparameter optimization.",
                "distance": 0.0111
            },
            {
                "reference": "Most Knowledge Graph (KG) embedding models are trained based on negative sampling, which helps to reduce the time complexity of model learning. However, this may fail to deliver stable model performance due to the uncertainty in the sampling procedure.",
                "distance": 0.0153
            },
            {
                "reference": "Knowledge graph completion aims to predict missing relations between entities in a knowledge graph. Existing methods primarily rely on entity embedding.",
                "distance": 0.0157
            },
            {
                "reference": "Despite neural link predictors' efficiency in identifying missing edges in large scale Knowledge Graphs, their use in answering complex queries, such as those using logical conjunctions, disjunctions, and existential quantifiers, especially while considering missing edges, is unclear.",
                "distance": 0.02
            },
            {
                "reference": "Existing knowledge graph embedding (KGE) based methods represent individual entities and links in knowledge graphs (KGs) as vectors in a low-dimension space. However, they mainly focus on link prediction of individual entities, and neglect that between group entities, which are widely present in real-world KGs.",
                "distance": 0.0214
            },
            {
                "reference": "Existing Knowledge Graph Embedding (KGE) models can extrapolate to unseen triples, predicting missing entities or relations. However, most research has focused on designing complex triple modeling functions without extensively studying why these models can extrapolate to unseen data, and what the critical factors are for successful extrapolation.",
                "distance": 0.0247
            },
            {
                "reference": "Knowledge Graph Embeddings (KGEs) map entities and relations from a knowledge graph into a geometric space, showing promising performance on link prediction tasks. However, many KGEs use Euclidean geometry, which limits their ability to preserve complex structures and often leads to incorrect inferences by the models.",
                "distance": 0.0285
            },
            {
                "reference": "In KG embeddings, entities and relations are embedded into a single geometric space such as Euclidean, hyperbolic, or hyperspherical space to maintain their geometric structures like chain, hierarchy and ring structures. Yet, the topology of KGs is complex and may contain multiple geometric structures at the same time. Existing methods are insufficient in capturing such complex structures.",
                "distance": 0.0287
            },
            {
                "reference": "Existing bi-encoder architectures for distantly supervised relation extraction, which use text and knowledge graphs (KG), either do not share any information between the text and KG encoders, or, if a KG-to-text attention model is used, only share information in one direction.",
                "distance": 0.0288
            },
            {
                "reference": "Recent studies have shown that entity representations extracted from pre-trained language models can be used to develop knowledge graph completion models that are more robust to the naturally occurring sparsity in knowledge graphs.",
                "distance": 0.0301
            },
            {
                "reference": "While embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, embedding models for uncertain KGs, which typically model the inherent uncertainty of relations facts with a confidence score, represent an unresolved challenge.",
                "distance": 0.0324
            },
            {
                "reference": "Continuous representations of knowledge graph (KG) components, such as entities, types and relations, are widely used for entity mention disambiguation, relation inference, and deep question answering. Existing models that use Gaussian, holographic, and complex embeddings don't directly enforce transitivity inherent in is-instance-of and is-subtype-of relations. A recent proposal, order embedding (OE), has attempted to enforce these transitive relations but has some limitations.",
                "distance": 0.0328
            },
            {
                "reference": "Conventional knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors, resulting in large model sizes on real-world graphs with millions of entities and limiting their utility in multi-stage pipelines.",
                "distance": 0.0333
            },
            {
                "reference": "Relation prediction on knowledge graphs (KGs) has been deeply studied, but most studies are limited to the transductive setting, which cannot handle emerging entities. The inductive setting, allowing entities in the testing phase to be unseen during training, is closer to real-life scenarios but requires entity-independent relation modeling and discrete logical reasoning for interoperability.",
                "distance": 0.0339
            },
            {
                "reference": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity.",
                "distance": 0.0351
            },
            {
                "reference": "Current models for embedding knowledge graphs (KGs) focus on discriminating relation-specific information from entities via elaborate feature engineering, thereby potentially suffering from a high time and space complexity due to unique parameters learnt for each relation.",
                "distance": 0.0358
            },
            {
                "reference": "Recent Knowledge Graph Embeddings (KGE) models have been improving their performance by increasing the embedding dimensions, this approach however leads to increased training costs and more space requirements.",
                "distance": 0.0366
            },
            {
                "reference": "Representation learning of knowledge graphs is a key research topic in machine learning and AI, however, reimplementing Knowledge Graph Embedding (KGE) methods can be laborious, especially for methods originally written in non-python programming languages.",
                "distance": 0.0374
            },
            {
                "reference": "Knowledge graphs represent relational information in terms of triples and embedding models can be used to make predictions of new triples. Subclass and subproperty information, or background taxonomic information, often exists, but cannot be provably respected by existing fully expressive (universal) models.",
                "distance": 0.0391
            },
            {
                "reference": "Existing methods for embedding knowledge graphs (KGs) into continuous vector spaces are limited to handling explicit relationships within each triple (local connectivity patterns), but cannot manage implicit relationships across different triples (contextual connectivity patterns).",
                "distance": 0.0425
            }
        ]
    },
    {
        "target": "In cross-domain named entity recognition (cross-domain NER), previous methods primarily utilize pre-training language models like BERT to represent words. These chaotic initial representations can present difficulties in distinguishing between the entity types.",
        "prediction": "Self-augmentation methods, such as token substitution and mixup, have been used to improve named entity recognition (NER) performance in low-resource scenarios. However, these methods may introduce noisy augmented data, and previous research has mainly used heuristic rule-based constraints to reduce this noise for specific self-augmentation methods individually.",
        "queries": [
            -21.619907,
            -4.15182
        ],
        "log": [
            {
                "reference": "Self-augmentation methods, such as token substitution and mixup, have been used to improve named entity recognition (NER) performance in low-resource scenarios. However, these methods may introduce noisy augmented data, and previous research has mainly used heuristic rule-based constraints to reduce this noise for specific self-augmentation methods individually.",
                "distance": 0.0039
            },
            {
                "reference": "In literary analysis, characters are fundamental. Current approaches heavily rely on Named Entity Recognition (NER) to identify characters, causing many characters to be overlooked.",
                "distance": 0.004
            },
            {
                "reference": "Training named entity recognizers (NER) on upper case text poses challenges in accuracy, and there exists a performance gap between this context and mixed case Named Entity Recognition.",
                "distance": 0.005
            },
            {
                "reference": "Most Named Entity Recognition (NER) systems employ additional features such as part-of-speech (POS) tags, shallow parsing, and gazetteers, which can be time-consuming and require extensive data cleaning.",
                "distance": 0.0076
            },
            {
                "reference": "Named Entity recognition (NER) is an important part of many natural language processing tasks. Current approaches often employ machine learning techniques and require supervised data, which many languages lack.",
                "distance": 0.0076
            },
            {
                "reference": "Named Entity Recognition (NER) systems require a variety of information for top results, but integrating this information optimally can be challenging.",
                "distance": 0.0081
            },
            {
                "reference": "Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is not practical in many domains where dictionaries do not exist. A previous study attempted to address this using phrase retrieval from Wikipedia to create pseudo-dictionaries, but this approach often resulted in limited coverage due to bias towards popular entities.",
                "distance": 0.0082
            },
            {
                "reference": "The task of Named Entity Recognition and Classification (NERC) in the context of the CoNLL-2003 Shared Task represents the backdrop for this study.",
                "distance": 0.009
            },
            {
                "reference": "A fundamental challenge in real-world named entity recognition (NER) scenarios is the presence of a large amount of noise from sources like pseudo, weak, or distant annotations, despite recent advancements in deep learning-based NER.",
                "distance": 0.0101
            },
            {
                "reference": "Named Entity Recognition (NER) is a crucial task in natural language understanding. Scaling to emerging and/or low-resource domains is challenging due to the cost of acquiring training data.",
                "distance": 0.0103
            },
            {
                "reference": "In named entity recognition (NER), a fundamental task in natural language processing, named entities are always the minority among all tokens in the text. This data imbalance problem challenges machine learning models as their learning objective is usually dominated by the majority of non-entity tokens.",
                "distance": 0.0116
            },
            {
                "reference": "Named Entity Recognition (NER) and Named Entity Normalization (NEN) refer to the recognition and normalization of raw texts to known entities. In the context of recruitment, accurate and detailed normalization of skills is key for predictive analysis of labor market dynamics and has significant commercial and social implications.",
                "distance": 0.0118
            },
            {
                "reference": "Named Entity Recognition (NER) remains challenging in real-world settings, especially with short texts, emerging entities, and complex entities. Utilizing gazetteer features can aid in the process but existing approaches have led to models either overusing or underusing such data, resulting in poor generalization.",
                "distance": 0.0118
            },
            {
                "reference": "Named Entity Recognition (NER) is a crucial task in natural language processing and previously machine learning systems employed for this task yield inferior performance.",
                "distance": 0.0131
            },
            {
                "reference": "Named Entity Recognition (NER) has been traditionally treated as a sequence labeling task, but there has been a recent shift towards formulating NER as a machine reading comprehension task, where entity names or other information are considered as questions, text as the context, and the entity value in text as an answer snippet. However, these approaches consider machine reading comprehension based on a single question or entity at a time.",
                "distance": 0.014
            },
            {
                "reference": "Current few-shot named entity recognition (NER) approaches rely extensively on standard meta-learning techniques.",
                "distance": 0.0154
            },
            {
                "reference": "Named entity recognition (NER) has advanced in the general domain with systems achieving human-level performance for common entity types. However, performance is moderate for specialized domains, which tend to have complicated contexts and jargonistic entity types.",
                "distance": 0.0157
            },
            {
                "reference": "Journalists and editors need to sift through documents to find new information such as facts, opinions or stakeholders (people, places and organizations), which is a tedious and cognitively demanding process. Traditional named entity recognition models (NERs) have limitations.",
                "distance": 0.0162
            },
            {
                "reference": "Named entity recognition (NER) is a challenge in the field of natural language processing. Traditionally, diverse classifiers are used separately to extract named entities.",
                "distance": 0.0173
            },
            {
                "reference": "Named Entity Recognition (NER) has been traditionally performed without the use of document-specific knowledge base (KB) tags.",
                "distance": 0.019
            }
        ]
    },
    {
        "target": "Problematic webpages that promote hate and violence are difficult to identify due to the lack of labeled data for such tasks.",
        "prediction": "Hate speech online is rising and effective countermeasures that do not block freedom of speech are of high social interest. Current Natural Language Generation (NLG) methods are primarily sequence-to-sequence neural models that often generate commonplace, repetitive, irrelevant and safe responses to hate speech.",
        "queries": [
            -23.700186,
            4.539361
        ],
        "log": [
            {
                "reference": "Hate speech online is rising and effective countermeasures that do not block freedom of speech are of high social interest. Current Natural Language Generation (NLG) methods are primarily sequence-to-sequence neural models that often generate commonplace, repetitive, irrelevant and safe responses to hate speech.",
                "distance": 0.0125
            },
            {
                "reference": "Fighting online hate speech is commonly addressed using Natural Language Processing for automatic detection and removal of hate content. Counter narratives, often employed by NGOs on social media platforms, are being studied for automatic generation, but existing resources for training such models are limited to 2-turn interactions.",
                "distance": 0.0237
            },
            {
                "reference": "The problem of identifying misogyny in tweets exists across languages, and there is a need to address both mono and multilingual settings.",
                "distance": 0.0294
            },
            {
                "reference": "Recent studies in the hate and counter-hate domain have mainly used synthetic replies to hateful content written by annotators instead of replies written by real users.",
                "distance": 0.0366
            },
            {
                "reference": "Hate speech on social media platforms is a significant issue, and despite the development of machine learning-based classification models to combat it, their effectiveness is limited. There is a need to integrate social network data into these models, but a dearth of suitable datasets hampers this.",
                "distance": 0.0435
            },
            {
                "reference": "Current methods for hate speech detection on social media platforms stereotype words and suffer from inherent bias in their training. While bias removal from structured datasets has been explored, bias mitigation from unstructured text data has not been fully addressed.",
                "distance": 0.0463
            },
            {
                "reference": "Researchers have recently employed pre-trained visual-linguistic models for hateful meme detection, which combines text and image classification. Yet, it's unclear whether these models successfully capture derogatory or slurs references in the multimodal elements of hateful memes.",
                "distance": 0.0504
            },
            {
                "reference": "Hate speech is a pressing issue affecting the usefulness of online social communities, and although large scale social platforms are engaging in efforts for its automatic detection and classification, real-world success has been limited. In contrast, literature reports near-perfect performance of supervised approaches but only within specific datasets.",
                "distance": 0.051
            },
            {
                "reference": "Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. The task is defined as being able to classify a tweet as racist, sexist or neither. The complexity of the natural language constructs makes this task very challenging.",
                "distance": 0.0522
            },
            {
                "reference": "The task of automatic Counter Narrative (CN) generation to combat online hate speech in English has been explored using pre-trained language models, but there exists a lack of comparative study to determine the most suitable Language Model (LM) and decoding mechanism for the task.",
                "distance": 0.0534
            },
            {
                "reference": "Previous work has reported high classification scores on popular datasets for abusive language detection, however, those datasets often created by focused rather than random sampling, have an issue with data bias.",
                "distance": 0.0552
            },
            {
                "reference": "Detecting offensive content is a crucial and challenging task in Natural Language Processing (NLP), but the annotation of harmful speech for training NLP models is often simplified to a single binary label for concepts like hate or incivility.",
                "distance": 0.0552
            },
            {
                "reference": "Hate speech is prevalent in the online space, especially in user-generated content. However, the role of conversational context, defined as the preceding comment in a conversation thread, in hate speech and counter speech detection is not well-studied.",
                "distance": 0.0593
            },
            {
                "reference": "The task of counter narrative generation to undermine the impact of hateful online content is emerging. Existing Natural Language Processing (NLP) studies have attempted to build hate speech/counter narrative (HS/CN) datasets for this, but struggle to achieve high quality and/or high quantity.",
                "distance": 0.0645
            },
            {
                "reference": "Hate speech detection models are increasingly prevalent on online social media, but little attention has been given to bias and interpretability aspects of hate speech detection.",
                "distance": 0.0652
            },
            {
                "reference": "Detection of implicitly abusive language towards identity groups has been challenging due to the absence of abusive words and bias in existing datasets.",
                "distance": 0.0658
            },
            {
                "reference": "Existing methods for automated hate speech classification in social media have fixed datasets and pre-defined classes, which doesn't adapt to the rapidly changing topics and increasing volume of data.",
                "distance": 0.0686
            },
            {
                "reference": "Detecting online hate speech is a difficult task that even state-of-the-art models struggle with. Typically, the models are evaluated using metrics such as accuracy and F1 score on held-out test data, which often overlooks specific weak points of the model and may overestimate performance due to systematic gaps and biases in the datasets.",
                "distance": 0.0707
            },
            {
                "reference": "Current work on multilingual hate speech and its reduction relies on datasets that are typically collected and annotated without the involvement of the communities directly affected by hate speech. This often leads to companies and governments having control over defining and combating hate speech.",
                "distance": 0.0742
            },
            {
                "reference": "Hate speech and offensive language on social media are a rampant issue. While machine learning has provided a way to moderate foul language at scale, the focus has been on overall performance, with models potentially performing poorly on text written in minority dialectal languages, such as African-American Vernacular English (AAVE). Curating data for all linguistic styles quickly is challenging and constrained by specific problems, social media platforms, and limited resources.",
                "distance": 0.0767
            }
        ]
    },
    {
        "target": "Sexism on social media platforms creates an unwelcoming environment, and binary classification tools are used to identify sexist content; however, they don't provide further insights. SemEval-2023 introduced the Explainable Detection of Online Sexism (EDOS) task that puts emphasis on detecting and explaining the category of sexist content.",
        "prediction": "Designing automated techniques for the identification and categorisation of hostile speech, especially in low resource languages like Hindi with multiple dialects and no established datasets for such purpose, is a challenging task.",
        "queries": [
            -24.106579,
            4.439003
        ],
        "log": [
            {
                "reference": "Designing automated techniques for the identification and categorisation of hostile speech, especially in low resource languages like Hindi with multiple dialects and no established datasets for such purpose, is a challenging task.",
                "distance": 0.2589
            },
            {
                "reference": "Code-switching in linguistically diverse, low resource languages is often semantically complex and lacks sophisticated methodologies that can be applied to real-world data for precisely detecting hate speech.",
                "distance": 0.2595
            }
        ]
    },
    {
        "target": "Emotion-Cause Pair Extraction (ECPE) aims to identify the document\u2019s emotion clauses and corresponding cause clauses. The data related to emotions, causes, and pairs in this task is extremely unbalanced. Current methods, mostly based on Graph Convolutional Networks, model the multiplex relations between clauses by using the same graph structure which is challenging with unbalanced data.",
        "prediction": "Emotion-Cause Pair Extraction (ECPE), a task in text emotion analysis aims to extract pairs of emotions and their causes in a document. The existing strategy is a two-step process that first extracts individual emotion and cause sets, then pairs them. However, this doesn't directly aim at extracting pairs and errors from the first step affect the second one.",
        "queries": [
            -2.860774,
            4.597735
        ],
        "log": [
            {
                "reference": "Emotion-Cause Pair Extraction (ECPE), a task in text emotion analysis aims to extract pairs of emotions and their causes in a document. The existing strategy is a two-step process that first extracts individual emotion and cause sets, then pairs them. However, this doesn't directly aim at extracting pairs and errors from the first step affect the second one.",
                "distance": 0.0279
            },
            {
                "reference": "Emotion-cause pair extraction (ECPE) is a growing field where the goal is to extract emotion and corresponding cause clauses. Current methods typically encode features sequentially and specifically, first encoding the emotion and cause features for clause extraction and then combining them for pair extraction, leading to an imbalance in inter-task feature interaction.",
                "distance": 0.0417
            },
            {
                "reference": "Emotion-Cause Pair Extraction (ECPE) task involves extracting emotions and causes as pairs from documents. In the typical ECPE dataset, the relative distance distribution of emotions and causes is extremely imbalanced. Existing methods have fixed size windows to capture relations between neighboring clauses, but they neglect the effective semantic connections between distant clauses, resulting in poor generalization ability towards position-insensitive data.",
                "distance": 0.0465
            },
            {
                "reference": "Previous work on emotion-cause pair extraction operates in two steps: first, emotion clauses and cause clauses are extracted separately, then a classifier is trained to filter out negative pairs. These pipeline-style systems can suffer from error propagation and lack of adaptability between the two steps.",
                "distance": 0.0681
            },
            {
                "reference": "Relation classification is an important research area in the field of natural language processing, and there are existing methods for performing this task.",
                "distance": 0.0797
            },
            {
                "reference": "Existing relation classification, a key task in natural language processing, relies on lexical resources such as WordNet or specific NLP systems like a dependency parser or named entity recognizers to extract high-level features. An additional challenge is that necessary information can surface anywhere in a sentence.",
                "distance": 0.0896
            }
        ]
    },
    {
        "target": "Vision-Language Pre-training has shown its potential for learning generalizable visual representations from language supervision with the help of zero-shot recognition ability. However, the present practices suffer due to semantic gaps between the visual and textual modalities where many visual concepts in images are missing from their paired captions.",
        "prediction": "Video-Language Pretraining (VLP), which aims to learn transferable representations for various video-text tasks, has become increasingly popular. Most successful studies use large-scale, 3rd-person video-text datasets like HowTo100M.",
        "queries": [
            -1.791367,
            17.068192
        ],
        "log": [
            {
                "reference": "Video-Language Pretraining (VLP), which aims to learn transferable representations for various video-text tasks, has become increasingly popular. Most successful studies use large-scale, 3rd-person video-text datasets like HowTo100M.",
                "distance": 0.0049
            },
            {
                "reference": "Despite the success of large-scale vision-language pretraining (VLP) of dual-stream architectures like CLIP, these models are not capable of multimodal generative tasks due to a weak text encoder.",
                "distance": 0.0056
            },
            {
                "reference": "Recent progress in unsupervised vision-and-language pre-training (VLP) has reduced data collection costs and improved results compared to supervised VLP. However, existing methods rely on pre-extracted region-based visual features from external object detectors, sacrificing flexibility and computational efficiency.",
                "distance": 0.0127
            },
            {
                "reference": "Vision-Language Pre-training (VLP) has advanced many vision-language tasks, but most models excel in either understanding-based or generation-based tasks. Performance improvement has largely been achieved by scaling up datasets with noisy image-text pairs collected from the web, a suboptimal source of supervision.",
                "distance": 0.016
            },
            {
                "reference": "Vision and language pre-training (VLP) models have advanced the state-of-the-art results in a variety of cross-modal downstream tasks. Aligning cross-modal semantics is claimed to be one of the essential capabilities of VLP models, but their inner working mechanism of alignment remains unclear.",
                "distance": 0.0181
            },
            {
                "reference": "Vision-language (VL) pre-training has received significant attention. However, current end-to-end VL pre-training approaches primarily focus on either high-level image understanding tasks such as visual question answering and image captioning, or region-level tasks such as phrase grounding and object detection.",
                "distance": 0.0211
            },
            {
                "reference": "Advancements in vision-language pre-training (VLP) have shown impressive performance across a range of tasks. However, there are challenges in measuring progression towards general multi-modal intelligence, with current methods potentially overestimating the generalization capabilities of VLP models or overlooking the trade-off between efficiency and performance.",
                "distance": 0.0234
            },
            {
                "reference": "The task of Visual Document Understanding (VDU), which aims to understand documents in their varied formats and layouts, is challenging to tackle with existing methods.",
                "distance": 0.0251
            },
            {
                "reference": "Recent advances in Vision-Language Pretraining (VLP) have led to impressive performance on a variety of multimodal tasks. However, existing methods rely on expensive annotations such as clean image captions and regional labels, which hampers scalability and introduces complexity with multiple dataset-specific objectives.",
                "distance": 0.0265
            },
            {
                "reference": "Existing vision-language pre-training (VLP) methods primarily rely on paired image-text datasets, which are either annotated by enormous human labors or crawled from the internet followed by elaborate data cleaning techniques. However, there is a potential to reduce dependency on well-aligned image-text pairs and utilize large-scale text-only and image-only corpora.",
                "distance": 0.0271
            },
            {
                "reference": "Current Vision-Language (VL) modeling approaches require labeled data and utilise complex multi-step pretraining objectives, limiting their effectiveness.",
                "distance": 0.0273
            },
            {
                "reference": "Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most current methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance.",
                "distance": 0.0281
            },
            {
                "reference": "Image-based visual-language (I-VL) pre-training has been highly successful in learning joint visual-textual representations, showing significant potential for 'zero-shot' generalisation. However, adapting these pre-trained models efficiently for video understanding tasks has been a challenge.",
                "distance": 0.0281
            },
            {
                "reference": "In the current scenario, adversarial training for vision-and-language (V+L) representation learning is limited and its potential for enhancing performance in V+L tasks remains largely untapped.",
                "distance": 0.034
            },
            {
                "reference": "Contextual vision-and-language (V&L) models that have been pre-trained have shown impressive performance on various benchmarks. However, such models usually require a large amount of parallel image-caption data for efficient pre-training, which is costly and requires significant curation.",
                "distance": 0.0344
            },
            {
                "reference": "Vision-Language Pre-training (VLP) models, trained with contrastive loss on large amounts of image-text pairs, have shown impressive performance. However, in practical applications, data is usually collected in a streaming fashion, which requires continual learning techniques to avoid catastrophic forgetting.",
                "distance": 0.0379
            },
            {
                "reference": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a small set of manually-annotated data to understand the visual world. Better generalization performance has been observed with large-scale pretraining models like CLIP, which are trained on a massive amount of image-caption pairs.",
                "distance": 0.0409
            },
            {
                "reference": "Existing vision and language pre-training (VLP) methods have primarily focused on feature extraction and alignment between visuals and text, but overlooked the crucial impact of in-batch hard negative sampling for image-text matching (ITM) and large masking probabilities for masked language modeling (MLM).",
                "distance": 0.0421
            },
            {
                "reference": "Visual-linguistic pre-training (VLP) methods are effective in image-text retrieval tasks but suffer from inefficiencies when applied naively due to the modality incompleteness of the task, where the query is either an image or a text and not an image-text pair. Existing VLP methods fail to extract comparable representations for single-modal queries and multi-modal database items.",
                "distance": 0.0423
            },
            {
                "reference": "Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and-Language (VL) models, trained jointly on text and image or video data, have been offered as a response, but it is not yet known how those representations compare to text-only models.",
                "distance": 0.0458
            }
        ]
    },
    {
        "target": "Neural decoding and its applications to brain computer interfaces (BCI) rely on assigning action potentials (spikes) to individual neurons. Current spike sorting algorithms can be inaccurate and discard information that could improve decoding performance.",
        "prediction": "Understanding the brain mechanisms involved in reward prediction on different time scales is a complex task that hasn't been thoroughly addressed.",
        "queries": [
            7.125507,
            -5.081997
        ],
        "log": [
            {
                "reference": "Understanding the brain mechanisms involved in reward prediction on different time scales is a complex task that hasn't been thoroughly addressed.",
                "distance": 0.0435
            },
            {
                "reference": "Existing models of population codes provide information about a single value of an underlying quantity.",
                "distance": 0.0577
            },
            {
                "reference": "In the study of neural population decoding, the simultaneous estimation of stimulus in the previous step is typically not used as prior knowledge for consecutive decoding.",
                "distance": 0.0583
            },
            {
                "reference": "Population codes often rely on the tuning of the mean responses to the stimulus parameters, but this information can be greatly suppressed by long range correlations.",
                "distance": 0.0618
            },
            {
                "reference": "Most theoretical and empirical studies of population codes assume that the underlying neuronal activities denote a unique and unambiguous value of an encoded quantity. However, these population activities may contain additional information about multiple values or uncertainty about the quantity.",
                "distance": 0.0834
            },
            {
                "reference": "The relative merits of different population coding schemes have traditionally been analyzed in the framework of stimulus reconstruction using Fisher Information.",
                "distance": 0.0893
            },
            {
                "reference": "Dysfunctional synchronous activity of neurons in the brain reportedly cause many neurological diseases, such as Parkinson's. Current methods to suppress and control this activity rely on limited engineering trials due to the need to experiment with live human brains.",
                "distance": 0.0912
            },
            {
                "reference": "The challenge for computational neuroscience has been how the brain constructs statistical models of the sensory world, and the understanding of this process is not yet clear.",
                "distance": 0.0949
            },
            {
                "reference": "A central challenge in sensory neuroscience is understanding neural computations and circuit mechanisms that encode ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present an obstacle to the creation of accurate computational models of responses to natural stimuli.",
                "distance": 0.0971
            },
            {
                "reference": "Neural responses are variable and this variability can be modulated by sensory input and behaviour. Current methods for characterising neural variability, primarily aimed at sensory coding in the lab, require repeatable experimental stimuli and behavioural covariates and make strong assumptions about variability's parametric form.",
                "distance": 0.0985
            },
            {
                "reference": "Neural computations involving correlations of pulses across both space and time have been challenging due to their complexities.",
                "distance": 0.0986
            }
        ]
    },
    {
        "target": "In online social networks, link recommendations help users discover people they may know, potentially increasing engagement. However, adding links can also affect the level of conflict in the network, in terms of polarization and disagreement. There is little understanding of how the goals of high relevance and conflict reduction relate in link formation.",
        "prediction": "People face challenges and successes when piecing together multiple social media to interact in the online public sphere for purposes such as seeking and disseminating information, and engaging in political discussions. However, little is known about these challenges and successes.",
        "queries": [
            -20.438181,
            4.739
        ],
        "log": [
            {
                "reference": "People face challenges and successes when piecing together multiple social media to interact in the online public sphere for purposes such as seeking and disseminating information, and engaging in political discussions. However, little is known about these challenges and successes.",
                "distance": 0.0386
            },
            {
                "reference": "The phenomenon of opinion formation in online social networks is quite prevalent, and the exposure of users to viral content, such as breaking news articles, plays a significant role. However, the correlation between these two aspects in online social networks has not been quantified.",
                "distance": 0.0571
            },
            {
                "reference": "Viral marketing techniques that use pre-existing social networks are experiencing significant encouragement. Twitter is the most studied social network in viral marketing, with rumor spread being a widely researched problem.",
                "distance": 0.07
            },
            {
                "reference": "A notable dynamic property of social media is burst behavior. However, the relationship between the structure of fluctuation and the successive burst response remains relatively unstudied.",
                "distance": 0.087
            },
            {
                "reference": "Increasing information flow through social media causes network members to compete for attention and influence by relying on others to spread their message. Meanwhile, a study of Twitter's information propagation reveals the majority of users are passive information consumers and don\u2019t forward content.",
                "distance": 0.0922
            },
            {
                "reference": "The objective of viral marketing is to utilize existing social interactions between customers for world\u2010of\u2010mouth advertising of products. Effective marketing strategies require an understanding of how influence is propagated across such social networks.",
                "distance": 0.0933
            }
        ]
    },
    {
        "target": "Competitive reinforcement learning (RL) in zero-sum Markov games (MGs) faces challenges when dealing with general function approximations and trading off between exploration and exploitation under self-play and adversarial settings.",
        "prediction": "In typical reinforcement learning problem set-ups, decision processes are stationary across episodes, which does not mirror the persistent change we experience in real-world settings. Off-policy algorithms that replay past experiences when learning have been particularly challenging to extend to non-stationary settings.",
        "queries": [
            8.735113,
            -21.528652
        ],
        "log": [
            {
                "reference": "In typical reinforcement learning problem set-ups, decision processes are stationary across episodes, which does not mirror the persistent change we experience in real-world settings. Off-policy algorithms that replay past experiences when learning have been particularly challenging to extend to non-stationary settings.",
                "distance": 0.0717
            },
            {
                "reference": "The problem of continuous state, continuous action batch reinforcement learning where the goal is to learn an optimized policy from a rich generated trajectory is well-studied. In such setting, traditional fitted Q-iteration is usually employed for action value optimization.",
                "distance": 0.0724
            },
            {
                "reference": "Recent research on reinforcement learning has been focused on algorithms based on Dynamic Programming (DP), particularly for the control of dynamical systems. However, there are no convergence proofs for problems with continuous state and action spaces, or for systems involving non-linear function approximators.",
                "distance": 0.0795
            },
            {
                "reference": "In reinforcement learning with continuous state and action spaces under a generative model, learning the $Q$-function in a sample efficient manner is important. If $Q$-function is Lipschitz continuous, the sample complexity for estimating an $\\epsilon$-optimal $Q$-function scales as ${\\Omega}(\frac{1}{\\epsilon^{d_1+d_2 +2}})$, where $d_1$ and $d_2$ represent the dimensions of the state and action spaces respectively.",
                "distance": 0.0892
            },
            {
                "reference": "Q-learning is a widely used reinforcement learning algorithm, but in multi-agent scenarios, it may fail to reach Nash equilibria, which are the optimal set of actions for the agents. Standard Q-learning performs sequential value function updates.",
                "distance": 0.0966
            }
        ]
    },
    {
        "target": "Continual learning (CL) aims to train deep neural networks on streaming data whilst limiting the forgetting of new tasks, but this is quite difficult due to interference between tasks. Moreover, the real-world deployment of CL models is restricted due to their inability to measure predictive uncertainties.",
        "prediction": "Existing methods for distinguishing instances of the same object category have exhibited the problem of catastrophic forgetting.",
        "queries": [
            -2.080346,
            -6.728468
        ],
        "log": [
            {
                "reference": "Existing methods for distinguishing instances of the same object category have exhibited the problem of catastrophic forgetting.",
                "distance": 0.0099
            },
            {
                "reference": "Online continual learning, where a model learns from a continuous stream of data without revisiting previously encountered data instances, is a challenging scenario. Catastrophic forgetting is worsened in this scenario as the model has to address forgetting at both the task-level and the data instance-level within the same task.",
                "distance": 0.0163
            },
            {
                "reference": "Continual Learning (CL) is a process that sequentially learns new tasks, aiming for better Stability (remembering past tasks) and Plasticity (adapting to new tasks). However, due to the absence of past training data, the difference in the influence on Stability and Plasticity among training examples has not been explored adequately.",
                "distance": 0.0216
            },
            {
                "reference": "Continual learning (CL) is a setting in which a model learns from a stream of incoming data without forgetting previously learned knowledge. Pre-trained language models (PLMs) have been applied successfully in continual learning of different natural language problems. Moreover, an array of continual learning methods and PLMs are undergoing rapid development.",
                "distance": 0.0219
            },
            {
                "reference": "Continual learning (CL) is a crucial problem in the field of machine learning, where agents have to learn from a sequentially incoming stream of data, solving new problems over time while retaining previous knowledge. However, existing solutions face challenges of catastrophic forgetting (CF), high memory costs, and lack of theoretical understanding of neural networks' behavior when learning new tasks.",
                "distance": 0.0224
            },
            {
                "reference": "Continual learning is a recent challenge in machine learning where models train well on the most recent data but face catastrophic forgetting of previous data due to shifting distributions, a problem that current replay-based methods attempt to solve with a small historical replay buffer.",
                "distance": 0.0225
            },
            {
                "reference": "Continual learning (CL) aims to learn a sequence of tasks without forgetting previously acquired knowledge. However, recent advances in CL are limited to supervised continual learning (SCL) scenarios, which are not scalable to real-world applications because data in such scenarios is often biased and unannotated.",
                "distance": 0.0262
            },
            {
                "reference": "Catastrophic forgetting is a challenge in neural networks where the system 'forgets' previously learned knowledge when learning new tasks. Existing methods have primarily focused on overcoming this issue in convolutional neural networks (CNNs), which handle grid-based data like images, while largely overlooking graph neural networks (GNNs) that deal with non-grid data.",
                "distance": 0.0286
            },
            {
                "reference": "Catastrophic forgetting (CF) frequently occurs when learning with non-stationary data distribution. CF is a significant challenge in sequential domain meta-learning (SDML), a process that involves meta-learning on a sequence of domains (datasets) but remains nearly unexplored.",
                "distance": 0.0298
            },
            {
                "reference": "Continual Learning (CL), which refers to learning multiple tasks sequentially without forgetting previous knowledge, remains a long-standing challenge for neural networks. Existing methods typically rely on adding network capacity or data replay.",
                "distance": 0.0312
            },
            {
                "reference": "Most research in continual learning has focused on mitigating catastrophic forgetting in feed-forward classification networks. However, the area of continual learning of recurrent models applied to problems like image captioning has been given little attention.",
                "distance": 0.0343
            },
            {
                "reference": "Common architectures in object detection are vulnerable to catastrophic forgetting, where they forget what has been learned when updating their parameters in the absence of the original training data. Existing works have extended standard classification methods in object detection task, often by adopting the knowledge distillation framework.",
                "distance": 0.0347
            },
            {
                "reference": "The problem of catastrophic forgetting in the context of class-incremental learning for video recognition hasn't been actively explored, though continual learning is popular.",
                "distance": 0.0388
            },
            {
                "reference": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning (learning tasks sequentially), especially with the rapid progress in computer vision and natural-language processing using large-scale pretrained models.",
                "distance": 0.0388
            },
            {
                "reference": "Deep neural networks suffer from catastrophic forgetting when learning incrementally, making it difficult to apply them in many tasks. Current methods mitigate this by maintaining a handful of samples (exemplars) from each task, but these are often too few to hold enough task-specific knowledge, keeping the forgetting problem persistent.",
                "distance": 0.039
            },
            {
                "reference": "Continual Learning (CL) methods mainly aim to avoid catastrophic forgetting and learn representations transferable to new tasks. SupSup, a CL method proposed by Wortsman et al. (2020), uses a fixed base network and finds a supermask for each new task to selectively keep or remove each weight to produce a subnetwork. However, the performances of the supermask are sub-optimal due to the fixed weights' restricted representational power and no knowledge transfer when new tasks are learned.",
                "distance": 0.0391
            },
            {
                "reference": "Continual learning methods are needed that learn from data, while minimizing memory footprint and power consumption. While memory replay techniques have been promising in addressing this task, the best method for deciding which buffered images to replay is still a debated question.",
                "distance": 0.0397
            },
            {
                "reference": "Continual learning is a growing paradigm in machine learning where a model, exposed to data from multiple different distributions in an online manner, is expected to adapt to changes in the distribution without increasing the model size. However, this setup has drawn attention primarily in applied studies, without formalization of its theoretical guarantees.",
                "distance": 0.0399
            },
            {
                "reference": "Catastrophic forgetting is a common problem in neural networks, where the network loses information about the first task after being trained on a second task.",
                "distance": 0.0408
            },
            {
                "reference": "Classical deep neural networks suffer from sharp performance degradation when trained sequentially on new or evolving tasks, also known as catastrophic forgetting. Existing solutions either store old data samples or only update a parameter set of deep neural networks, but these approaches demand a large memory budget or limit the flexibility of models to learn incrementally.",
                "distance": 0.0411
            }
        ]
    },
    {
        "target": "Existing methods for the unsupervised learning of constructive solid geometry (CSG) representations of 3D CAD shapes lack compactness and do not efficiently handle complex and high-genus CAD shapes.",
        "prediction": "Amorphous shapes have always posed challenges to CG modelers due to their complex and hard-to-retrieve topology. Fractals represent one of the most viable solutions for this, but their application in 3D domain is rare. Mostly, fractal objects are generated through quaternion representations for nonlinear systems in four dimensions.",
        "queries": [
            19.322134,
            -3.299105
        ],
        "log": [
            {
                "reference": "Amorphous shapes have always posed challenges to CG modelers due to their complex and hard-to-retrieve topology. Fractals represent one of the most viable solutions for this, but their application in 3D domain is rare. Mostly, fractal objects are generated through quaternion representations for nonlinear systems in four dimensions.",
                "distance": 0.0377
            },
            {
                "reference": "Automatically matching 3D objects to features derived from range images in the presence of uncertainty is a challenging task, with no clear solutions recorded in the literature.",
                "distance": 0.0415
            },
            {
                "reference": "Current generative models for 3D shapes struggle with achieving diversity and high-quality results, often due to the complexity of searching in high-resolution 3D grid spaces.",
                "distance": 0.0516
            },
            {
                "reference": "The aspect graph concept, as traditionally conceived, is limited in its applicability to objects with articulated connections between their rigid parts.",
                "distance": 0.0573
            },
            {
                "reference": "The problem of recovering deformable object class models from unordered images is challenging, especially when each image is of a different object instance with unique surface texture, which does not result in usable image point correspondences.",
                "distance": 0.0654
            },
            {
                "reference": "Existing literature often struggles to effectively model non-rigid objects from natural images.",
                "distance": 0.0791
            },
            {
                "reference": "The existing diffusion models used for 3D point cloud synthesis aren't suited for the needs of digital artists due to their limitations in generation quality, limited flexibility for manipulation and application such as conditional synthesis and shape interpolation, and the inability to output smooth surfaces or meshes.",
                "distance": 0.0842
            },
            {
                "reference": "Existing data-driven shape completion methods lack the training data and representation capacity to synthesize fine-grained details with complex geometry and topology in missing region of 3D shapes.",
                "distance": 0.0976
            },
            {
                "reference": "Neural representations for static 3D shapes are widely studied, but representations for deformable surfaces are either dependent on templates or lack efficiency.",
                "distance": 0.0978
            },
            {
                "reference": "Traditional aspect graphs are topology-based and are impractical for articulated objects.",
                "distance": 0.0993
            }
        ]
    },
    {
        "target": "Regular expressions (REs) are widely used for many networking tasks like network intrusion detection. However, they completely depend on expert knowledge and can't utilize labeled data for better accuracy. Neural networks (NNs), though capable of learning from labeled data and providing better accuracy, are ill-suited in cold-start scenarios and too complex for deployment on network devices.",
        "prediction": "Traditional RNNs update the entire hidden state for each input token, causing significant computational cost and latency.",
        "queries": [
            7.388899,
            -7.397387
        ],
        "log": [
            {
                "reference": "Traditional RNNs update the entire hidden state for each input token, causing significant computational cost and latency.",
                "distance": 0.051
            },
            {
                "reference": "Complex numbers have long been favoured for digital signal processing, but complex representations rarely appear in deep learning architectures, including Recurrent Neural Networks (RNNs), which are widely used to process time series and sequence information.",
                "distance": 0.0577
            },
            {
                "reference": "There is an ongoing effort to seek biologically-plausible approximations of gradient descent algorithms for training recurrent neural networks (RNNs). However, the question is, how these learning rules affect the generalization compared to nonbiologically-plausible ones.",
                "distance": 0.0599
            },
            {
                "reference": "The difficulty of training recurrent neural networks (RNNs) is widely recognized, specifically problems related to vanishing and exploding gradients, as detailed in Bengio et al. (1994).",
                "distance": 0.0648
            },
            {
                "reference": "Current recurrent neural network (RNN) models struggle with learning long time dependencies due to the problem of exploding and vanishing gradients.",
                "distance": 0.0684
            },
            {
                "reference": "Deep learning architectures rarely utilize complex number representations, despite their popularity in digital signal processing. This is also the case for Recurrent Neural Networks (RNNs), which are widely used for processing time series and sequence information.",
                "distance": 0.0694
            },
            {
                "reference": "Learning long-term sequential dependencies is a challenge for gradient-based recurrent sequential learning methods due to the well-known exploding and vanishing gradients problem.",
                "distance": 0.0844
            },
            {
                "reference": "Vanishing and exploding gradients are among the main obstacles when training deep neural networks, particularly in capturing long-range dependencies in recurrent neural networks (RNNs).",
                "distance": 0.088
            },
            {
                "reference": "Training recurrent neural networks (RNNs) often face challenges related to stability.",
                "distance": 0.0955
            },
            {
                "reference": "Training deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies is challenging due to the vanishing or exploding gradient problem. One approach to address these problems is to use either soft or hard constraints on weight matrices to encourage or enforce orthogonality.",
                "distance": 0.0985
            },
            {
                "reference": "RNNs have challenges related to parameter-space geometry, affecting their learnability, particularly those with ReLU activations, and despite recent suggestions on initialization schemes, these challenges remain.",
                "distance": 0.0988
            }
        ]
    },
    {
        "target": "In knowledge-grounded dialogue generation, two sequential modules: a knowledge retriever and a response generator are used. These modules are usually trained separately and obtaining intermediate labels for ground-truth knowledge is difficult and expensive, especially in open-domain conversations.",
        "prediction": "The task of adding ISO standard 24617-2 annotations to the dialogues in the Switchboard corpus, leveraging the existing SWBD-DAMSL annotations presents several issues.",
        "queries": [
            -6.469256,
            12.813562
        ],
        "log": [
            {
                "reference": "The task of adding ISO standard 24617-2 annotations to the dialogues in the Switchboard corpus, leveraging the existing SWBD-DAMSL annotations presents several issues.",
                "distance": 0.0227
            },
            {
                "reference": "In open-domain generative dialogue systems, the issue of generating generic responses is often addressed by incorporating commonsense knowledge. However, selecting the appropriate knowledge facts for the dialogue context remains a challenge, with the widely used Entity Name Matching approach often retrieving irrelevant facts.",
                "distance": 0.0346
            },
            {
                "reference": "Dialogue agents use external textual knowledge to generate quality responses. However, most existing works in knowledge grounded dialogue settings assume that user intentions are always answerable, which is not always the case as the knowledge retriever may not always retrieve the desired knowledge.",
                "distance": 0.0386
            },
            {
                "reference": "Current neural dialog models often condition dialog generation on externally retrieved knowledge",
                "distance": 0.0394
            },
            {
                "reference": "Dialogue generation models can be improved with the injection of relevant knowledge, however, previous research in extending generative models with relevant knowledge has not explicitly assessed the quality of this knowledge.",
                "distance": 0.0411
            },
            {
                "reference": "Existing methods for conversation disentanglement have two main limitations. They overemphasize the pairwise relations between utterances and undervalue the relation between an utterance and its context. Moreover, these methods require a large amount of annotated data for training, which is difficult and costly to obtain.",
                "distance": 0.045
            },
            {
                "reference": "The current mainstream approach to train natural language systems is to expose them to large amounts of text, a passive learning approach which may not be suitable for developing interactive machines, such as conversational agents.",
                "distance": 0.05
            },
            {
                "reference": "Previous methodologies for learning a disambiguation model for deep pragmatic interpretations in the context of situated task-oriented dialogue require human annotation.",
                "distance": 0.0506
            },
            {
                "reference": "Knowledge selection is critical in knowledge-grounded dialogue for generating informative responses. Although latent variable models have been proposed to manage the diversity of knowledge selection, a large gap exists between prior and posterior knowledge selection in these models, resulting in insufficient knowledge selection and exposure bias.",
                "distance": 0.0534
            },
            {
                "reference": "As dialogue systems become more prevalent in roles like personalized assistants or learning companions, there is an increasing need for these systems to be able to socially engage the user. One observed phenomenon in human dialogue that is associated with social engagement, rapport and task success is acoustic-prosodic entrainment, where speakers adjust their speech features to match each other.",
                "distance": 0.0563
            },
            {
                "reference": "Interpreting the communicative intents of an utterance often requires grounding it in something outside of language or in world modalities. However, the interpretation of dialogue clarification mechanisms, which ground speaker's utterances in various modalities of the dialogue, is an area needing more research attention.",
                "distance": 0.068
            },
            {
                "reference": "Utterances referring to other utterances frequently appear in task-oriented dialogues, such as those arising in the context of a natural language interface to an operating system, but existing frameworks struggle with processing these meta-language utterances.",
                "distance": 0.0727
            },
            {
                "reference": "Knowledge is crucial for intelligent conversation systems to generate useful responses. However, current approaches struggle with limited abilities in understanding language and utilizing different types of knowledge. Some researchers try to enhance models\u2019 language comprehension ability through pre-trained language models, yet they overlook the importance of external knowledge in specific tasks.",
                "distance": 0.0775
            },
            {
                "reference": "In dialogues, repetitions, responses that repeat words from a previous speaker's utterance, are important for building trust. However, the generation of such repetitions has not been addressed by neural approaches prior to this study.",
                "distance": 0.0779
            },
            {
                "reference": "Currently, research in grounding dialogue generation in extra knowledge focuses primarily on synthesizing a response with appropriate knowledge, largely overlooking the fact that the same information can be expressed differently by speakers in the same context.",
                "distance": 0.0788
            },
            {
                "reference": "Current models for document-grounded response generation in dialog often struggle with maintaining factuality.",
                "distance": 0.0797
            },
            {
                "reference": "Task-solving in dialogue depends on the linguistic alignment of the interlocutors, according to Pickering & Garrod (2004), which is assumed to be based on mechanistic repetition effects.",
                "distance": 0.0854
            },
            {
                "reference": "In knowledge-grounded conversations, current models lack fine-grained control over knowledge selection and integration with dialogues. This leads to problems with knowledge-irrelevant response generation, over-fitting during training, and overlooking selected knowledge in generated responses.",
                "distance": 0.0902
            },
            {
                "reference": "In multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns can offer insights for knowledge selection, a factor often overlooked in previous research.",
                "distance": 0.0903
            },
            {
                "reference": "Existing dialogue generation models commonly encounter issues such as topic drift, and typically use word or sentence level similarities to detect relevant contexts, but fail to capture relevance at the topical level.",
                "distance": 0.092
            }
        ]
    },
    {
        "target": "In building a reliable autonomous driving system, trajectory prediction is a critical task that aims to anticipate possible dangers. However, generating consistent trajectory predictions without collision is a key challenge.",
        "prediction": "In vehicle ego-motion forecasting, achieving a balance between diversity (producing most likely paths) and precision (mostly producing likely paths) is a challenge. Existing methods fail to achieve this balance and there are issues with evaluation metrics used previously.",
        "queries": [
            10.789493,
            2.542503
        ],
        "log": [
            {
                "reference": "In vehicle ego-motion forecasting, achieving a balance between diversity (producing most likely paths) and precision (mostly producing likely paths) is a challenge. Existing methods fail to achieve this balance and there are issues with evaluation metrics used previously.",
                "distance": 0.0045
            },
            {
                "reference": "Behavior prediction in dynamic, multi-agent systems is critical for self-driving cars, but it's challenging due to the complex representations and interactions of road components. Most current approaches render trajectories of moving agents and road context information as bird-eye images, and encode them with convolutional neural networks (ConvNets), which can be computationally intensive and may cause lossy rendering.",
                "distance": 0.0069
            },
            {
                "reference": "The motion prediction of surrounding vehicles is difficult in autonomous driving planning due to the interaction and uncertainty in driving behaviors. Traditional prediction methods typically rely solely on historical information and are decoupled from planning.",
                "distance": 0.0237
            },
            {
                "reference": "For autonomous systems such as self-driving cars, predicting futures of surrounding agents is essential. An object-agnostic Sequential Pointcloud Forecasting (SPF) task has been proposed, which enables a forecast-then-detect pipeline for effective detection and trajectory prediction. However, previous work could only forecast a deterministic sequence of future point clouds, notwithstanding the uncertainty inherent in dynamic scenes.",
                "distance": 0.0239
            },
            {
                "reference": "In autonomous driving, trajectory prediction is crucial, and state-of-the-art models usually employ attention mechanisms to model interactions between agents.",
                "distance": 0.0291
            },
            {
                "reference": "In autonomous vehicle technology, it's crucial to anticipate the stochastic future trajectories of other agents such as pedestrians and other cars. While normalizing flows have become an attractive tool to model these multi-modal distributions, their independent samples often fail to adequately capture all modes in the underlying distribution.",
                "distance": 0.0351
            },
            {
                "reference": "Accurate prediction of future motions of traffic agents is crucial for autonomous vehicles. While vectorized approaches have shown capability in capturing complex interactions in traffic scenes, they suffer from high computational cost and ignoring the symmetries of the problem, making real-time, multi-agent motion prediction challenging without sacrificing performance.",
                "distance": 0.0386
            },
            {
                "reference": "Trajectory prediction for surrounding traffic is essential for autonomous driving but predicting multiple plausible trajectories is challenging. Proposal-based methods have traditionally addressed this through a two-stage approach of intention classification and motion regression.",
                "distance": 0.0453
            },
            {
                "reference": "Goal-based multi-trajectory prediction methods have been proven effective in predicting future trajectories of road agents for autonomous driving. However, these methods typically involve goal predictions based on sparse pre-defined anchors and heuristic goal selection algorithms.",
                "distance": 0.049
            },
            {
                "reference": "Current end-to-end autonomous driving methods either run a controller based on a planned trajectory or perform control prediction directly, which have resulted in two separately studied lines of research.",
                "distance": 0.0524
            },
            {
                "reference": "The trajectory planning in autonomous driving largely depends on sampling-based motion planning (SBMP), with the sampling strategy being key to finding real-time, smooth, collision-free trajectories. While existing bias sampling strategies utilized in literature have been explored to speed up SBMP, they may cause sharp lane changing.",
                "distance": 0.053
            },
            {
                "reference": "Current trajectory prediction methods, crucial for autonomous systems like self-driving vehicles, do not enforce scene consistency leading to self-collisions between predicted trajectories of different agents. Moreover, they generate individual predictions per agent instead of joint predictions making downstream planning difficult.",
                "distance": 0.0542
            },
            {
                "reference": "Existing solutions for predicting multimodal future behavior of traffic participants for robotic vehicles either use latent features to predict trajectories, or rely on dense goal candidates to identify agents' destinations. The former strategy converges slowly because all motion modes are derived from the same feature, while the latter has efficiency issues because its performance heavily depends on the density of goal candidates.",
                "distance": 0.0659
            },
            {
                "reference": "Driving behavior models typically rely on traditional syntactic pattern recognition, utilizing fixed-rate sampling of signal values from onboard sensors, which can result in complex models and difficult learning problems.",
                "distance": 0.0689
            },
            {
                "reference": "Automotive surround view camera system is an emerging automotive ADAS (Advanced Driver Assistance System) technology that assists the driver in parking the vehicle safely by providing a 360 degree view of the vehicle's surroundings, usually synthesized from the input of four to six wide-angle cameras.",
                "distance": 0.0693
            },
            {
                "reference": "In self-driving, predicting the future locations and movements of agents around a vehicle is important for planning. Recently, there has been a trend to fuse data from multiple cameras into a bird's-eye view for these predictions, but the quality of predictions decreases over time due to the uncertainty and multiple feasible predictions.",
                "distance": 0.0713
            },
            {
                "reference": "Trajectory prediction is essential for autonomous vehicles to plan correct and safe driving behaviors. While many prior works aim to achieve higher prediction accuracy, few study the adversarial robustness of their methods.",
                "distance": 0.0714
            },
            {
                "reference": "Trajectory prediction, which is crucial for autonomous driving, is challenging due to the need to analyze agents' past movements, social interactions among a variety of agents, scene context constraints, and human behavior unpredictability.",
                "distance": 0.0763
            },
            {
                "reference": "Numerous methods for scene representation exist, but often struggle with efficiently and accurately generating both feasible and plausible trajectories for agents in different types of environments.",
                "distance": 0.0772
            },
            {
                "reference": "Forecasting the motion of surrounding vehicles is crucial for an autonomous vehicle deployed in complex traffic where the motion of all vehicles in a scene is determined by the traffic context including the motion and relative spatial configuration of neighboring vehicles.",
                "distance": 0.0786
            }
        ]
    },
    {
        "target": "Deep neural networks often produce overconfident yet erroneous predictions when faced with unfamiliar data, highlighting the need for effective out-of-distribution (OOD) detection systems.",
        "prediction": "Out-of-distribution example detection is crucial in safety-critical machine learning applications, but prior research has predominantly focused on simple, small-scale settings.",
        "queries": [
            -4.731114,
            -19.392017
        ],
        "log": [
            {
                "reference": "Out-of-distribution example detection is crucial in safety-critical machine learning applications, but prior research has predominantly focused on simple, small-scale settings.",
                "distance": 0.0106
            },
            {
                "reference": "Deep neural networks can be easily biased towards undesirable features, resulting in poor generalization. There have been efforts to de-bias these features using out-of-distribution (OOD) examples, and a recent study demonstrates successful undesirable feature removal at the softmax layer. However, this approach is confined to classification tasks and does not sufficiently explore its impact on a DNN feature extractor.",
                "distance": 0.02
            },
            {
                "reference": "Deep neural networks perform well with in-distribution data but can significantly degrade when applied to out-of-distribution (OoD) data. Current methods for OoD detection often rely on training or tuning with both in-distribution and out-of-distribution data, which can be difficult to define a-priori and may lead to bias in learning.",
                "distance": 0.0229
            },
            {
                "reference": "Deep learning systems frequently fail at out-of-context (OOC) prediction, that is predicting on uncommon, unusual or distinctive subgroups of the training distribution. There have been recent introductions of benchmarks for measuring OOC performance.",
                "distance": 0.0357
            },
            {
                "reference": "The use of deep neural networks (DNNs) in real-world applications is hindered because of their poor performance on out-of-distribution (OOD) data as compared to in-distribution (ID) data. Several promising approaches for improving OOD robustness, including model pruning, data augmentation, and ensembling, have been developed, but there isn't a clear understanding of the conditions on OOD data and model properties that lead to effective robustness.",
                "distance": 0.0582
            },
            {
                "reference": "Deep learning methods are crucial for commercially important applications such as autonomous driving and medical diagnostics. However, reliably detecting out-of-distribution (OOD) inputs while employing these algorithms is a challenge.",
                "distance": 0.0624
            },
            {
                "reference": "Overconfident predictions on out-of-distribution (OOD) samples is a problem for deep neural networks. Current strategies seek to build a subset of OOD samples and then suppress predictions on them.",
                "distance": 0.0649
            },
            {
                "reference": "Discriminative neural networks often perform poorly on data that is not generated by the same process as the training distribution, especially in applications such as bacteria identification based on genomic sequences. Thus, there is a need for a model that can confidently handle out-of-distribution (OOD) inputs.",
                "distance": 0.0692
            },
            {
                "reference": "The problem of distinguishing between in-distribution (sample from the same distribution as training data) and out-of-distribution (distinct samples) arises often in machine learning applications but is not handled well by current deep neural networks, which tend to be overconfident in their predictions. Prior works attempted to manage this issue with inference procedure enhancements and threshold-based detectors; however, their performance depends heavily on the training of classifiers.",
                "distance": 0.0713
            },
            {
                "reference": "Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data.",
                "distance": 0.0718
            },
            {
                "reference": "Explanation Regularization (ER) is a method designed to improve language model generalization by aligning machine rationales with human rationales. However, its effect on out-of-distribution (OOD) generalization has been largely unexplored thus far.",
                "distance": 0.0721
            },
            {
                "reference": "Deep generative models (DGMs) have been observed to assign higher probabilities or densities to out-of-distribution (OOD) images than images from the training distribution, indicating a failure in OOD detection.",
                "distance": 0.0772
            },
            {
                "reference": "Out-of-distribution (OOD) detection is crucial for decision making processes and is commonly done through deep ensembles where the variance of predictions over different neural networks substitutes for input uncertainty. However, understanding the inductive biases leading to the performance of deep ensemble's uncertainty estimation is still not investigated.",
                "distance": 0.0786
            },
            {
                "reference": "Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem in higher-level cognition research.",
                "distance": 0.079
            },
            {
                "reference": "Domain generalization techniques aim to improve generalization capabilities of machine learning systems to out-of-distribution (OOD) data, often by tackling issues arising from such data in stationary and discrete environments. However, real-world tasks in non-stationary environments often involve complex, continuously evolving domain drift that poses new challenges.",
                "distance": 0.0812
            },
            {
                "reference": "There is a growing concern about neural networks' sensitivity to distribution shifts, particularly for out-of-distribution (OOD) generalization. However, most of the research in this domain focuses on Euclidean data, leaving the understanding and handling of distribution shifts on graph-structured data largely unexplored.",
                "distance": 0.0835
            },
            {
                "reference": "For safety-critical applications of deep neural networks, it is crucial to determine when new inputs significantly deviate from the training distribution. This task is known as out-of-distribution (OOD) detection.",
                "distance": 0.0862
            },
            {
                "reference": "Out-of-Distribution (OoD) generalization algorithms, which deal with distributional shifts are gaining attention but often ignore the variation in the quality of training data, which can harm their accuracy.",
                "distance": 0.0866
            },
            {
                "reference": "There often is a dilemma between ease of optimization and robust out-of-distribution (OoD) generalization in machine learning. Many OoD methods rely on penalty terms whose optimization is challenging, leading to them being either too strong to optimize reliably or too weak to achieve their goals.",
                "distance": 0.0868
            },
            {
                "reference": "Deep learning models often struggle with out-of-distribution (OoD) generalization, where the test data come from a different distribution than the training data. Designing a general out-of-distribution generalization framework for a wide range of applications is a challenging task because of the various kinds of distribution shifts encountered in real-world situations.",
                "distance": 0.0878
            }
        ]
    },
    {
        "target": "In the field of technical human-computer interaction (HCI), there exists a community of researchers who innovate in hardware, exploring new device form factors, sensing methods, actuation, and displays. However, developing hardware is considered difficult, often requiring different disciplines, involvement of third parties such as parts suppliers and manufacturers, and it is generally costlier than software-based activities.",
        "prediction": "The abstract suggests that the world of research, particularly in HCI and computer science, is not exclusively focused on traditional areas of study but also involves artistic and creative practices, which form an integral part of the identities and interests of practitioners.",
        "queries": [
            -17.635155,
            11.401409
        ],
        "log": [
            {
                "reference": "The abstract suggests that the world of research, particularly in HCI and computer science, is not exclusively focused on traditional areas of study but also involves artistic and creative practices, which form an integral part of the identities and interests of practitioners.",
                "distance": 0.0158
            },
            {
                "reference": "The Human-Computer Interaction (HCI) community has been actively seeking new methods to engage people with data beyond the conventional visualizations on screen.",
                "distance": 0.0465
            },
            {
                "reference": "There is a growing interest in Human-Computer Interaction (HCI) research to explore cross-device interaction, focusing on different approaches to facilitate interaction between handheld devices and large displays.",
                "distance": 0.0476
            },
            {
                "reference": "Human-Computer Interaction (HCI) in outdoor recreation is a growing research area but lacks a cohesive agenda for understanding this application domain.",
                "distance": 0.0644
            },
            {
                "reference": "Human-Computer Interaction (HCI) research has recently been focused on transient, non-digital methods of displaying information in public places, such as chalk infographics. Despite their advantages, such as being easy to deploy, easy to interact with, and more sustainable, they have the limitation of only having local scale and impact.",
                "distance": 0.0647
            },
            {
                "reference": "Since 2000, Human Computer Interaction (HCI) has turned to the artistic, focusing more on provocative, cultural and social experiences. Much of this work engages the public in the spectacle of interactive experiences.",
                "distance": 0.0782
            },
            {
                "reference": "While many HCI Designers and Researchers are increasingly designing tools for health and wellbeing, few have a background in human anatomy and physiology.",
                "distance": 0.0834
            },
            {
                "reference": "In the field of Human-Computer Interaction (HCI), creative interactions sometimes involve the human body, persuasive technologies, and intimate interactions with anthropomorphized devices for which consent methodologies have not been clearly defined or integrated.",
                "distance": 0.088
            },
            {
                "reference": "HCI as a broadly pragmatic, experience-centered, and participant-focused field is well placed to innovate methods that invite first-hand interaction and experience with speculative design projects.",
                "distance": 0.0928
            },
            {
                "reference": "Iteration is a central feature of most HCI design methods, creating as it does opportunities for engagements with stakeholder groups. However, the demands that iteration places on these groups and the conditions under which iterative engagements arise are questions with so far unclear answers.",
                "distance": 0.0971
            }
        ]
    },
    {
        "target": "Arguments often do not explicitly illustrate how a conclusion follows from its premises, leading to gaps in understanding. Current knowledge extraction methods lack contextual awareness, limiting their usefulness for augmenting argumentation.",
        "prediction": "Enthymemes, arguments with missing premises, are common in natural language text. They are challenging for the field of argument mining, which aims to extract arguments from these texts. The current practice is either to fill the missing premise from similar/related arguments or discard such enthymemes altogether and focus on complete arguments.",
        "queries": [
            -1.800553,
            3.930421
        ],
        "log": [
            {
                "reference": "Enthymemes, arguments with missing premises, are common in natural language text. They are challenging for the field of argument mining, which aims to extract arguments from these texts. The current practice is either to fill the missing premise from similar/related arguments or discard such enthymemes altogether and focus on complete arguments.",
                "distance": 0.0454
            },
            {
                "reference": "The ability to generate effective evaluative arguments is critical for systems intended to advise and persuade their users but there is a lack of understanding on how user tailoring can impact argument effectiveness.",
                "distance": 0.0479
            },
            {
                "reference": "Existing argument generation systems in debating often lack structure and coherence, leaving room for improved user interaction and system output.",
                "distance": 0.0537
            },
            {
                "reference": "Research in argumentation mining lacks a standardized dataset and evaluation method for comparing and evaluating systems.",
                "distance": 0.0552
            },
            {
                "reference": "The existing work lacks methods for identifying argumentation schemes such as premises, conclusions, and names of argumentation schemes in scientific claims in genetics research articles.",
                "distance": 0.0772
            },
            {
                "reference": "Argumentation in a scientific article is composed of unexpressed and explicit statements of old and new knowledge combined into a logically coherent textual argument. A biomedical relation exhibits a relationship between biomedical entities.",
                "distance": 0.0853
            },
            {
                "reference": "Online debates often result in argumentative discussions where generally accepted arguments emerge, but identifying these prominent arguments in an unsupervised manner remains a challenge.",
                "distance": 0.0862
            }
        ]
    },
    {
        "target": "Recommending a variety of product types is important for a good shopping experience when customers are looking for products around their high-level shopping interests. However, connecting shopping interests with product types is absent in e-commerce product catalogs and is expensive to construct manually due to the volume of potential shopping interests.",
        "prediction": "Product pages on e-commerce websites provide a wealth of data which can overwhelm customers, making discovery of relevant information challenging.",
        "queries": [
            -14.957547,
            3.264146
        ],
        "log": [
            {
                "reference": "Product pages on e-commerce websites provide a wealth of data which can overwhelm customers, making discovery of relevant information challenging.",
                "distance": 0.0689
            },
            {
                "reference": "Online shopping availability of a wide range of products necessitates sophisticated techniques to help users navigate and explore the available items.",
                "distance": 0.0777
            },
            {
                "reference": "On e-commerce platforms, predicting product compatibility is essential for effective product recommendation and search experience. However, the heterogeneous product data and lack of manually curated training data make accurate prediction challenging.",
                "distance": 0.0812
            },
            {
                "reference": "The lack of information and references about a product often leads to frustration after the acquisition.",
                "distance": 0.087
            },
            {
                "reference": "Current user profile models lack a detailed description of the products.",
                "distance": 0.0923
            },
            {
                "reference": "In large e-commerce enterprises, sellers enter millions of items daily. While some sellers provide structured descriptions of their items, a majority provides unstructured natural language descriptions.",
                "distance": 0.0996
            }
        ]
    },
    {
        "target": "Unsupervised pre-training produces large language models (LMs) that are remarkably well-calibrated. However, LMs in practice are often fine-tuned with reinforcement learning with human feedback (RLHF-LMs) after the initial unsupervised pre-training stage, and there is mixed evidence about whether these models preserve the well-calibratedness of their unsupervised ancestors.",
        "prediction": "The growing interest in dataset generation is due to the superior generative capacity of large pre-trained language models (PLMs). However, challenges still exist in zero-shot learning.",
        "queries": [
            -2.402689,
            15.171613
        ],
        "log": [
            {
                "reference": "The growing interest in dataset generation is due to the superior generative capacity of large pre-trained language models (PLMs). However, challenges still exist in zero-shot learning.",
                "distance": 0.0224
            },
            {
                "reference": "Prompting has been effective in enabling large pretrained language models (LMs) for performing diverse NLP tasks, especially when there is limited downstream data. However, finding the optimal prompt for each task is challenging. Existing methods often tune soft prompts, which lack interpretability and reusability across LMs, and they are not applicable when gradients are not accessible. Discrete prompts, on the other hand, are difficult to optimize and are often created by enumeration or paraphrasing heuristics that do not systematically explore the prompt space.",
                "distance": 0.0335
            },
            {
                "reference": "Prompt-based paradigms in Natural Language Processing (NLP) tasks have shown good performance. However, their success can depend heavily on the design of the prompt, and their effectiveness varies depending on the model and training data.",
                "distance": 0.0406
            },
            {
                "reference": "Prompt-based learning using pre-trained language models with textual prompts and appropriate answer-category mapping has been successful for few-shot text classification and natural language inference. However, the issues of diverse linguistic expression and constraints on the answer space limit the performance of both manual answer design and automatic answer search.",
                "distance": 0.0491
            },
            {
                "reference": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. However, ICL incurs substantial computational, memory, and storage costs as it involves processing all training examples each time a prediction is made.",
                "distance": 0.0648
            },
            {
                "reference": "Prompt-based techniques have demonstrated potential in a variety of natural language processing tasks as a recent development in few-shot learning. However, existing prompt-based techniques struggle with the semantic distinction task of the Word-in-Context (WiC) dataset, performing no better than the random baseline.",
                "distance": 0.076
            },
            {
                "reference": "Prompting methods have shown impressive success in few-shot learning. However, their application to slot tagging tasks is inefficient as they need to enumerate all possible n-gram token spans in a sentence, which slows down the prediction process.",
                "distance": 0.0785
            },
            {
                "reference": "Prompt-based learning, also known as prompting, is an emerging paradigm for using information learned by a pretrained language model.",
                "distance": 0.0833
            },
            {
                "reference": "Prompt-based learning paradigm has achieved state-of-the-art results on several NLP tasks, especially in the few-shot scenarios, but the security vulnerability of this approach in continuous prompt learning algorithms hasn't been adequately studied.",
                "distance": 0.0896
            },
            {
                "reference": "Prompting has emerged as a promising paradigm for few-shot and zero-shot learning. However, it is often brittle and requires much larger models compared to the standard supervised setup.",
                "distance": 0.0902
            },
            {
                "reference": "The use of prompts to condition language models is a wide-spread practice, but it is not fully understood how much information the model retains from these prompts, especially when they are compressed, and how this can be effectively utilized for controllability and toxicity reduction.",
                "distance": 0.0935
            },
            {
                "reference": "Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification, but it remains unclear how to transfer prompting knowledge across similar NLP tasks for mutual reinforcement.",
                "distance": 0.0961
            },
            {
                "reference": "The recent progress in few-shot learning has shown that various prompt-based models perform extraordinarily well. It is widely perceived that prompts help models learn in the same way humans learn from task instructions expressed in natural language.",
                "distance": 0.0973
            },
            {
                "reference": "Previous work has shown that it's possible to retrieve factual information from a pre-trained language model by expressing them as cloze-style prompts. Subsequent work has attempted to refine this method by searching for better prompts, employing a separate set of facts as training data.",
                "distance": 0.0975
            }
        ]
    },
    {
        "target": "Contextual language models output representations that are more anisotropic and display outlier dimensions, which holds true for both monolingual and multilingual models. The reason for such outliers and their impact are still areas of open research.",
        "prediction": "Multilingual topic models are used to reveal patterns in cross-lingual document collections. However, pre-existing models are slow and lack interactivity, which prohibits their use in dynamic situations such as natural disasters or political instability.",
        "queries": [
            -0.71504,
            13.762493
        ],
        "log": [
            {
                "reference": "Multilingual topic models are used to reveal patterns in cross-lingual document collections. However, pre-existing models are slow and lack interactivity, which prohibits their use in dynamic situations such as natural disasters or political instability.",
                "distance": 0.0718
            },
            {
                "reference": "The study focuses on multilingual automated reply suggestions (RS) models that serve many languages simultaneously, which are often challenged by model capacity and data distribution skew across languages.",
                "distance": 0.0927
            },
            {
                "reference": "Scaling automated suggested replies for a commercial email application to multiple languages presents challenges due to increased compute requirements and the scarcity of language resources. Additionally, restricted data movement across regional centers prevents joint training across languages.",
                "distance": 0.0943
            }
        ]
    },
    {
        "target": "Intelligent Traffic Monitoring (ITMo) technologies can contribute to road safety/security and smart city infrastructure. Understanding traffic situations requires the fusion of perceptual information with domain-specific and causal commonsense knowledge. However, previous work has not definitively established whether models can effectively align these information sources and reason in novel scenarios.",
        "prediction": "In the area of Intelligent Transportation System, predicting the changes in traffic flow poses a challenge. Existing physics-based methods provide interpretable results but lack accuracy, while data-driven methods offer improved accuracy but lack a clear physical basis due to their black-box nature.",
        "queries": [
            9.504233,
            1.907364
        ],
        "log": [
            {
                "reference": "In the area of Intelligent Transportation System, predicting the changes in traffic flow poses a challenge. Existing physics-based methods provide interpretable results but lack accuracy, while data-driven methods offer improved accuracy but lack a clear physical basis due to their black-box nature.",
                "distance": 0.013
            },
            {
                "reference": "Collecting speed reports from vehicles is a common and cheap way to understand traffic conditions, but due to privacy and bandwidth considerations, not all vehicle occupants want to share their location and speed data in real time.",
                "distance": 0.0176
            },
            {
                "reference": "Accurate road speed predictions can assist drivers in smart route planning. Most existing research focuses on arterial roads, where sensors are closely configured to collect complete real-time data. However, speed predictions for collector roads, where sensors are not as densely configured, are often ignored. Traditional approaches for arterial roads that rely on complete historical data cannot directly be applied to collector roads due to substantial missing data.",
                "distance": 0.0568
            },
            {
                "reference": "Effective intersection control can reduce traffic congestion and associated vehicular emissions, which is critically needed in developing countries where air pollution is reaching life threatening levels. However, these regions often suffer from budget constraints and poor network connectivity.",
                "distance": 0.0568
            },
            {
                "reference": "Traditionally, the speed of each road segment is modeled as a function of time to understand traffic flow patterns, which requires significant data collection and fails to generalize to new areas.",
                "distance": 0.057
            },
            {
                "reference": "Creating a road safety map for cities is crucial but a challenging task due to the need for accurate data collection, which can be expensive and labor-intensive.",
                "distance": 0.0579
            },
            {
                "reference": "Traffic sensing, an important aspect for sustainable city planning and management, traditionally relies on humans using a combination of complementary information signals from different sensors, such as viewing and/or hearing traffic to identify road conditions.",
                "distance": 0.0584
            },
            {
                "reference": "Energy costs in the United States are at an all-time high with increasing demand and decreasing supply. Despite the high costs, traffic congestion is increasing. Carpooling is an underutilized option that can help reduce congestion.",
                "distance": 0.0595
            },
            {
                "reference": "The research field of Intelligent Transportation Systems (ITS) focuses on forecasting short-term traffic flow to reduce congestion and provide data for optimal traffic scheduling.",
                "distance": 0.0611
            },
            {
                "reference": "Traffic within key urban areas is monitored in real-time to manage mobility. However, unpredictability caused by factors like roadworks, accidents and special events necessitates a monitoring system's ability to adapt dynamically to changing areas, which may be challenged by the large amount of information to be transmitted and processed.",
                "distance": 0.0616
            },
            {
                "reference": "Intelligent Transportation System (ITS) faces a great challenge of traffic congestion. Traditional traffic information estimation methods rely on global positioning system (GPS) and vehicle detector (VD), shortcomings of which have prompted the exploration of alternative strategies.",
                "distance": 0.0669
            },
            {
                "reference": "Understanding traffic statistics in real city-scale camera networks is critical in the intelligent transportation field. Recently, vehicle route understanding, which aims to measure the traffic density by identifying the route of each vehicle in traffic cameras, has emerged as a new challenge.",
                "distance": 0.0738
            },
            {
                "reference": "Road traffic conditions are influenced by various events like extreme weather or sports games. Though web advancements make real-time retrieval of such events and weather conditions possible, predicting road traffic conditions accurately remains a challenge.",
                "distance": 0.084
            },
            {
                "reference": "Urban traffic gridlock is a common phenomenon while the mean occupancy rate of personal vehicle trips in the United States is only 1.6 persons per vehicle mile, suggesting a demand for ridesharing solutions to mitigate congestion.",
                "distance": 0.0849
            },
            {
                "reference": "Travel time prediction is an important subject in the area of Intelligent Transport System (ITS), and probe-car system, an upcoming data collection method, can collect data concerning larger areas compared with traditional fixed detectors.",
                "distance": 0.0875
            },
            {
                "reference": "Ridesharing companies are collecting large amounts of vehicle trajectories and speed data that can be used for traffic prediction. Many models have attempted to use Graph Convolutional Networks (GCNs) to forecast traffic, but these models have struggled to capture the intricate dependencies of consecutive road segments.",
                "distance": 0.0914
            },
            {
                "reference": "Existing methods of predicting travel times in an automobile, which may involve predicting speeds at different stages along a route, have provided unsatisfactory results.",
                "distance": 0.0915
            },
            {
                "reference": "Traffic accident forecasting has a lot of importance but is challenging due to multi-scale dependencies on spatial and temporal dimensional features. Additionally, traffic accidents are rare events, leading to zero-inflation problems. These issues can't be simultaneously addressed by existing methods.",
                "distance": 0.0948
            },
            {
                "reference": "In Intelligent Transport Systems (ITS), predicting and quantifying the impact of traffic accidents is essential. Current graph neural networks, which are state-of-the-art in graph learning, heavily rely on graph Fourier transform, assuming homophily among the neighborhood. However, this assumption makes it difficult to characterize abrupt signals such as traffic accidents.",
                "distance": 0.0964
            },
            {
                "reference": "Traffic congestion significantly impacts quality of life and the economy.",
                "distance": 0.0984
            }
        ]
    },
    {
        "target": "The challenges in takeaway recommendation systems are unique and twofold: firstly, users have dual preferences, i.e., for both stores and foods, and secondly, users' preferences change significantly depending on the time of day.",
        "prediction": "Existing context-based movie recommendation methods lack the ability to incorporate visual features like movie posters and still frames, which provide valuable insights into a movie and users' preferences.",
        "queries": [
            -10.341147,
            18.858728
        ],
        "log": [
            {
                "reference": "Existing context-based movie recommendation methods lack the ability to incorporate visual features like movie posters and still frames, which provide valuable insights into a movie and users' preferences.",
                "distance": 0.0192
            },
            {
                "reference": "Traditional recommendation systems often suggest similar items of the same type as the query object, without considering the semantics of different similarity measures.",
                "distance": 0.0231
            },
            {
                "reference": "Traditional recommendation systems choose the set of alternatives based only on their independent evaluations, which often does not take into account the uncertainty over user preferences.",
                "distance": 0.0426
            },
            {
                "reference": "Traditional recommendation systems focus on recommending items to users based on their preferences, but they don't consider specific aspects of the consumption of the items which could enhance the user experience.",
                "distance": 0.0487
            },
            {
                "reference": "Existing recommendation systems have a limitation in harnessing visual cues in an unsupervised manner to learn the co-occurrence distribution of items in real-world images for complementary recommendation.",
                "distance": 0.0499
            },
            {
                "reference": "Product recommender systems are often blocked by the limitation in product information hosted in e-commerce sites and are only active when users are performing e-commerce activities.",
                "distance": 0.0623
            },
            {
                "reference": "Current research in restaurant recommender systems behave differently from humans when making recommendations. Existing systems struggle to quickly establish user preferences, particularly for new users.",
                "distance": 0.063
            },
            {
                "reference": "Existing recommendation services often lack an element of surprise, providing users with predictable or commonly recommended items.",
                "distance": 0.0684
            },
            {
                "reference": "A variety of techniques have been proposed and investigated for delivering personalized recommendations for electronic commerce and other web applications. These methods are sometimes combined in hybrid recommenders to improve performance.",
                "distance": 0.0736
            },
            {
                "reference": "Interactive recommendation systems often determine the availability of content and users' opportunities for discovery based on preference models, but there is a lack of robust methods to quantify and evaluate this.",
                "distance": 0.0836
            },
            {
                "reference": "The diversity of recommendation algorithms makes choosing one technique increasingly difficult.",
                "distance": 0.0863
            },
            {
                "reference": "In content-based recommendation systems, the importance of each attribute used for recommendations is not always evident or considered, thus leading to less personalized and efficient recommendations.",
                "distance": 0.0866
            },
            {
                "reference": "Existing recommender systems do not take into account the inventory size of items being recommended, resulting in popular items quickly going out of stock and negatively impacting user experience.",
                "distance": 0.087
            },
            {
                "reference": "Conventional recommendation methods recommend each item independently, which may not be efficient or tailored to users' preferences in certain applications.",
                "distance": 0.0909
            },
            {
                "reference": "Current image recommendation systems encounter two critical issues: change in long-term needs of users and evolution of image collections.",
                "distance": 0.0922
            }
        ]
    },
    {
        "target": "Predicting multiple node labels on graphs is a complex problem due to highly nonlinear relationships across different node labeling. Negative transfer is prevalent when applying naive multitask learning to tasks such as multiple community detection.",
        "prediction": "The conventional graph deep learning models struggle in the scenario of low-labeling rates where there are only a limited number of labeled nodes available. They also suffer from the over-smoothing issue, which impedes deep architecture learning on graphs.",
        "queries": [
            -8.437802,
            -16.221266
        ],
        "log": [
            {
                "reference": "The conventional graph deep learning models struggle in the scenario of low-labeling rates where there are only a limited number of labeled nodes available. They also suffer from the over-smoothing issue, which impedes deep architecture learning on graphs.",
                "distance": 0.0639
            },
            {
                "reference": "Graph neural networks have been extensively used for node representation learning on single graph data. However, as the models were designed for the nodes on a single graph, they are unable to leverage information from multiple graphs, a characteristic common in real-world data, where nodes are partially aligned across multiple graphs.",
                "distance": 0.0756
            },
            {
                "reference": "Learning high-level representations for graphs is crucial for graph analysis tasks. Despite its importance, graph pooling, a significant aspect of this, is a less explored area and most existing techniques do not explicitly consider graph structural information.",
                "distance": 0.0966
            }
        ]
    },
    {
        "target": "Adversarial sample detection is crucial for socially-secure applications. Existing approaches require significant training data leading to potential privacy issues and concerns about generalizability.",
        "prediction": "Transferability of adversarial examples is key for attacking an unknown model in practical scenarios like blackbox attacks. Existing transferable attacks craft adversarial examples by indiscriminately distorting features to degrade prediction accuracy in a source model without being aware of intrinsic features of objects in the images.",
        "queries": [
            -2.736916,
            -22.1817
        ],
        "log": [
            {
                "reference": "Transferability of adversarial examples is key for attacking an unknown model in practical scenarios like blackbox attacks. Existing transferable attacks craft adversarial examples by indiscriminately distorting features to degrade prediction accuracy in a source model without being aware of intrinsic features of objects in the images.",
                "distance": 0.0173
            },
            {
                "reference": "Adversarial examples have attracted significant attention in machine learning, yet the reasons for their existence and pervasiveness are not clear.",
                "distance": 0.0205
            },
            {
                "reference": "Characterizing and assessing the adversarial vulnerability of classification models with categorical input is an important but underexplored area in research.",
                "distance": 0.0251
            },
            {
                "reference": "Evaluating the robustness of machine-learning models to adversarial examples is challenging. Previously, many defenses have inadvertently caused gradient-based attacks to fail, and have been broken under more rigorous evaluations. The lack of automatic testing and debugging tools makes the systematic application of existing guidelines and best practices difficult.",
                "distance": 0.0312
            },
            {
                "reference": "Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, to initialize optimization-based white-box attacks or generate update directions in black-box attacks. However, these simple perturbations could be sub-optimal as they are agnostic to the model being attacked.",
                "distance": 0.0335
            },
            {
                "reference": "The black-box transferability of adversarial perturbations has been examined before, however, changing an unseen model's decisions to a specific `targeted' class remains challenging. Existing methods are less suitable due to their reliance on class-boundary information that changes from one model to another, thus reducing transferability.",
                "distance": 0.039
            },
            {
                "reference": "Adaptive attacks have become the standard for evaluating defenses to adversarial examples, but current evaluations have been found to be incomplete. Past studies mainly focus on the end result and show that a defense was ineffective, with thirteen defenses recently published at ICLR, ICML, and NeurIPS that can be circumvented.",
                "distance": 0.0406
            },
            {
                "reference": "Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack.",
                "distance": 0.0465
            },
            {
                "reference": "Transfer-based targeted attacks on black-box models are of great interest due to their practical applicability. Maximizing the transfer success rate of adversarial examples requires avoiding overfitting to the source model, and image augmentation is a primary approach to this. However, previous works typically use simple image transformations like resizing, limiting input diversity.",
                "distance": 0.0487
            },
            {
                "reference": "Machine learned models are often required to abide by certain requirements, and there's a growing interest in developing approaches that can verify whether a model satisfies certain properties. Previous works focused specifically on adversarial example generation or robustness checking for this requirement.",
                "distance": 0.0531
            },
            {
                "reference": "Adversarial examples are inputs to machine learning models that are intentionally designed to cause the model to make a mistake, and the use of surrogate losses as a proxy for minimizing the 0/1 loss in machine learning often fails when dealing with adversarial examples.",
                "distance": 0.0571
            },
            {
                "reference": "Adversarial examples, characterized as malicious inputs to confuse machine learning models, are extensively studied in two types - sensitivity-based that introduce small changes resulting in a different model prediction, and invariance-based that induce minimal semantic changes altering the true label while maintaining model\u2019s prediction.",
                "distance": 0.0607
            },
            {
                "reference": "Adversarial examples, created by perturbations, have garnered significant attention in research areas. Recent works suggest that the presence of robust and non-robust features is a key reason for the existence of adversarial examples and have studied their interactions in the feature space.",
                "distance": 0.061
            },
            {
                "reference": "The problem of adversarial examples in machine learning has been tackled mostly from a game theoretic perspective. This entails viewing the interaction between the attacker and the classifier as a zero-sum game. However, previous works have predominantly allowed only one player to use randomized strategies.",
                "distance": 0.0627
            },
            {
                "reference": "Existing attack techniques for evading adversarial example detection defenses often fall short as they over-optimize against one constraint at the cost of satisfying another, thus failing to find adversarial examples that are both misclassified by the model and detected as non-adversarial.",
                "distance": 0.0661
            },
            {
                "reference": "In adversarial learning, there is an ongoing cycle between attackers and defenders where new attacks are proposed, mitigated by new defenses, and then new attacks are proposed to break earlier defenses. However, it was unclear if there exist conditions where no further advancements in attacks or defenses can be made.",
                "distance": 0.0663
            },
            {
                "reference": "Current classifiers are often vulnerable to adversarial examples, which are small input perturbations that change the classification output. Current attack algorithms work for ranged settings, from white-box to black-box classifiers, but generally assume deterministic answers and often fail when these are not the case.",
                "distance": 0.0685
            },
            {
                "reference": "Current methods lack a general theoretical approach for analyzing the risk bound while learning in the presence of adversaries.",
                "distance": 0.0688
            },
            {
                "reference": "In the field of adversarial attacks, several methods have exhibited substantial untargeted transferability, while generating efficient targeted transferability remains a challenge.",
                "distance": 0.0703
            },
            {
                "reference": "Recent studies have shown that adversarial examples created on one model can be used to attack other models, a phenomenon known as cross-model transferability. However, the existing research mostly focuses on adversarial transferability across deep models that share the same data modality. The cross-modal transferability of adversarial perturbation has not yet been explored.",
                "distance": 0.074
            }
        ]
    },
    {
        "target": "Classifying and generating clickbait spoilers has typically been tackled with conventional, shallow learned classifying models.",
        "prediction": "Clickbait posts arouse curiosity by not providing an informative summary of the linked web page's content.",
        "queries": [
            -23.275612,
            2.015262
        ],
        "log": [
            {
                "reference": "Clickbait posts arouse curiosity by not providing an informative summary of the linked web page's content.",
                "distance": 0.2458
            },
            {
                "reference": "The Hyperpartisan News Detection task has attracted interest, and previous studies, such as Potthast et al., 2018, have used text classification methods.",
                "distance": 0.3807
            }
        ]
    },
    {
        "target": "Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones.",
        "prediction": "Deep models are vulnerable to adversarial attacks. Most existing attacks focus on measuring perturbations under the $\\ell_p$ metric, while Wasserstein distance, a metric that takes pixel space geometry into account, is becoming a compelling alternative. However, creating an effective attack under the Wasserstein metric is computationally challenging.",
        "queries": [
            -2.358122,
            -21.333273
        ],
        "log": [
            {
                "reference": "Deep models are vulnerable to adversarial attacks. Most existing attacks focus on measuring perturbations under the $\\ell_p$ metric, while Wasserstein distance, a metric that takes pixel space geometry into account, is becoming a compelling alternative. However, creating an effective attack under the Wasserstein metric is computationally challenging.",
                "distance": 0.0198
            },
            {
                "reference": "Deep learning has shown phenomenal success in solving complex problems in the field of Computer Vision. However, recent studies show that deep neural networks are vulnerable to adversarial attacks which involve making subtle modifications to inputs to deceive the model into making incorrect predictions.",
                "distance": 0.0214
            },
            {
                "reference": "Deep neural networks have outstanding performance in many tasks, but they also have been shown susceptible to adversarial perturbations, which can cause misclassification of the inputs. This could pose a security risk when these systems are deployed in real-world scenarios.",
                "distance": 0.0248
            },
            {
                "reference": "Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples, where imperceptible perturbations added to clean images can fool the well-trained networks.",
                "distance": 0.0283
            },
            {
                "reference": "Deep neural networks (DNNs) have shown great performance but have been found vulnerable against adversarial examples, where small adversarial perturbations can mislead DNNs.",
                "distance": 0.0296
            },
            {
                "reference": "Data-agnostic quasi-imperceptible perturbations on inputs are known to degrade the recognition accuracy of deep convolutional networks, presenting as potential security issues. However, the specific characterization of the directions of such harmful perturbations remain unknown.",
                "distance": 0.0307
            },
            {
                "reference": "Deep neural networks are known to be susceptible to adversarial noise, often indentified through tiny and imperceptible perturbations. While vulnerability of image models to adversarial noise has been extensively researched, the vulnerability of video models is less explored.",
                "distance": 0.0346
            },
            {
                "reference": "Deep neural networks (DNNs) are vulnerable to adversarial examples, which result from adding small-magnitude perturbations to inputs. Different attack strategies have been proposed to generate these examples, but there is a need for methods that produce high perceptual quality examples more efficiently.",
                "distance": 0.0409
            },
            {
                "reference": "3D deep learning models are also vulnerable to adversarial attacks, similar to 2D models. However, existing adversarial attack methods are not sophisticated enough and exhibit significant performance reduction in the physical world, owing to the difficulty in bounding the perturbations in Euclidean space for highly structured 3D data.",
                "distance": 0.0414
            },
            {
                "reference": "Adversarial samples of deep neural networks (DNNs) have been intensively studied on static images, but their extensions in videos have not been explored. When attacking a video, spatial back and temporal cues need to be considered, and the adversarial perturbations should be temporally sparse to improve imperceptibility and reduce computation cost.",
                "distance": 0.0423
            },
            {
                "reference": "Deep neural networks have significantly improved various computer vision tasks. However, they remain highly vulnerable to adversarial attacks.",
                "distance": 0.0449
            },
            {
                "reference": "Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been well-studied with a focus on universal adversarial perturbations - image-agnostic perturbations that lead to high misclassification rates when added to natural images.",
                "distance": 0.0466
            },
            {
                "reference": "Recent research has shown that Deep Neural Networks (DNNs) are vulnerable to adversarial patches that introduce perceptible but localized changes to the input. Although there have been approaches to generate such adversarial patches for images, the same has not been explored thoroughly for videos which introduces additional complexity due to the need to consider temporal cues along with spatial cues.",
                "distance": 0.0476
            },
            {
                "reference": "Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks, which are small perturbations added to their input images to mislead their prediction. Detecting adversarial examples is a crucial requirement for robust classification frameworks.",
                "distance": 0.049
            },
            {
                "reference": "Deep Neural Nets (DNNs) can be fooled by adding human-imperceptible perturbations to images. Feature space targeted attacks, a mainstream method, perturb images by modulating their intermediate feature maps to minimize the discrepancy between intermediate source and target features. However, the current measure of discrepancy using pixel-wise Euclidean Distance unreasonably imposes a spatial-consistency constraint.",
                "distance": 0.0493
            },
            {
                "reference": "Neural networks are prone to adversarial attacks. Most of such attacks either slightly alter the majority of the input's pixels or occlude it with a patch, deteriorating the input quality.",
                "distance": 0.05
            },
            {
                "reference": "Deep neural networks for video classification, similar to image classification networks, may be vulnerable to adversarial manipulation.",
                "distance": 0.0526
            },
            {
                "reference": "Features of a pixel extracted by DNNs are influenced by its surrounding regions, and different DNNs generally focus on different discriminative regions in recognition. Existing attack methods manipulate pixel-wise noise to create adversarial examples, which could fool deep neural networks.",
                "distance": 0.0534
            },
            {
                "reference": "Many recent works have shown that deep learning models are vulnerable to quasi-imperceptible input perturbations, yet practitioners cannot fully explain this behavior.",
                "distance": 0.0545
            },
            {
                "reference": "Deep-learning-based methods have been showing vulnerability to adversarial examples, making their deployment in safety-critical tasks questionable. This vulnerability has also been demonstrated in medical imaging tasks where deep neural networks are used as inverse problem solvers.",
                "distance": 0.056
            }
        ]
    },
    {
        "target": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Existing selective prediction models, which abstain on low-confidence examples, show overconfidence towards unseen classes.",
        "prediction": "In selective prediction, a classifier abstains from making predictions on low-confidence examples. However, this has rarely been examined in natural language processing (NLP) tasks.",
        "queries": [
            0.467596,
            -9.913236
        ],
        "log": [
            {
                "reference": "In selective prediction, a classifier abstains from making predictions on low-confidence examples. However, this has rarely been examined in natural language processing (NLP) tasks.",
                "distance": 0.0293
            },
            {
                "reference": "Several machine learning methods allow for abstaining from uncertain predictions in conventional classification. However, this concept of abstention has not been widely explored in the context of learning to rank settings.",
                "distance": 0.0308
            },
            {
                "reference": "Conventional evaluation of two-class classifiers relies on setting a classification threshold and does not efficiently accommodate complex goals like identifying the k most likely or least likely responders out of a group.",
                "distance": 0.0386
            },
            {
                "reference": "Previous studies focused on scenarios of binary classification without considering the option to abstain from predicting the label of a point at a fixed cost.",
                "distance": 0.0494
            },
            {
                "reference": "There has been increasing attention towards developing hierarchy-aware classifiers in deep learning, aimed to leverage the label hierarchy for mistake severity evaluation and mitigation. However, current hierarchy-aware deep classifiers do not materially improve upon the standard cross-entropy baseline in reducing mistake-severity.",
                "distance": 0.0522
            },
            {
                "reference": "Classifiers that abstain from classification in uncertain cases can reduce misclassification cost but their parameters are often set in an ad-hoc, non-systematic manner.",
                "distance": 0.0557
            },
            {
                "reference": "Current state-of-the-art hierarchy-aware deep classifiers, despite showing potential to reduce the severity of mistakes through the use of label hierarchies, do not provide practical improvement over traditional cross-entropy baseline models.",
                "distance": 0.0635
            },
            {
                "reference": "In modern classification tasks, increasing number of classes and sizes of datasets presents challenges such as class ambiguity and class imbalance, making it difficult to achieve high top-1 accuracy. Top-K metrics have become popular, however, proposing top-K losses tailored for deep learning is a complex task both theoretically and practically.",
                "distance": 0.0679
            },
            {
                "reference": "Existing approaches to reducing multi-class cost-sensitive classification to binary classification lack guarantees regarding the error rates of transformed classifiers.",
                "distance": 0.0796
            },
            {
                "reference": "The task of finding a binary linear classifier or providing a near-infeasibility certificate if there is none is a long-standing problem in Machine Learning. Solutions for each problem have traditionally been addressed separately.",
                "distance": 0.0919
            },
            {
                "reference": "Classification problems in machine learning often involve the use of majority votes and margins as indicators of confidence, especially in semi-supervised learning contexts where labeled data may be limited.",
                "distance": 0.0927
            },
            {
                "reference": "The problem of multi-class classification in cases where the number of classes is very large is a challenge due to the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems.",
                "distance": 0.0931
            },
            {
                "reference": "Softmax classifiers typically consider all classes while performing classification, with no consideration for target versus non-target classes or the potential confusion between certain classes.",
                "distance": 0.0941
            }
        ]
    },
    {
        "target": "Egocentric visual data is a critical research area in egocentric vision. However, existing datasets for object understanding are either not egocentric, or they are limited in terms of object categories, visual content, and annotation granularities.",
        "prediction": "Egocentric visual data, often captured as egocentric videos, are vast and largely redundant. Traditional methods for managing such data, including global clustering strategies such as spectral clustering and multi-level graph partitioning, don't perform well.",
        "queries": [
            16.600172,
            6.233661
        ],
        "log": [
            {
                "reference": "Egocentric visual data, often captured as egocentric videos, are vast and largely redundant. Traditional methods for managing such data, including global clustering strategies such as spectral clustering and multi-level graph partitioning, don't perform well.",
                "distance": 0.0471
            },
            {
                "reference": "Egocentric interaction recognition aims to recognize the camera wearer's interactions with the interactor who faces the camera wearer in egocentric videos. Most existing works directly model the interactions as a whole and lack modeling the relations between the two interacting persons.",
                "distance": 0.0754
            },
            {
                "reference": "Gesture recognition in egocentric vision is challenged by the global camera motion caused by the spontaneous head movement of the device wearer. Existing solutions for this problem do not completely address this issue.",
                "distance": 0.099
            }
        ]
    },
    {
        "target": "Kernelized bandits, also known as Bayesian optimization (BO), are popular methods to optimize complicated reward functions. Although BO algorithms have theoretically shown upper bounds on their cumulative regret that are sub-linear in the number of iterations, the lower bound of the regret is derived as Omega(sqrt(T)), representing the unavoidable regrets for any classical BO algorithm. Recent works have shown that with quantum computing, it is possible to achieve tighter regret upper bounds, but these works are restricted to either multi-armed or linear bandits and do not solve sophisticated real-world problems with non-linear reward functions.",
        "prediction": "Finding the optimal online experiment for recommender selection through online exploration-exploitation has become increasingly important, as traditional methods like A/B testing can be slow and costly, and offline evaluations may be biased due to historical data. The problem can be framed using multi-armed bandits, but existing solutions are unsatisfactory as they don't take into account the contextual features of users and recommendations, particularly in e-commerce.",
        "queries": [
            1.443451,
            -27.444927
        ],
        "log": [
            {
                "reference": "Finding the optimal online experiment for recommender selection through online exploration-exploitation has become increasingly important, as traditional methods like A/B testing can be slow and costly, and offline evaluations may be biased due to historical data. The problem can be framed using multi-armed bandits, but existing solutions are unsatisfactory as they don't take into account the contextual features of users and recommendations, particularly in e-commerce.",
                "distance": 0.0149
            },
            {
                "reference": "(Srinivas et al., 2010) studied Gaussian process (GP) bandits with Gaussian observation noise, proving that the regret vanishes at an approximate rate of O(1/\u221at) where t is the number of observations.",
                "distance": 0.0256
            },
            {
                "reference": "Collaborative bandit learning, which uses bandit algorithms and collaborative filtering techniques to enhance sample efficiency in online interactive recommendations, has been a major method in the field. However, all existing solutions impose a stationary assumption about the environment, assuming that the user preferences and dependencies among users remain constant over time. In reality, both of these factors can vary, ultimately costing a recommender system optimal performance.",
                "distance": 0.0264
            },
            {
                "reference": "The problem of online selection of a bundle of items when the cost of each item changes arbitrarily from round to round and the valuation function is initially unknown and revealed only through the noisy values of selected bundles, the bandit feedback setting, is challenging due to the exponentially many bundles and the need for assumptions on the valuation functions.",
                "distance": 0.0265
            },
            {
                "reference": "The problem of bandit optimization is inspired by stochastic optimization and online learning problems with bandit feedback, where the objective is to minimize a global loss function of all the actions rather than a cumulative loss.",
                "distance": 0.0295
            },
            {
                "reference": "Online stochastic linear optimization, particularly in a unique bandit setting where only one-bit of feedback is provided to the learner at each stage, presents a significant issue. These types of situations have broad applications, notably in online advertising and recommendations, but the computational costs of existing methodologies make them impractical.",
                "distance": 0.0316
            },
            {
                "reference": "The generalized linear bandit framework is used to model complex reward structures like the logistic model, which is widely used when rewards are binary. For logistic bandits, the regret guarantees of existing algorithms are proportional to the square root of time (T), but with a problem-dependent constant (\u03ba) which can scale exponentially with the decision set size, causing loose regret bounds and poor performance.",
                "distance": 0.037
            },
            {
                "reference": "The primary challenge in logistic bandits is to minimize the dependence on a large problem-dependent constant that can scale exponentially with the norm of the unknown parameter vector. Previous works attempted to address this by leveraging the self-concordance of the logistic function, which provided improved regret guarantees but still had dependence on the problem-dependent constant and linear dependency on the dimension of the unknown parameter.",
                "distance": 0.0374
            },
            {
                "reference": "The study is motivated by the problem of a contextual bandit in recommender systems operating in a highly non-stationary environment, characterized by time-varying interests of users.",
                "distance": 0.0376
            },
            {
                "reference": "In the existing algorithms for sparse bandits, a priori knowledge of the sparsity index is required. This is almost never available in practice, and mis-specification can significantly degrade the performance of existing methods.",
                "distance": 0.038
            },
            {
                "reference": "Current multiclass algorithms in the bandit framework, where the learning algorithm receives only partial feedback (a single bit indicating whether the prediction is correct or not) after making a prediction, rely primarily on random sampling to trade-off exploration and exploitation.",
                "distance": 0.0403
            },
            {
                "reference": "The existing studies of adversarial multi-armed bandits do not leverage the common side information: contexts and side observations in the form of feedback graphs, which are particularly relevant in applications such as social networks.",
                "distance": 0.0405
            },
            {
                "reference": "Contextual bandit problems are traditionally solved using complex pattern recognition tasks, involving complex computations.",
                "distance": 0.0409
            },
            {
                "reference": "The instance-independent regret for stochastic contextual bandits with linear payoff previously had a lower bound of $\\mathcal{O}(\\sqrt{T})$ for the contextual linear bandit problem with arbitrary (adversarially chosen) contexts.",
                "distance": 0.0413
            },
            {
                "reference": "The authors focus on a sequential assortment selection problem in contextual bandits, where user choice is determined by a multinomial logit (MNL) choice model with unknown parameters. In this problem, the learning agent uses contextual information about the user and available items to decide which assortment of items to offer.",
                "distance": 0.0445
            },
            {
                "reference": "In the adaptive control literature, handling the explore-exploit trade-off in linear bandits problems is crucial and still challenging.",
                "distance": 0.0468
            },
            {
                "reference": "Prior to this study, there was an open question regarding the design of near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback.",
                "distance": 0.047
            },
            {
                "reference": "Stochastic linear bandits are widely used but often the feedback is randomly delayed and delays are only partially observable. The learner typically only observes delayed positive events, but existing algorithms don't account for this.",
                "distance": 0.0477
            },
            {
                "reference": "The Multinomial Logit Bandit (MNL-bandit) model is popular in online learning and operations research, and has received much attention due to its applications in fast fashion retailing and online advertising.",
                "distance": 0.0483
            },
            {
                "reference": "The problem of finding the optimal policy in multitask contextual bandits is well-explored in online recommendation systems. Established results in collaborative filtering show that a small number of per-user interactions suffice to learn a good policy when there are no adversarial users.",
                "distance": 0.0498
            }
        ]
    },
    {
        "target": "Rand-$k$ sparsification is a commonly used technique in distributed vector mean estimation for reducing communication cost by having each client send $k < d$ of its coordinates to the server. However, it does not take into account any correlations that may exist between clients. The recently proposed Rand-$k$-Spatial estimator improves upon Rand-$k$ by utilising cross-client correlation information, but its performance is still suboptimal.",
        "prediction": "In distributed systems, estimating the mean of a set of vectors is often needed, with applications in distributed Stochastic Gradient Descent (SGD) and several other areas where limited communication capacity poses challenges to computational scale-up.",
        "queries": [
            4.323784,
            -12.795267
        ],
        "log": [
            {
                "reference": "In distributed systems, estimating the mean of a set of vectors is often needed, with applications in distributed Stochastic Gradient Descent (SGD) and several other areas where limited communication capacity poses challenges to computational scale-up.",
                "distance": 0.0697
            },
            {
                "reference": "The problem of distributed mean estimation (DME) is a fundamental construct in distributed machine learning and there has been considerable work, especially in the context of distributed variance reduction for stochastic gradients in parallel SGD. Most of this work assumes an upper bound on the norm of the input vectors to achieve an error bound, which performs poorly when input vectors are concentrated around the correct output with large norm.",
                "distance": 0.079
            },
            {
                "reference": "In distributed learning problems, estimating the mean of an unknown d-dimensional Gaussian distribution is a commonly studied problem. Here, the samples from the unknown distribution are distributed among m different machines and the goal is to estimate the mean at the optimal minimax rate while communicating as few bits as possible.",
                "distance": 0.0798
            },
            {
                "reference": "The regularized risk minimization method is commonly used in Support Vector estimation, regression, Gaussian processes and others. However, a globally convergent method for handling these problems is lacking.",
                "distance": 0.084
            }
        ]
    },
    {
        "target": "General-purpose language models have been adapted successfully for downstream tasks in specific domains, but extraction of named entities from the economics literature on impact evaluation remains a challenge.",
        "prediction": "Previous work for guiding the generation of large pre-trained language models has focused on directly fine-tuning the language model or utilizing an attribute discriminator.",
        "queries": [
            -1.756772,
            14.229622
        ],
        "log": [
            {
                "reference": "Previous work for guiding the generation of large pre-trained language models has focused on directly fine-tuning the language model or utilizing an attribute discriminator.",
                "distance": 0.042
            },
            {
                "reference": "Achieving state-of-the-art performance on natural language understanding tasks typically requires fine-tuning separate models for each task. This results in a higher overall parameter cost and increased technical maintenance for serving multiple models, making a single, effective multi-task model a more desirable, yet challenging, goal.",
                "distance": 0.0421
            },
            {
                "reference": "The pre-training technique is now ubiquitous in the natural language processing field with ProphetNet, a pre-training based natural language generation method, demonstrating powerful performance on English text summarization and question generation tasks.",
                "distance": 0.0561
            },
            {
                "reference": "Although humans can perform unseen tasks by recalling relevant skills acquired previously and then generalizing them to the target tasks without supervision, current multi-task language models such as T0 and FLAN lack this cross-task generalization ability in an unsupervised setting.",
                "distance": 0.0577
            },
            {
                "reference": "Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on text summarization tasks but they are usually fine-tuned on hundreds of thousands of data points, proving infeasible for new, niche domains.",
                "distance": 0.0627
            },
            {
                "reference": "Randomised techniques allow very big language models to be represented succinctly, but being batch-based they are unsuitable for modeling an unbounded stream of language while maintaining a constant error rate.",
                "distance": 0.079
            },
            {
                "reference": "Pre-trained language models have achieved state-of-the-art accuracies on various text classification tasks. However, the reliability of the fine-tuned text classifiers is often overlooked, such as the ability to detect out-of-distribution samples or be robust against domain shifts. Further, there is an over-reliance on a limited number of keywords instead of looking at the whole context.",
                "distance": 0.0943
            },
            {
                "reference": "Existing pre-trained language models have proven effective for a broad range of natural language processing tasks, but they are not explicitly aware of domain-specific knowledge, posing a challenge for tasks in specialized areas such as e-commerce.",
                "distance": 0.0953
            },
            {
                "reference": "Large pre-trained language models are promising for few-shot learning, however, current benchmarks don't measure progress in real-world, applied settings.",
                "distance": 0.0971
            }
        ]
    },
    {
        "target": "The abstracts of scientific papers contain premises and conclusions, and although conclusion sentences are highlighted in structured abstracts, in non-structured abstracts this is not marked explicitly, making the automatic segmentation of conclusions a challenging task.",
        "prediction": "Key knowledge components of biological research papers are uploaded in sentences that summarize the main findings of experiments. However, no prior research has systematically characterised such sentences.",
        "queries": [
            -7.364194,
            3.902761
        ],
        "log": [
            {
                "reference": "Key knowledge components of biological research papers are uploaded in sentences that summarize the main findings of experiments. However, no prior research has systematically characterised such sentences.",
                "distance": 0.2467
            },
            {
                "reference": "The process of extracting and generating bibliographical information from technical and scientific documents raises challenges in key term extraction.",
                "distance": 0.2771
            }
        ]
    },
    {
        "target": "Classical Chinese poetry is a unique cultural heritage that often contains special words rarely found in general Chinese texts, posing significant challenges for natural language processing. There has been little research on text segmentation and word discovery for classical Chinese poetry.",
        "prediction": "Query segmentation, particularly in languages with no spaces such as Chinese, can benefit from the information carried by query boundaries. However, this information has not been fully utilized, especially in scenarios with limited labeled training data.",
        "queries": [
            3.751376,
            9.441168
        ],
        "log": [
            {
                "reference": "Query segmentation, particularly in languages with no spaces such as Chinese, can benefit from the information carried by query boundaries. However, this information has not been fully utilized, especially in scenarios with limited labeled training data.",
                "distance": 0.0298
            },
            {
                "reference": "Existing systems for Chinese classical poetry generation are mostly template-based and very few of them can accept multi-modal input.",
                "distance": 0.0407
            },
            {
                "reference": "Current models for generating classical Chinese poetry only allow user interference through keyword inputs, limiting user control over the semantic aspect of the generated poems.",
                "distance": 0.0527
            },
            {
                "reference": "Chinese word segmentation (CWS) and POS tagging have traditionally required task-specific feature engineering.",
                "distance": 0.0543
            },
            {
                "reference": "Supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features.",
                "distance": 0.0657
            },
            {
                "reference": "The problem of word sense disambiguation in Chinese language processing is challenging due to the highly ambiguous nature of the language.",
                "distance": 0.0741
            },
            {
                "reference": "The task of disambiguating sentiment ambiguous adjectives remains a challenge, especially in Chinese language.",
                "distance": 0.0772
            },
            {
                "reference": "The problem of classifying Chinese unknown words into fine-grained semantic categories defined in a Chinese thesaurus is addressed.",
                "distance": 0.0809
            },
            {
                "reference": "Previous research in Chinese unknown word classification focused mostly on proper nouns and used features related to unknown word contexts.",
                "distance": 0.0863
            },
            {
                "reference": "Existing Chinese poetry generation systems do not optimally capture both poetic content and form.",
                "distance": 0.0912
            },
            {
                "reference": "Chinese Couplet is a unique form of Chinese culture where an antecedent clause is matched with a subsequent clause with specific constraints on semantic and/or syntactic relatedness. However, the automatic generation of Chinese couplets by computer is complex and has not been fully explored.",
                "distance": 0.0917
            },
            {
                "reference": "Word segmentation is a key step for automatic syntactic analysis of Chinese text. While Chinese segmentation performs well on news data, its accuracy decreases significantly on other domains such as science and literature. For literature, especially novels, there isn't a fixed set of domain terms, as each novel tends to have a specific set of names for persons, organizations, and locations.",
                "distance": 0.0987
            }
        ]
    },
    {
        "target": "Experimenting with new website content is a key lever to improve customer engagement in digital marketing. Currently, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles.",
        "prediction": "In large e-commerce enterprises, sellers enter millions of items daily. While some sellers provide structured descriptions of their items, a majority provides unstructured natural language descriptions.",
        "queries": [
            -14.929984,
            3.17936
        ],
        "log": [
            {
                "reference": "In large e-commerce enterprises, sellers enter millions of items daily. While some sellers provide structured descriptions of their items, a majority provides unstructured natural language descriptions.",
                "distance": 0.0152
            },
            {
                "reference": "In e-commerce, there are motivations for e-shoppers, sellers and manufacturers to require an automated approach for matching product offers from various online sources referring to the same or a similar real-world product. Current methods for matching identical and similar product offers do not provide adequate information for further calculations and analyses.",
                "distance": 0.0153
            },
            {
                "reference": "Product pages on e-commerce websites provide a wealth of data which can overwhelm customers, making discovery of relevant information challenging.",
                "distance": 0.0408
            },
            {
                "reference": "Classifying products into categories precisely and efficiently is a significant challenge in e-commerce, due to the influx of new products daily and the dynamic nature of categories that necessitates the need for machine learning models to reduce the time and cost for human editors.",
                "distance": 0.0493
            },
            {
                "reference": "An e-commerce catalog typically consists of millions of product specifications, and the search engine receives millions of offers from thousands of independent merchants that need to be matched to the right products. There are significant challenges in matching unstructured offers to structured product descriptions.",
                "distance": 0.0526
            },
            {
                "reference": "Many e-Commerce sellers target cross-country and cross-lingual markets, which can be challenging due to the erroneous or incomplete seller descriptions. A key issue that arises is the alignment of items across multilingual e-commerce catalogues.",
                "distance": 0.0542
            },
            {
                "reference": "Product catalogs are crucial for electronic commerce on the Internet, but it is still a challenging task for casual users to perform effective product selection. An incremental restriction on interactive tables is a promising technique for product selection.",
                "distance": 0.0545
            },
            {
                "reference": "The lack of information and references about a product often leads to frustration after the acquisition.",
                "distance": 0.0624
            },
            {
                "reference": "On e-commerce platforms, predicting product compatibility is essential for effective product recommendation and search experience. However, the heterogeneous product data and lack of manually curated training data make accurate prediction challenging.",
                "distance": 0.0836
            },
            {
                "reference": "Analyzing commercial pages to infer the products or services offered by a web-based business is important for multiple e-commerce tasks. Challenges in this task arise due to the presence of two types of e-commerce product pages: single-product (SP) and multi-product (MP) pages.",
                "distance": 0.0857
            },
            {
                "reference": "The substantial increase in online e-commerce products has made manual annotation of tags, which assist users in finding these products, infeasible.",
                "distance": 0.0912
            },
            {
                "reference": "Currently, product descriptions do not have structured attribute-value pairs, making tasks like product recommendations, product comparison, and demand forecasting challenging.",
                "distance": 0.0913
            },
            {
                "reference": "Retrieving online product information in e-commerce websites is often difficult due to different descriptions for the same product.",
                "distance": 0.0914
            },
            {
                "reference": "Hot trends often bring new business opportunities, and it's critical for ecommerce companies to respond quickly and accurately to these trends to enhance product sales. However, identifying products related to these trends remains a challenging task.",
                "distance": 0.0998
            }
        ]
    },
    {
        "target": "Multi-view image generation methods mostly follow a paradigm where a 3D representation is first synthesized, then rendered into 2D images to ensure photo-consistency across viewpoints. However, this approach often results in geometry artifacts and loss of fine-scale details when applied to real image editing.",
        "prediction": "Existing work on image-based program synthesis mainly focuses on images containing a single visible 2D plane, without considering regular textures, patterns or the 3D posing of these planes in the real-world scene.",
        "queries": [
            21.337921,
            -2.744191
        ],
        "log": [
            {
                "reference": "Existing work on image-based program synthesis mainly focuses on images containing a single visible 2D plane, without considering regular textures, patterns or the 3D posing of these planes in the real-world scene.",
                "distance": 0.021
            },
            {
                "reference": "Free-viewpoint rendering of realistic scenes is challenging due to the need for detailed appearance and geometry models. Current approaches often yield blurry rendering due to the limitations of network capacity and difficulty in accurately intersecting camera rays with the scene geometry, often necessitating time-consuming optical ray marching.",
                "distance": 0.0492
            },
            {
                "reference": "Traditional methods for view synthesis from real images use explicit modelling of scene geometry, which can be computationally expensive and may not fully resolve issues like occlusions.",
                "distance": 0.0517
            },
            {
                "reference": "Synthesizing novel views from a 2D image requires inferring the 3D structure and projecting it back to 2D from a new viewpoint, which is considered a problem in the current field.",
                "distance": 0.0594
            },
            {
                "reference": "Previous methods for image synthesis from 3D point clouds have struggled to produce photo-realistic images from novel viewpoints.",
                "distance": 0.0733
            },
            {
                "reference": "Image-based and model-based methods are two representative rendering methods for generating virtual images of objects from their real images. However, both methods still have several drawbacks when applied to mixed reality where virtual images are integrated with real background images.",
                "distance": 0.0834
            }
        ]
    },
    {
        "target": "Textual backdoor attack, a novel attack model that adds a backdoor to the model during training, has been proven effective. However, defending against such backdoor attacks has become urgent and important.",
        "prediction": "Previous understanding of backdoor data poisoning attacks, where attackers inject mislabeled training examples into a training set, are limited and lack a formal theoretical framework, making predictions and analyses challenging.",
        "queries": [
            -3.731079,
            -21.370867
        ],
        "log": [
            {
                "reference": "Previous understanding of backdoor data poisoning attacks, where attackers inject mislabeled training examples into a training set, are limited and lack a formal theoretical framework, making predictions and analyses challenging.",
                "distance": 0.0144
            },
            {
                "reference": "Previous works have attempted to evaluate privacy loss through poisoning attacks or membership inference for differentially private machine learning systems. However, these methods have been tailored to specific models or have demonstrated relatively low statistical power.",
                "distance": 0.0491
            },
            {
                "reference": "Poisoning-based backdoor attacks present a serious concern for the training of deep models on untrusted data sources.",
                "distance": 0.0499
            },
            {
                "reference": "In outsourced machine learning training, $backdoor$ $attacks$ are practical where the third party training the model may inject malicious behaviors into an otherwise accurate model. Until now, the method to inject backdoors has been limited to $poisoning$.",
                "distance": 0.0541
            },
            {
                "reference": "Existing model poisoning attacks on federated learning systems assume an attacker has access to a significant fraction of compromised genuine clients, which is unrealistic in production systems that involve millions of clients.",
                "distance": 0.0601
            },
            {
                "reference": "Current backdoor attacks on NLP models often involve injecting triggers into selected sentences and changing the original label to a target label. This strategy has a flaw of being easily detected by defense models or manual inspections because of the abnormal language expression and mistaken labels.",
                "distance": 0.0639
            },
            {
                "reference": "Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process.",
                "distance": 0.0642
            },
            {
                "reference": "Recent studies have revealed a security threat to natural language processing (NLP) models, the Backdoor Attack, where victim models behave anomalously when a specific trigger word is inserted. Previous backdoor attacking methods usually assume that attackers have knowledge of the dataset which users use or proxy datasets for a similar task.",
                "distance": 0.0731
            },
            {
                "reference": "Previous off-path TCP-injection attacks assume vulnerabilities such as client-malware or predictable choice of client port or IP-ID.",
                "distance": 0.0741
            },
            {
                "reference": "Backdoor attacks manipulate the output of deep neural networks and pose high insidiousness in the field of natural language processing. These attacks are highly successful on multiple popular models, but there is a lack of studies conducted on defending against textual backdoor attacks.",
                "distance": 0.0756
            },
            {
                "reference": "Data poisoning attacks have raised concerns as they can lead to neural backdoors that misclassify certain inputs crafted by an attacker and the sample-targeted backdoor attack is a new challenge in these. The existing backdoor detection schemes fail to detect the sample-targeted backdoor as they depend on reverse-engineering the trigger.",
                "distance": 0.0784
            },
            {
                "reference": "In the context of poisoned models exploited by backdoor attacks, previous backdoor removal methods often decomposed the task into separate inner and outer problems, but this approach does not account for their interdependence.",
                "distance": 0.0795
            },
            {
                "reference": "A new form of data poisoning known as backdoor attacks has been uncovered recently. These attacks do not affect a network's behavior on typical, benign data but cause deviation only when triggered by an adversary's planted perturbation.",
                "distance": 0.0855
            },
            {
                "reference": "Deep Neural Networks in Natural Language Processing are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model with poisoned samples. Existing methods have not used this clean weight information to defend NLP models against backdoor attacks.",
                "distance": 0.0875
            },
            {
                "reference": "Machine learning models have been shown to be vulnerable to backdoor attacks. In these attacks, a model behaves normally but will misclassify based on a trigger. However, existing backdoor attacks only allow the model to misclassify toward a single predefined class.",
                "distance": 0.0883
            },
            {
                "reference": "System side channels denote effects imposed on the underlying system and hardware when running a program, allowing attackers to infer program secrets based on observed side channel signals. Reconstructing private user images from these side channels in the context of machine learning as a service (MLaaS) is highly challenging due to the technical knowledge required of victim software's internal operations.",
                "distance": 0.0929
            },
            {
                "reference": "Model inversion attacks (MIAs) aim to create synthetic images that reflect the class-wise characteristics from a target classifier\u2019s private training data by exploiting the model\u2019s learned knowledge. Previous research has developed generative MIAs that use generative adversarial networks (GANs) as image priors tailored to a specific target model. This makes the attacks time- and resource-consuming, inflexible, and susceptible to distributional shifts between datasets.",
                "distance": 0.0973
            },
            {
                "reference": "Pre-Trained Models (PTMs) have been widely used, but they are vulnerable to backdoor attacks. PTMs can have weights maliciously poisoned with certain triggers, resulting in inaccurate predefined predictions and security challenges. Existing poisoning methods can have their backdoors erased by changing hyper-parameters during fine-tuning or detected by finding the triggers.",
                "distance": 0.098
            }
        ]
    },
    {
        "target": "Large language models (LMs) have shown the ability to generate free-text rationalizations, which can significantly enhance their performances. However, the usefulness of these machine-generated rationales for human users, especially non-expert users trying to answer questions based on those rationales, remains unclear.",
        "prediction": "Large language models with over 100 billion parameters show improved reasoning capabilities when using chain of thought prompting, achieving state-of-the-art results on various datasets. However, these reasoning capabilities are not prevalent in smaller models.",
        "queries": [
            -2.802424,
            14.209677
        ],
        "log": [
            {
                "reference": "Large language models with over 100 billion parameters show improved reasoning capabilities when using chain of thought prompting, achieving state-of-the-art results on various datasets. However, these reasoning capabilities are not prevalent in smaller models.",
                "distance": 0.0051
            },
            {
                "reference": "Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction.",
                "distance": 0.0706
            }
        ]
    },
    {
        "target": "In simulating language emergence with neural networks, compositionality, a hallmark of human language, has been shown to improve communication performance. However, the impact of compositionality on imitation learning has not been investigated yet.",
        "prediction": "Neural network encoders are used in language processing tasks, but the kind of structural knowledge learned in these encoders that is transferable to natural language processing is unclear.",
        "queries": [
            -1.603326,
            12.462197
        ],
        "log": [
            {
                "reference": "Neural network encoders are used in language processing tasks, but the kind of structural knowledge learned in these encoders that is transferable to natural language processing is unclear.",
                "distance": 0.0448
            },
            {
                "reference": "Neural language models are a crucial tool to embed words into semantic vector spaces. However, learning such models relies heavily on abundant and varied training examples, a requirement that may not be fulfilled in highly specialised domains due to the limited range of expression or difficulties in obtaining a large corpus.",
                "distance": 0.0664
            },
            {
                "reference": "Modelling multilingual text data over time, especially for domain-specific short to mid length time-stamped textual data, presents challenges.",
                "distance": 0.0692
            },
            {
                "reference": "Most targeted syntactic evaluation datasets ask models to make judgements based on a single context-free sentence as input, which is inconsistent with language models' training regime where sentences are highly contextualized by the surrounding corpus. Consequently, the stability of language models' syntactic judgement in different contexts is questionable.",
                "distance": 0.0717
            },
            {
                "reference": "Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution.",
                "distance": 0.0837
            },
            {
                "reference": "The input vocabulary and the learned representations are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models. This has led to common practices of using smaller vocabulary to lower the memory requirements and construct more interpretable models.",
                "distance": 0.0861
            }
        ]
    },
    {
        "target": "In the fields of Natural Language Processing (NLP) and Information Retrieval (IR), there's a prevalence challenge of aggregating results over incomparable metrics and scenarios, resulting in less reliable conclusions and take-away messages.",
        "prediction": "Natural Language Inference traditionally involves predicting a single judgement, however, this approach does not consider the diversity of human opinion.",
        "queries": [
            -3.418593,
            11.231023
        ],
        "log": [
            {
                "reference": "Natural Language Inference traditionally involves predicting a single judgement, however, this approach does not consider the diversity of human opinion.",
                "distance": 0.0895
            },
            {
                "reference": "Using Natural Language Processing (NLP) for Information Retrieval has been a challenging research area, and most results indicate that using NLP effectively is complex.",
                "distance": 0.1012
            }
        ]
    },
    {
        "target": "Current methods to evaluate the factual consistency of text generation applications generally rely on specific metrics, like natural language inference (NLI) or question answering (QA), which are trained on limited data. This makes it difficult for such metrics to assess a wide variety of factual discrepancies across diverse text generation scenarios.",
        "prediction": "Current text generation models often generate text that is factually inconsistent with their inputs. Existing methods for evaluating factual consistency use models trained on other data-rich tasks like question answering (QA) and natural language inference (NLI) without further adaptation. As a result, they perform poorly on generated text and show heavy bias towards their original tasks due to the lack of annotated data.",
        "queries": [
            -5.556011,
            9.610711
        ],
        "log": [
            {
                "reference": "Current text generation models often generate text that is factually inconsistent with their inputs. Existing methods for evaluating factual consistency use models trained on other data-rich tasks like question answering (QA) and natural language inference (NLI) without further adaptation. As a result, they perform poorly on generated text and show heavy bias towards their original tasks due to the lack of annotated data.",
                "distance": 0.0407
            },
            {
                "reference": "Large neural models often generate factually incorrect text, and there is a lack of a standard automatic evaluation for factuality. While grounded generation, where models draw on a reliable external document for factual information, simplifies the challenge of factuality and its measurement, there is no standard automatic metric for factual consistency.",
                "distance": 0.0452
            }
        ]
    },
    {
        "target": "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Previously, comparisons between the two approaches were performed using models of different sizes, making it unclear whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup.",
        "prediction": "Fine-tuning large pre-trained models on downstream tasks is time and resource-intensive since it requires updating the entire parameter set of the pre-trained models. Although parameter-efficient transfer learning (PETL) techniques update only a fraction of the parameters, it still requires a substantial amount of memory due to backpropagation through the pre-trained backbone model.",
        "queries": [
            1.484668,
            -19.43479
        ],
        "log": [
            {
                "reference": "Fine-tuning large pre-trained models on downstream tasks is time and resource-intensive since it requires updating the entire parameter set of the pre-trained models. Although parameter-efficient transfer learning (PETL) techniques update only a fraction of the parameters, it still requires a substantial amount of memory due to backpropagation through the pre-trained backbone model.",
                "distance": 0.0399
            },
            {
                "reference": "Fine-tuning large pre-trained language models on downstream tasks has become the standard learning paradigm in NLP but involves tuning all parameters, which is increasingly prohibitive. Several parameter-efficient transfer learning methods have been developed that only fine-tune a small number of parameters, but their critical components for success and the connections among the methods are not well understood.",
                "distance": 0.0477
            },
            {
                "reference": "Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. However, it may cause efficiency issues, catastrophic forgetting and interference. To address this, techniques like adapters and sparse fine-tuning have been developed where adapters are modular and can be combined to adapt a model towards different knowledge facets whereas sparse fine-tuning controls the behavior of all model components.",
                "distance": 0.0566
            },
            {
                "reference": "Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, which increases the cost of tuning in direct proportion to the model size. Past work on gradient-free tuning, which only requires the forward computation of the PTM, often introduces gradient descent to ensure good prompt initialization and lacks versatility across tasks and PTMs.",
                "distance": 0.0577
            },
            {
                "reference": "Pre-training (PT) followed by fine-tuning (FT) for training neural networks is a common practice that introduces various hyperparameters and design choices such as task and data reweighting strategies, augmentation policies, and noise models. However, optimizing these PT hyperparameters is challenging due to issues with scalability, efficiency, and applicability to the two-stage PT and FT learning process.",
                "distance": 0.0583
            },
            {
                "reference": "Adapting large pre-trained models (PTMs) through fine-tuning imposes prohibitive computational and storage burdens. Current delta tuning (DT) methods rely on manual designation to determine fine-grained positions inside PTMs, often leading to sub-optimal results.",
                "distance": 0.0626
            },
            {
                "reference": "Lightweight fine-tuning methods facilitate parameter-efficient transfer learning by updating only a small set of parameters while keeping the pretrained language model parameters frozen. Despite their efficiency, there are no studies analyzing if and how the pretraining stage should be affected by these downstream fine-tuning approaches.",
                "distance": 0.0692
            },
            {
                "reference": "Multi-stage training and knowledge transfer, which involves the transition from a large-scale pretraining task to various finetuning tasks, have revolutionized natural language processing and computer vision, leading to improvements in performance.",
                "distance": 0.0697
            },
            {
                "reference": "The workflow of pretraining and fine-tuning has become a common approach for solving various Natural Language Processing (NLP) and Vision-and-Language (V&L) downstream tasks. As the capacity of pretrained models grows, the need for parameter-efficient fine-tuning becomes crucial for quick transfer learning and deployment.",
                "distance": 0.0714
            },
            {
                "reference": "The existing approach for transferring pretrained language models for knowledge graph completion relies on finetuning, which modifies all language model parameters.",
                "distance": 0.0931
            },
            {
                "reference": "Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant effort in fine-tuning and therefore, researchers propose feature-based model selection (FMS) methods, helping assess PTMs' transferability to a specific task swiftly without fine-tuning.",
                "distance": 0.0938
            },
            {
                "reference": "Fine-tuning over large pretrained language models has established many state-of-the-art results, but this method can be unstable, resulting in significant variance in performance and potential risks for practical applications. It has been suggested that top-down fine-tuning of layers can address this issue.",
                "distance": 0.0983
            }
        ]
    },
    {
        "target": "Attention layers, as used in transformers, are fundamental to modern deep learning, but there is no mathematical description of their benefits and shortcomings compared to other architectures.",
        "prediction": "Transformer networks have redefined the state of the art in many NLP tasks, but suffer from quadratic computational cost in the input sequence length to compute attention in each layer. This has prompted research into faster attention models involving sparsifying the connections in the attention layers.",
        "queries": [
            0.121781,
            17.18623
        ],
        "log": [
            {
                "reference": "Transformer networks have redefined the state of the art in many NLP tasks, but suffer from quadratic computational cost in the input sequence length to compute attention in each layer. This has prompted research into faster attention models involving sparsifying the connections in the attention layers.",
                "distance": 0.0151
            },
            {
                "reference": "The transformer architecture and its variants have been successful across many machine learning tasks due to their ability to handle long sequences and context-dependent weights from the attention mechanism.",
                "distance": 0.0216
            },
            {
                "reference": "Linear transformers are aimed to reduce the quadratic space-time complexity of vanilla transformers, but they often suffer from degraded performances on various tasks and corpus because of issues with unbounded gradients in attention computation and attention dilution.",
                "distance": 0.028
            },
            {
                "reference": "Transformers have become the preferred model for tasks in natural language processing and vision and there has been substantial effort towards making training and deployment of Transformers more efficient. One common strategy involves approximating the self-attention matrix, which is a crucial component in Transformers, using strategies such as prespecified sparsity patterns, low-rank basis expansions and combinations thereof.",
                "distance": 0.0389
            },
            {
                "reference": "Transformers are slow and memory-hungry on long sequences due to the quadratic time and memory complexity of self-attention. Approximate attention methods have tried to solve this by trading off model quality to reduce the compute complexity but generally fail to achieve significant speedup.",
                "distance": 0.0405
            },
            {
                "reference": "Transformers primarily use sparse attention patterns for their operation, and pre-training is commonly done for accommodating these patterns. There is a popular belief in using local attention patterns and less focus on less-studied global attention patterns.",
                "distance": 0.051
            },
            {
                "reference": "Transformers have made progress in various tasks, but they suffer from quadratic computational and memory complexities. Sparse transformers with attention on sparse graphs have been proposed to address this, but how dense a graph needs to be for optimal performance is not fully explored.",
                "distance": 0.0524
            },
            {
                "reference": "The Transformer model has shown great successes in various fields but the softmax attention, as one of its core components, prohibits its scaling due to the quadratic space and time complexity relative to the sequence length. Kernel methods, used to reduce this complexity by approximating the softmax operator, perform inconsistently across tasks and corpus and generally suffer performance drops when compared to the vanilla softmax attention.",
                "distance": 0.0593
            },
            {
                "reference": "Transformers have been successful in sequencing tasks, however, the computation of the attention matrix, a key component, has quadratic complexity with respect to sequence length. This makes them difficult to use for large sequences.",
                "distance": 0.0653
            },
            {
                "reference": "The demand for long-term forecasting, in scenarios such as extreme weather early warning and long-term energy consumption planning, is increasing. Prior Transformer-based models adopt various self-attention mechanisms to discover long-range dependencies, but face difficulties in uncovering reliable dependencies due to intricate temporal patterns of the long-term future. Efficiency limitations also lead to Transformers using sparse versions of point-wise self-attentions, resulting in an information utilization bottleneck.",
                "distance": 0.067
            },
            {
                "reference": "The recently proposed Conformer model, which features a hybrid attention-convolution architecture, has become the standard model for various downstream speech tasks. However, systematic studies have revealed that the design choices of the Conformer model might not be optimal.",
                "distance": 0.0721
            },
            {
                "reference": "In the set2vec problem, or the task of extracting vector representations from an input set of variable number of feature vectors, recent models like transformers have seen success. However, such models have high computational costs. Alternatives like inducing-point attention and optimal transport kernel embedding (OTKE) aim to reduce computational cost, but employ fixed learnable queries in attention, which may be limiting.",
                "distance": 0.0734
            },
            {
                "reference": "Interpreting the underlying dynamics of Transformers is of growing interest. While self-attention patterns were initially considered the primary option for interpreting Transformers, recent studies have indicated that integrating other components can provide more accurate explanations.",
                "distance": 0.0735
            },
            {
                "reference": "The Transformer architecture, especially its core components such as self-attention and contextual embeddings, have been widely used but not fully understood.",
                "distance": 0.0738
            },
            {
                "reference": "Transformer encoder architectures are known for their efficiency but they utilize self-attention sublayers, which can be computationally intensive.",
                "distance": 0.0767
            },
            {
                "reference": "The Transformer-Kernel (TK) model is recognized for its reranking performance on the TREC Deep Learning benchmark and is considered a slightly less effective but efficient alternative to other Transformer-based architectures. TKL, a variant of TK with local self-attention, is developed to process longer input sequences for document ranking.",
                "distance": 0.0771
            },
            {
                "reference": "Transformers based on the attention mechanism have achieved success in many domains, yet the quadratic complexity of attention mechanisms impedes their ability to handle numerous tokens and scale up to bigger models. Existing methods mainly use similarity decomposition and associativity of matrix multiplication to create linear-time attention mechanisms, but reintroduce inductive biases such as locality at the cost of model generality and expressiveness.",
                "distance": 0.0836
            },
            {
                "reference": "Existing generative transformer models, used for synthesizing high-fidelity and high-resolution images, treat an image naively as a sequence of tokens and decode an image sequentially following the raster scan ordering.",
                "distance": 0.0905
            },
            {
                "reference": "The Transformer model, which uses dot-product attention to update entity representations, has been extensively used in various natural language processing tasks.",
                "distance": 0.093
            },
            {
                "reference": "Transformers are state-of-the-art models for a variety of sequence modeling tasks consisting of an attention function which models pairwise interactions between the inputs at every timestep. However, the attention does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length.",
                "distance": 0.0976
            }
        ]
    },
    {
        "target": "Neural network-based models with embeddings-based representation have been dominant for the development of text-based reinforcement learning agents. While they learn uninterpretable policies, they often do not generalize well in unseen games. Meanwhile, neuro-symbolic methods, leveraging intermediate formal representation, are gaining attention due to their inherent interpretability, lower requirements for training data, and generalizability in scenarios with unseen data.",
        "prediction": "Self-attention architectures have shown success in natural language processing, but have not been successfully applied to reinforcement learning (RL) due to difficulty in optimization. This optimization problem has been observed in supervised learning settings and becomes especially pronounced with RL objectives.",
        "queries": [
            9.231012,
            -22.680405
        ],
        "log": [
            {
                "reference": "Self-attention architectures have shown success in natural language processing, but have not been successfully applied to reinforcement learning (RL) due to difficulty in optimization. This optimization problem has been observed in supervised learning settings and becomes especially pronounced with RL objectives.",
                "distance": 0.0161
            },
            {
                "reference": "Replay buffers are a key component in many reinforcement learning schemes, but their theoretical properties are not fully understood.",
                "distance": 0.0227
            },
            {
                "reference": "Most Reinforcement Learning (RL) research aims to minimize the expected total discounted cost using policies for sequential decision tasks. However, using the expected value as a decision criterion has been proven to be unreliable, yet most RL researchers have not concerned themselves with this issue until now.",
                "distance": 0.0402
            },
            {
                "reference": "Dynamic treatment regimes (DTR) with reinforcement learning (RL) on electronic health records (EHR) have attracted interest for individualized ventilation strategies. However, these might be biased due to confounders causing RL models, guided by long-term outcomes (90-day mortality), to suboptimally punish certain treatment actions linked to mortality.",
                "distance": 0.0462
            },
            {
                "reference": "Many applications of reinforcement learning (RL) require guarantees for robust performance amidst disturbances to the dynamics or reward function. Prior work has proposed multiple robust RL algorithms to handle such disturbances, which typically necessitate adding additional components and hyperparameters on top of a base RL algorithm.",
                "distance": 0.058
            },
            {
                "reference": "In reinforcement learning, model-free approaches address the task by directly mapping external and internal states to actions while model-based methods attempt to construct a model of the environment and then select optimal actions based on that model.",
                "distance": 0.0603
            },
            {
                "reference": "Off-policy deep reinforcement learning (RL) has seen success in complex tasks from visual observations using experience replay and convolutional neural networks (CNNs). However, these techniques demand significant memory and computational resources.",
                "distance": 0.0659
            },
            {
                "reference": "Reinforcement learning algorithms for real-world robotic applications must be able to handle complex, unknown dynamical systems while maintaining data-efficient learning. These requirements are well handled by model-free and model-based reinforcement learning (RL) approaches, respectively.",
                "distance": 0.069
            },
            {
                "reference": "Model-based reinforcement learning (RL) algorithms use a learned model for planning and are great for offline RL. However, practical variants require explicit uncertainty quantification which can be unreliable with complex models like deep neural networks, leading to poor performance.",
                "distance": 0.0737
            },
            {
                "reference": "Deep reinforcement learning (DRL) in spoken dialogue systems (SDS) often suffers from sample inefficiency and instability introduced by complex cognitions such as the Imagination Augmented Agent (I2A). I2A achieves a higher success rate by augmenting predicted future into a policy network.",
                "distance": 0.0755
            },
            {
                "reference": "Model-based reinforcement learning (RL) uses imaginary trajectories generated by a learned dynamics model to improve sample efficiency. However, if the model is inaccurate or biased, these imaginary trajectories may negatively impact the training of the action-value and policy functions.",
                "distance": 0.0877
            }
        ]
    },
    {
        "target": "Animate entities in narrative comics stories are expressed through a number of visual representations across panels. Identifying these entities is necessary for recognizing characters and analysing narrative affordances unique to comics, and integrating these with linguistic reference annotation, however an annotation process for animate entity identification has not received adequate attention.",
        "prediction": "Narrative is used as a mode of creative expression and communication in various contexts. However, the understanding of how creativity shapes the design of a narrative artifact in terms of plot, point, and pacing is still at the brink of our scientific understanding.",
        "queries": [
            -4.669941,
            9.516429
        ],
        "log": [
            {
                "reference": "Narrative is used as a mode of creative expression and communication in various contexts. However, the understanding of how creativity shapes the design of a narrative artifact in terms of plot, point, and pacing is still at the brink of our scientific understanding.",
                "distance": 0.0103
            },
            {
                "reference": "Narrative originates in forms common to oral cultures. A body of work that examines and analyzes myths, legends, and folklore already exists, providing a foundation for a psychologically resonant experience. Such work includes the characteristics of oral narrative sketched by Ong, the structural interpretation of myth described by Levi-Strauss, the psychological types outlined by Jung, and Propp\u2019s morphology.",
                "distance": 0.0143
            },
            {
                "reference": "Narrative has a fundamental role in making sense of texts due to its provision of structure and coherence. However, it is challenging to understand its functionality in the context of multimedia interactive learning environments (MILES).",
                "distance": 0.0247
            },
            {
                "reference": "Understanding videos such as TV series and movies involves analyzing the characters and their actions. The common approach to this problem often requires prior knowledge about the number of characters and often discards minor or background characters.",
                "distance": 0.0281
            },
            {
                "reference": "Narrative is seen as an effective medium for contextualizing learning. Additionally, students\u2019 ability to self-regulate learning significantly impacts performance and academic achievement.",
                "distance": 0.0364
            },
            {
                "reference": "The understanding of narrative, especially for oral narratives, is often treated without sufficient regard for the social dimensions of the storytelling.",
                "distance": 0.04
            },
            {
                "reference": "Expressive narrative visualization often requires a well-planned narrative order. Research shows that anachronies in novels and films enhance story expressiveness, but there's limited understanding of how they can be used in narrative visualization.",
                "distance": 0.0509
            },
            {
                "reference": "Understanding narrative content has become an increasingly popular topic, but research on identifying common types of narrative characters or personae is impeded by the lack of automatic and broad-coverage evaluation methods.",
                "distance": 0.0532
            },
            {
                "reference": "Understanding narrative text involves capturing characters' motivations, goals, and mental states. Traditional text modeling methods may not effectively capture the internal states of characters in a story.",
                "distance": 0.0557
            },
            {
                "reference": "Stereotypical character roles, such as archetypes or dramatis personae, play an important role in narratives by facilitating communication and easing understanding of characters' roles. Currently, identifying these roles relies on human interpretation.",
                "distance": 0.0569
            },
            {
                "reference": "Automatic summarization evaluation methods developed for English are routinely applied to other languages without any systematic study of their efficacy across different languages.",
                "distance": 0.0605
            },
            {
                "reference": "Readers of novels need to identify and learn about the characters as they develop an understanding of the plot.",
                "distance": 0.063
            },
            {
                "reference": "Narrative passages told from a character's perspective are known to convey thoughts and perceptions of that character. An open question is how to recognize such perspectives and especially how to understand references in narrative that reflect the character's beliefs.",
                "distance": 0.0642
            },
            {
                "reference": "Understanding and writing moral stories bridges story plots and morals, posing challenges in grasping abstract moral concepts, capturing discourse relations between events, and aligning value preferences concerning behaviors.",
                "distance": 0.0771
            },
            {
                "reference": "Recent years have seen a growing interest in the role that narrative can play in learning and the emergence of narrative-centered learning environments that engage students with compelling characters.",
                "distance": 0.078
            },
            {
                "reference": "Crime stories by Agatha Christie are interesting to readers, but the underlying components and reasons behind their interestingness and engagement are not well-understood.",
                "distance": 0.0876
            },
            {
                "reference": "Understanding narratives requires the characterization of relationships between people yet the polarity of these relationships in narrative summaries is not well addressed.",
                "distance": 0.0888
            }
        ]
    },
    {
        "target": "The dominance of fine-tuning biomedical pre-trained language models (BioPLMs), such as BioBERT, for named entity recognition (NER) tasks has overlooked the consideration of class distributions in targeted datasets, which often leads to problems when dealing with imbalanced biomedical datasets where most entities are underrepresented.",
        "prediction": "Named entity recognition (NER) has been broadly divided into flat, overlapped (nested), and discontinuous NER, and these have been studied separately. Recently there has been interest in unified NER that tackles these three aspects with one model. Current best-methods are span-based and sequence-to-sequence models, which have their respective drawbacks such as focus only on boundary identification and exposure to bias.",
        "queries": [
            -21.729382,
            -4.25818
        ],
        "log": [
            {
                "reference": "Named entity recognition (NER) has been broadly divided into flat, overlapped (nested), and discontinuous NER, and these have been studied separately. Recently there has been interest in unified NER that tackles these three aspects with one model. Current best-methods are span-based and sequence-to-sequence models, which have their respective drawbacks such as focus only on boundary identification and exposure to bias.",
                "distance": 0.0009
            },
            {
                "reference": "Zero-shot cross-lingual named entity recognition (NER) transfers knowledge from annotated data in source languages to unlabeled data in target languages. Existing methods based on the teacher-student distillation framework ignore the information in the intermediate layers of pre-trained language models, leading to loss of domain-invariant information during transfer.",
                "distance": 0.007
            },
            {
                "reference": "Statistical machine learning methods like Maximum Entropy and Conditional Random Fields that are used for Named Entity Recognition (NER) training tend to overfit when the available training corpus is limited, especially if the number of features is large or the number of values for a feature is large.",
                "distance": 0.0144
            },
            {
                "reference": "Neural named entity recognition (NER) models often encounter the issue of over-confidence, which can deteriorate performance and calibration.",
                "distance": 0.0144
            },
            {
                "reference": "Span-based methods for nested named entity recognition (NER) with neural networks have shown potential, but they face issues such as degeneration when positive and negative instances overlap significantly. Additionally, they often have limited generalizability, as many entities in test sets do not appear in training sets.",
                "distance": 0.019
            },
            {
                "reference": "Cross-domain Named Entity Recognition (NER) aims to transfer the NER knowledge from high-resource domains to low-resource target domains which is a challenging task due to domain shift and limited labeled resources.",
                "distance": 0.0191
            },
            {
                "reference": "Existing span-based named entity recognition (NER) systems only perform shallow aggregation of token representations to span representations. This approach often results in ineffectiveness for long-span entities, coupling between representations of overlapping spans, and degraded performance.",
                "distance": 0.0223
            },
            {
                "reference": "Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mainly due to the sophisticated design of templates and label words. However, these methods are time-consuming when applied to token-level labeling tasks like named entity recognition (NER) because it requires enumerating the template queries over all potential entity spans.",
                "distance": 0.0244
            },
            {
                "reference": "Named entity recognition (NER) is normally divided into nested NER and flat NER, requiring different models since sequence labeling models, widely used for flat NER, are unsuitable for nested NER where a token may have multiple labels.",
                "distance": 0.0245
            },
            {
                "reference": "Domain adaptation in named entity recognition (NER) is important as NER classifiers lose accuracy during the domain transfer due to different data distribution between the source and target domains. Generally, existing approaches require a significant amount of labeled target domain data to tune the original model, however, creating annotated training datasets for every target domain is labor-intensive and time-consuming.",
                "distance": 0.0249
            },
            {
                "reference": "Previous methods for cross-lingual named entity recognition (NER) with limited labeled data fell into two categories: model transfer and data transfer. However, these methods either exploited context information without task-specific information or used translated pseudo-target-language data with weakened context due to translation inaccuracies. They also rarely utilized abundant unlabeled target language data.",
                "distance": 0.0255
            },
            {
                "reference": "Named Entity Recognition (NER) methods are generally based on either sequence labelling or span classification. Unconstrained span-based methods often result in overlapping spans, which are undesirable especially for NER tasks without nested entities.",
                "distance": 0.0306
            },
            {
                "reference": "Multi-modal Named Entity Recognition (MNER) typically uses image information through a pretrained object detector and relies on an attention mechanism to model interactions between image and text representations. However, this approach is difficult as image and text representations are trained separately on different data and not aligned in the same space.",
                "distance": 0.0323
            },
            {
                "reference": "Current neural architectures in named entity recognition (NER) perform well on single domain data but suffer from overfitting and performance degradation when there is a domain shift between training and testing.",
                "distance": 0.0351
            },
            {
                "reference": "Few-shot named entity recognition (NER) aims to generalize to unseen labels and/or domains with few labeled examples. Existing metric learning methods compute token-level similarities between query and support sets, but struggle to incorporate label semantics into their models.",
                "distance": 0.0392
            },
            {
                "reference": "Nested named entity recognition (NER), which deals with overlapping entity spans in text, is commonly approached with span-based methods. These methods generate a score matrix, but previous work overlooks the spatial relations within this matrix.",
                "distance": 0.0445
            },
            {
                "reference": "Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER aims to mitigate this issue by transferring knowledge from high-resource languages to low-resource languages, but it is severely affected by the quality of translation or label projection.",
                "distance": 0.0452
            },
            {
                "reference": "Named Entity Recognition (NER) is traditionally performed using supervised learning methods like Conditional Random Fields (CRFs). These methods, however, can face limitations in accommodating unlabeled data for improvement.",
                "distance": 0.0465
            },
            {
                "reference": "Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework. A natural solution is to treat the task as a span classification problem, but it requires effective integration of heterogeneous factors for better span representation and classification performance.",
                "distance": 0.0569
            },
            {
                "reference": "Named Entity Recognition (NER) is a challenge in NLP due to difficulties in detecting nested entities using sequence labeling frameworks. Additionally, span-based methods, though capable of detecting nested entities, are computationally expensive and inefficient at inference. They also lack explicit boundary supervision.",
                "distance": 0.0586
            }
        ]
    },
    {
        "target": "Comparative Opinion Quintuple Extraction (COQE) is used to identify comparative opinion sentences in product reviews and incorporate them into quintuples. Existing methods solve COQE by decomposing the task into multiple primary subtasks and processing in a pipeline manner which ignores the intrinsic connection between subtasks and may lead to error propagation.",
        "prediction": "In opinion mining of product reviews, the need to produce summaries based on product features is often complicated by the different words and phrases people use to express the same feature. Existing topic modeling methods freely find groupings, but may not fully capture these variances.",
        "queries": [
            -20.483097,
            -0.044231
        ],
        "log": [
            {
                "reference": "In opinion mining of product reviews, the need to produce summaries based on product features is often complicated by the different words and phrases people use to express the same feature. Existing topic modeling methods freely find groupings, but may not fully capture these variances.",
                "distance": 0.4647
            },
            {
                "reference": "Opinion mining is widely used to identify strengths and weaknesses of products or services based on user feedback. Feature extraction is a crucial step for opinion mining to collect crucial information from user reviews. However, most existing approaches only find individual features without understanding the structural relationships between them.",
                "distance": 0.4696
            }
        ]
    },
    {
        "target": "Existing Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which often fails to handle informal or noisy location names.",
        "prediction": "Research on microblog content mining is becoming more focused than user relationship analysis. Traditional text mining methods have been well studied, but no algorithm has been designed specifically for microblog data, which contain structured information on social networks besides plain text.",
        "queries": [
            -20.782776,
            2.776679
        ],
        "log": [
            {
                "reference": "Research on microblog content mining is becoming more focused than user relationship analysis. Traditional text mining methods have been well studied, but no algorithm has been designed specifically for microblog data, which contain structured information on social networks besides plain text.",
                "distance": 0.0171
            },
            {
                "reference": "Recently, there has been a great deal of interest in analyzing inherent structures in posts on microblogs such as Twitter. Most research utilizes well-known topic modeling techniques.",
                "distance": 0.0484
            },
            {
                "reference": "Detecting topics in Twitter streams is gaining increased attention, with applications in community support and understanding user opinions. Traditional topic detection approaches focus on representing topics using terms, but are negatively affected by length limitation and the lack of context associated with tweets.",
                "distance": 0.0519
            },
            {
                "reference": "Searching microblog data such as Twitter is challenging due to user information overload resulting from a diversity of topics, sparse user data, and a highly social nature. Traditional methods for personalized web search do not perform well in the microblog domain.",
                "distance": 0.0682
            },
            {
                "reference": "Existing methods of predicting geolocation for Twitter users have room for accuracy improvement and they don't extensively consider aspects such as user-declared metadata and temporal factors.",
                "distance": 0.071
            },
            {
                "reference": "The increasing volume of information on micro-blogging sites such as Twitter raises challenges to traditional text mining techniques as most texts are abbreviated and the input comes in streams of large volumes.",
                "distance": 0.0845
            },
            {
                "reference": "The analysis of microblogs such as Twitter often falls into the framework of existing Conversation Analysis assumptions, despite their distinct social characteristics and the need for close examination of conversations in projects such as PHEME.",
                "distance": 0.0887
            }
        ]
    },
    {
        "target": "Humans naturally understand the connection between sketches and real-world objects, an ability that goes beyond simple categorization and extends to understanding correspondences between elements within sketches and real-world objects. Current computational models do not fully achieve this level of understanding.",
        "prediction": "Current models for encoding free-hand sketches, such as LSTM sequence-to-sequence architectures like SketchRNN, struggle with tasks like sketch classification, sketch-based image retrieval, and sketch reconstruction and interpolation.",
        "queries": [
            11.255157,
            -1.33654
        ],
        "log": [
            {
                "reference": "Current models for encoding free-hand sketches, such as LSTM sequence-to-sequence architectures like SketchRNN, struggle with tasks like sketch classification, sketch-based image retrieval, and sketch reconstruction and interpolation.",
                "distance": 0.0576
            },
            {
                "reference": "In the construction management industry, large photo collections are amassed to document project progress, often challenging to effectively browse and search due to the lack of effective indexing and visualization of spatial coverage, time, and content.",
                "distance": 0.0647
            },
            {
                "reference": "Current methods for learning from image datasets representing sparse data such as handwriting, pen strokes, freehand sketches, etc., involve storing the sparse sketches in regular dense tensors, which is less efficient in terms of computation time and memory usage.",
                "distance": 0.0744
            },
            {
                "reference": "Sketches often convey conceptual relationships between entities via the visual relationships between their depictions in the sketch. Existing cognitive simulations of analogical matching and retrieval are used to generate suggestions for new sketches based on analogies with prior sketches.",
                "distance": 0.0814
            }
        ]
    },
    {
        "target": "Cognitive diagnosis (CD) plays a vital role in intelligent education systems through personalized learning guidance, but recent developments mostly focus on improving diagnostic results accuracy and overlook domain-level zero-shot cognitive diagnosis (DZCD). DZCD is challenged by lack of student behavior data in the target domain due to absent student-exercise interactions or unavailable exercising records for training.",
        "prediction": "Cognitive diagnosis in intelligent education aims to determine the proficiency level of students in specific knowledge concepts. Existing methods usually use manual-designed functions to find linear interactions between students' exercises, which doesn't effectively capture the complex relationships present.",
        "queries": [
            -18.509634,
            -2.830719
        ],
        "log": [
            {
                "reference": "Cognitive diagnosis in intelligent education aims to determine the proficiency level of students in specific knowledge concepts. Existing methods usually use manual-designed functions to find linear interactions between students' exercises, which doesn't effectively capture the complex relationships present.",
                "distance": 0.0118
            },
            {
                "reference": "Cognitive diagnosis (CD), which assesses students' mastery levels on different knowledge concepts, is an important part of intelligent educational settings. Historically, CD was viewed as an inter-layer interaction modeling problem, with models largely focusing on student-exercise interactions. However, previous work has largely overlooked the inner-layer structural relationships such as educational interdependencies among concepts, leading to a lack of comprehensive modeling of the student-exercise-concept hierarchy.",
                "distance": 0.0127
            },
            {
                "reference": "Cognitive diagnosis, which reveals the proficiency level of learners on knowledge concepts, is important in intelligent education. Most methods proposed in recent years acquire the traits parameters of learners and items in a transductive way, which suits stationary data. However, in real scenarios data is collected online, where learners, test items, and interactions grow continuously, violating the stationary condition.",
                "distance": 0.0166
            },
            {
                "reference": "Cognitive diagnostic models (CDMs), used to quantify students' cognitive level on knowledge attributes, exist, but few incorporate attribute hierarchy, which is important due to learning dependencies among attributes. Current methods struggle with high time complexity when attributes are numerous and lack flexibility due to rigid hierarchy assumptions.",
                "distance": 0.0472
            }
        ]
    },
    {
        "target": "Generalizable face anti-spoofing (FAS) based on domain generalization (DG) is robust in real-world applications. However, existing DG methods heavily rely on labeled source data, which are usually costly and hard to access. Unsupervised learning methods often learn identity-biased and domain-biased features and fail to specify spoof cues.",
        "prediction": "Face anti-spoofing methods based on domain generalization have gained attention for their robustness in unseen scenarios. However, most techniques assume that domain labels are known, which is often not the case in real-world applications where datasets contain mixed domains.",
        "queries": [
            23.106531,
            7.882137
        ],
        "log": [
            {
                "reference": "Face anti-spoofing methods based on domain generalization have gained attention for their robustness in unseen scenarios. However, most techniques assume that domain labels are known, which is often not the case in real-world applications where datasets contain mixed domains.",
                "distance": 0.0968
            },
            {
                "reference": "Existing domain generalization methods for face anti-spoofing aim to extract common differentiation features to improve the generalization but struggle to create a compact and generalized feature space for fake faces due to large distribution discrepancies among different domains.",
                "distance": 0.1504
            }
        ]
    },
    {
        "target": "High-fidelity virtual human avatar applications seek photorealistic video face synthesis with controllable semantic editing over facial features. Existing generative neural methods have made progress in portrait video synthesis but intuitive control over facial attributes, such as mouth interior and gaze, remains a challenge.",
        "prediction": "Existing methodologies for 3D face alignment often require complex inputs and can struggle with producing accurate landmarks in challenging poses.",
        "queries": [
            20.757008,
            4.911147
        ],
        "log": [
            {
                "reference": "Existing methodologies for 3D face alignment often require complex inputs and can struggle with producing accurate landmarks in challenging poses.",
                "distance": 0.0591
            },
            {
                "reference": "Previous curve evolution frameworks did not systematically incorporate a global parameterized shape into the process, impacting their effectiveness for tasks such as face contour extraction.",
                "distance": 0.0626
            },
            {
                "reference": "Cascaded regression, despite its success for face alignment and head pose estimation, presents several drawbacks, specifically: regressors are learnt independently, descent directions may cancel one another out, and handcrafted features, which may be sub-optimal, are primarily used.",
                "distance": 0.076
            },
            {
                "reference": "Face recognition and many medical imaging applications require the computation of dense correspondence vector fields that match one surface with another. Traditionally, this process relies on a large set of manually-defined landmarks to constrain these surface correspondences.",
                "distance": 0.0951
            }
        ]
    },
    {
        "target": "Current face recognition (FR) distillation methods usually utilize the Feature Consistency Distillation (FCD) on the learned embeddings extracted by the teacher and student models. However, after using FCD, intra-class similarities of the student model are lower than the intra-class similarities of the teacher model.",
        "prediction": "Current state-of-the-art semantic segmentation methods require high computational resources for accurate segmentation. One method to address this efficiency-accuracy trade-off is knowledge distillation, which typically focuses on densely pairwise relations.",
        "queries": [
            9.235862,
            20.768965
        ],
        "log": [
            {
                "reference": "Current state-of-the-art semantic segmentation methods require high computational resources for accurate segmentation. One method to address this efficiency-accuracy trade-off is knowledge distillation, which typically focuses on densely pairwise relations.",
                "distance": 0.0174
            },
            {
                "reference": "Online knowledge distillation methods typically utilize only class probabilities, ignoring the rich information contained in feature maps about image intensity and spatial correlation.",
                "distance": 0.0354
            },
            {
                "reference": "Despite the recent observation that multi-generational self-distillation can improve generalization, the reasons for this enhancement remain poorly understood. Furthermore, label smoothing, a commonly used regularization technique that regularizes predictive uncertainty, is often performed independently of self-distillation.",
                "distance": 0.0541
            },
            {
                "reference": "The prevalent issue in training compact semantic segmentation networks involves leveraging cumbersome networks using knowledge distillation. The traditional method, pixel-wise distillation, applies the distillation scheme initially introduced for image classification and performs knowledge distillation for each pixel separately.",
                "distance": 0.0624
            },
            {
                "reference": "Deep learning regularization techniques have focused on the regularization of weight parameters for effective generalization, with some efforts aimed at label regularization to soften labels but neglected the relationship between classes. Meanwhile, knowledge distillation proposes to distill soft labels that contain class relation knowledge, but it requires pre-training a burdensome teacher model.",
                "distance": 0.0653
            },
            {
                "reference": "State-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. Knowledge distillation emerges as a promising technique to obtain accurate yet lightweight models but current pose distillation techniques depend on heavy pre-trained models and require complex two-stage learning procedures.",
                "distance": 0.068
            },
            {
                "reference": "Deploying large-scale deep models on resource-limited devices is challenging due to computational complexity and storage requirements. Current knowledge distillation methods for object detection mainly imitate features near bounding boxes, ignoring potentially beneficial features outside these areas and including detrimental features mistakenly identified as background by the teacher detector.",
                "distance": 0.0755
            },
            {
                "reference": "Recently, a perspective has emerged suggesting that label smoothing is incompatible with knowledge distillation. The root of this incompatibility is believed to be that label smoothing erases relative information between teacher logits.",
                "distance": 0.0768
            },
            {
                "reference": "Standard distillation of networks for visual odometry relies on 'dark knowledge' for successful knowledge transfer. However, this knowledge is not always available in pose regression, and the teacher prediction is not always accurate.",
                "distance": 0.0894
            },
            {
                "reference": "Knowledge distillation uses both real hard labels and soft labels predicted by teacher models as supervision. However, critical order violations between hard labels and soft labels in augmented samples have been found. These violations are common and injure the knowledge transfer.",
                "distance": 0.0916
            },
            {
                "reference": "In previous studies, such as those by Muller et al. (2019) and Shen et al. (2021b), there have been mixed findings on the compatibility between label smoothing (LS) and knowledge distillation (KD), but no effort has been made to understand and resolve these conflicting findings.",
                "distance": 0.0924
            },
            {
                "reference": "Traditional object detectors are not designed for incremental learning, and fine-tuning them directly on a well-trained detection model with new data often results in catastrophic forgetting. Knowledge distillation is one solution to this problem but previous works in Incremental Object Detection (IOD) have concentrated mainly on distilling features and responses, often neglecting the valuable information contained in responses.",
                "distance": 0.096
            },
            {
                "reference": "Existing knowledge distillation approaches based on feature-map transfer often manually assign target student layers to specific teacher layers, risking negative regularization due to potential semantic mismatches between these layer pairs.",
                "distance": 0.0985
            }
        ]
    },
    {
        "target": "Previous research has shown that 2D and 3D features are complementary for point cloud segmentation, but existing methods require additional 2D annotations for effective fusion of 2D and 3D information. The high cost of annotating point clouds necessitates weakly supervised learning methods for effective feature fusion.",
        "prediction": "The construction of 3D point cloud datasets is difficult due to the high amount of required human effort, posing challenges for 3D scene understanding, including issues such as 3D model collection and labor-intensive annotation.",
        "queries": [
            18.259531,
            -2.1434
        ],
        "log": [
            {
                "reference": "The construction of 3D point cloud datasets is difficult due to the high amount of required human effort, posing challenges for 3D scene understanding, including issues such as 3D model collection and labor-intensive annotation.",
                "distance": 0.0035
            },
            {
                "reference": "Point cloud segmentation is a fundamental task in 3D. While recent progress on point cloud segmentation has used deep networks, these methods are often unreliable when dealing with mislabeled or noisy labels, an issue common in real-world point cloud datasets.",
                "distance": 0.0119
            },
            {
                "reference": "Deep learning approaches have greatly advanced 2D semantic segmentation, but direct semantic segmentation of unstructured 3D point clouds remains a challenge. The PointNet architecture, which operates on unstructured point clouds and gets promising results, processes each block of the input points individually.",
                "distance": 0.0208
            },
            {
                "reference": "In 3D object point cloud understanding, previous deep networks treat all points or local patches equally, thereby neglecting the complementary geometries between the contour and flat parts of objects.",
                "distance": 0.0253
            },
            {
                "reference": "Deep Neural Networks have demonstrated excellent performance for No-reference quality assessment metrics, however, applying it to the assessment of 3D point clouds is challenging due to the lack of essential large-scale subjective databases.",
                "distance": 0.0331
            },
            {
                "reference": "Despite advancements in 3D understanding, state-of-the-art encoders cannot effectively learn semantic representations directly from raw point clouds due to their restrictiveness to canonicalized point clouds and lack of performance when confronting geometric transformation distortions.",
                "distance": 0.0337
            },
            {
                "reference": "Although point-based networks are accurate for 3D point cloud modeling, they lag behind voxel-based competitors in 3D detection due to an inherent issue with too much unimportant background information being maintained.",
                "distance": 0.0361
            },
            {
                "reference": "3D point cloud data is increasingly used in safety-critical applications such as autonomous driving, and the robustness of 3D deep learning models against adversarial attacks becomes a major consideration.",
                "distance": 0.0409
            },
            {
                "reference": "Semantic learning on 3D point clouds using a deep network is challenging due to the naturally unordered data structure. Among existing works, PointNet has achieved promising results by directly learning on point sets. However, PointNet does not take full advantage of a point's local neighborhood that contains fine-grained structural information, which could be helpful towards better semantic learning.",
                "distance": 0.0422
            },
            {
                "reference": "Point clouds and sets are input data-types which pose unique problems to deep learning due to their variable cardinality and invariance to permutation, forming an infinite-dimensional non-Euclidean space. Despite these challenges, PointNet and DeepSets introduced foundational neural network architectures to address these problems.",
                "distance": 0.0427
            },
            {
                "reference": "The work stems from an exploratory study on the application of various segmentation techniques to point clouds of dental models. Previous studies on point clouds representing urban landscapes suggest hybridization of current methods for point cloud segmentation could yield better outcomes.",
                "distance": 0.0469
            },
            {
                "reference": "Existing methods for 3D instance and semantic segmentation of point clouds focus on designing convolutional operators which limit the exploration of point relations for better segmentation.",
                "distance": 0.052
            },
            {
                "reference": "Segmenting humans in 3D indoor scenes is increasingly important but challenging due to lack of training data on humans interacting with 3D scenes. Existing works rarely attempted to directly segment humans in point clouds.",
                "distance": 0.0535
            },
            {
                "reference": "Despite the extensive usage of point clouds in 3D vision, relatively limited data are available for training deep neural networks. Furthermore, data augmentation, a standard approach to compensate for the scarcity of data, is less explored in the point cloud literature.",
                "distance": 0.0569
            },
            {
                "reference": "Understanding the semantics of point clouds is a topic of growing interest, however, point cloud labeling remains an open problem due to the difficulty in acquiring sufficient 3D point labels for training effective classifiers.",
                "distance": 0.057
            },
            {
                "reference": "Previous adversarial attacks on 3D point cloud recognition have often been criticized for their noticeable point outliers, as they just involve an implicit constraint like global distance loss to limit the generated noise. Point cloud is a highly structured data format, making it challenging to constrain its perturbation with a simple loss appropriately.",
                "distance": 0.0596
            },
            {
                "reference": "Current point cloud analysis methods need improvements in terms of better encapsulating local characteristics of the underlying structure, improved classification and segmentation performance, and reducing training time.",
                "distance": 0.0607
            },
            {
                "reference": "PointNet is a pioneer in deep learning for point sets, but it is limited in its ability to recognize fine-grained patterns and generalize to complex scenes because it does not capture local structures induced by the metric space points live in.",
                "distance": 0.0634
            },
            {
                "reference": "How to segment diversified elements in 3D point clouds, which precisely and intuitively describe real scenes, is rarely discussed.",
                "distance": 0.0646
            },
            {
                "reference": "Deep learning networks such as PointNet that directly handle points in a point set have advanced the field of supervised learning tasks on point clouds such as classification and segmentation. However, the challenges of unsupervised learning on point clouds remain.",
                "distance": 0.0667
            }
        ]
    },
    {
        "target": "Existing multimodal contrastive learning frameworks rely on shared information between sensory modalities, but do not effectively consider exclusive modality information that is critical for understanding underlying sensing physics. Furthermore, these frameworks do not handle the temporal information locality in time series properly.",
        "prediction": "Multimodal learning seeks to improve generalization performance by using information from different data modalities. Current approaches either seek common information shared across modalities or fuse supplementary, modality-specific information. However, these methods tend to only work with samples featuring complete modalities, leading to wastage of data. Moreover, using model-based imputation to fill in missing values can introduce undesired noise, particularly when sample size is limited.",
        "queries": [
            7.44544,
            -2.439238
        ],
        "log": [
            {
                "reference": "Multimodal learning seeks to improve generalization performance by using information from different data modalities. Current approaches either seek common information shared across modalities or fuse supplementary, modality-specific information. However, these methods tend to only work with samples featuring complete modalities, leading to wastage of data. Moreover, using model-based imputation to fill in missing values can introduce undesired noise, particularly when sample size is limited.",
                "distance": 0.0027
            },
            {
                "reference": "Real-world data often involves multiple modalities delivering rich information from varying perspectives. However, existing methods for analyzing such data treat cross-modality relations as simple observation-fitting tasks, and do not effectively adapt to the complexities and divergence in multi-modal data.",
                "distance": 0.0055
            },
            {
                "reference": "Multi-modality (multi-sensor) data is widely used in various vision tasks, but its increased data modalities present new challenges for data storage and transmission. Existing data compression approaches usually handle each modality individually without considering the correlation between different modalities.",
                "distance": 0.0059
            },
            {
                "reference": "Multimodal fusion methods, which use multiple sources of data for classification or regression, have proven to be advantageous in various applications over unimodal counterparts. However, existing methods struggle to balance the trade-off between inter-modal fusion and intra-modal processing, creating a bottleneck for performance improvement.",
                "distance": 0.0137
            },
            {
                "reference": "In multimodal representation learning, effective unimodal representation and complementary crossmodal representation fusion are crucial. However, prior works often straightforwardly modulate one modal feature to another, underutilizing both unimodal and crossmodal representation refinements and leading to performance bottlenecks.",
                "distance": 0.0139
            },
            {
                "reference": "The challenge of understanding scenes from multiple sources of sensor data when there is no one-to-one correspondence across modalities is a common scenario in multi-sensor data analysis. Previous solutions have addressed this by restricting interpretation to a single representation in one of the domains and augmenting features from other modalities.",
                "distance": 0.0175
            },
            {
                "reference": "Multi-modal learning aims to relate information from multiple modalities, but two challenges include the difficulty of collecting a large, complete dataset containing all required modalities and the prevalence of high dimensional and noisy features on modalities.",
                "distance": 0.0191
            },
            {
                "reference": "The fusion of multiple modalities such as text, acoustic, and visual information is a complex task, since these modalities hold diverse types of information and their contributions vary. This can be more challenging in low-resource settings, with fewer training samples.",
                "distance": 0.0207
            },
            {
                "reference": "While diverse input data modalities can enhance model performance, there's often a discrepancy between available modalities at training and testing stages. At test time, certain modalities might be missing or noisy, posing a challenge to implement robust models.",
                "distance": 0.023
            },
            {
                "reference": "The attention of research has moved from single-modal learning to multi-modal learning due to the existence of real-world data in the form of different modalities. However, multi-modal models often carry more information than single-modal models and are applied in sensitive scenarios. The existing membership inference techniques are focused on machine learning classifiers where input and output are in the same modality.",
                "distance": 0.0239
            },
            {
                "reference": "In the emergent field of multimodal medical AI, the integration of multiple data modalities (e.g., vision, language) has become popular due to its proven benefits in improving performance, robustness, needing less training samples, and providing complementary information, but lack technical reproducibility and transparency.",
                "distance": 0.0248
            },
            {
                "reference": "Multi-modal learning, which allows for the integration of both shared and modality-specific information from different data types, is essential in understanding real-world data. However, current strategies often fail when data from certain modalities are incomplete or missing.",
                "distance": 0.0266
            },
            {
                "reference": "Acquiring and modeling data with multiple modalities has gained interest with affordable sensors. However, malfunctions or configuration issues can leave gaps in the data, and most existing multi-modal learning algorithms cannot handle these missing modalities, often discarding valuable information in the process.",
                "distance": 0.0268
            },
            {
                "reference": "Learning multimodal representations, which integrates information from various data sources, is a complex yet important field with numerous applications. However, there are limited resources to study generalization across domains and modalities, the level of complexity during training and inference, and robustness to noise and missing modalities.",
                "distance": 0.0274
            },
            {
                "reference": "The majority of existing multimodal sequential learning methods focus on representations and neglect the importance of multimodal fusion. The Bilinear attention network (BAN) is a common fusion method, but it struggles with increased computational complexity as the number of modalities rises.",
                "distance": 0.028
            },
            {
                "reference": "Many multi-modal learning approaches have been proposed to integrate information from different sources. However, due to issues such as data collection failures and self-deficiencies, multi-modal instances often suffer from incompleteness and inconsistent anomalies, which impacts the performance and generalization abilities of multi-modal learning algorithms.",
                "distance": 0.0285
            },
            {
                "reference": "As technology develops, multi-modal visual data are easier to access, but an underlying problem with the emerging multi-modality techniques is the potential failure of one or more modal data.",
                "distance": 0.0293
            },
            {
                "reference": "Previous work on multimodal representation learning mostly focuses on either uni-modality pre-training or cross-modality fusion, but rarely addresses both simultaneously.",
                "distance": 0.0299
            },
            {
                "reference": "Multimodal learning tasks are challenged by the disparity of information levels between different modalities.",
                "distance": 0.0308
            },
            {
                "reference": "The majority of past multimodal fusion approaches work by either projecting the features of different modalities into the same space or by coordinating representations of each modality through the use of constraints.",
                "distance": 0.0324
            }
        ]
    },
    {
        "target": "Deep neural networks (DNNs) have been effectively used for solving Markovian optimal stopping problems. However, extension of DNN-based methods to non-Markovian settings leads to an increase in state and parameter space, leading to the issue of the curse of dimensionality. Also, efficient state-space transformations for Markovian approximations, facilitated by recurrent neural networks (RNNs), are either not structurally feasible or are challenged by the curse of non-Markovianity.",
        "prediction": "Recurrent neural networks (RNNs) perform exceptionally well on a wide range of problems. However, their high computational and memory requirements pose significant challenges for deployment on resource-limited mobile devices.",
        "queries": [
            7.587495,
            -7.301816
        ],
        "log": [
            {
                "reference": "Recurrent neural networks (RNNs) perform exceptionally well on a wide range of problems. However, their high computational and memory requirements pose significant challenges for deployment on resource-limited mobile devices.",
                "distance": 0.0089
            },
            {
                "reference": "The recurrent connectivity matrices in Recurrent Neural Networks (RNNs) are constrained to be orthogonal or unitary to circumvent the exploding and vanishing gradient problem and allow for stable propagation of signals over long time scales. However, this imposes a limitation on expressivity due to the limited variety of orthogonal transformations.",
                "distance": 0.0211
            },
            {
                "reference": "The use of unitary matrices in artificial neural networks (ANNs) has been suggested as a potential solution to the gradient explosion/vanishing problem and as a method for enabling ANNs to learn long-term correlations in data. This approach seems particularly promising for Recurrent Neural Networks (RNNs).",
                "distance": 0.0284
            },
            {
                "reference": "Recurrent neural networks are used as the letter posterior probability estimator for a hidden Markov model in off-line handwriting recognition systems. However, the training algorithm used, backpropagation through time, requires target outputs to be provided for each frame which is challenging.",
                "distance": 0.0359
            },
            {
                "reference": "The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is a major challenge. Recent methods have attempted to solve this by constraining the transition matrix to be unitary, which ensures that its norm is equal to one, preventing exploding gradients. However, these methods either have limited expressiveness or don't scale well with the size of the network, especially when using stochastic gradient descent with a small mini-batch size.",
                "distance": 0.0491
            },
            {
                "reference": "Recurrent neural networks (RNNs) face two major challenges: they are over-parameterized, increasing the sample complexity of learning and training time, and they are ill-conditioned, causing the the vanishing and exploding gradient problem.",
                "distance": 0.0491
            },
            {
                "reference": "Recurrent neural networks (RNNs) have been uniquely used for several sequential data problems. However, they suffer from the issue of vanishing or exploding gradient problem and recent mitigation attempts have retained an orthogonal or unitary recurrent weight matrix.",
                "distance": 0.0517
            },
            {
                "reference": "Unitary recurrent neural networks (URNNs) have been proposed as a method to overcome the vanishing and exploding gradient problem in modeling data with long-term dependencies. Yet, it's unclear how restrictive the unitary constraint on the possible input-output mappings of such a network is.",
                "distance": 0.0526
            },
            {
                "reference": "Running nonlinear recurrent neural networks (RNNs) for a certain number of steps takes linear time, which limits their efficiency.",
                "distance": 0.0545
            },
            {
                "reference": "Parallelizing Gated Recurrent Unit (GRU) networks is difficult due to their inherent sequential nature. Current efforts, based on traditional parallelization strategies like data-parallel and model-parallel training algorithms, might be performance-limited when dealing with long sequences.",
                "distance": 0.0552
            },
            {
                "reference": "Recurrent neural networks (RNNs) are notoriously difficult to train, with optimization becoming difficult due to the well-studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies.",
                "distance": 0.074
            },
            {
                "reference": "Recurrent Neural Networks (RNNs) can be difficult to train accurately and efficiently. Unitary RNNs have improved accuracy by restricting the range of the state transition matrix's singular values but also increased the model size. Gated RNNs have reached state-of-the-art accuracy by increasing the number of parameters but as a result, the model size becomes significantly bigger.",
                "distance": 0.0747
            },
            {
                "reference": "Recurrent neural networks (RNNs) are suitable for long sequential chains of reasoning but don't naturally incorporate program structure and thus perform worse on tasks like code completion, bug finding, and program repair. Graph neural networks (GNNs) are beneficial for leveraging program structure, but they aren't well-suited for tasks like program execution that require more sequential reasoning steps than number of GNN propagation steps.",
                "distance": 0.0774
            },
            {
                "reference": "Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have been proposed as a tool to avoid the vanishing and exploding gradient problems common in traditional recurrent neural networks. However, previous uRNN models used recurrence matrices that are a product of parameterized unitary matrices, limiting their representational capacity.",
                "distance": 0.0781
            },
            {
                "reference": "In learning with recurrent or very deep feed-forward networks, incorporating unitary matrices in each layer can maintain long-range stability. However, constraining network parameters to be unitary may result in expensive parameterizations or increased training runtime.",
                "distance": 0.0831
            },
            {
                "reference": "Recurrent neural networks (RNN) are used extensively across various problems, but as data quantity and compute availability increase, so does the size of the models, making them difficult to deploy on devices like mobile phones and embedded systems due to their size and the time it takes to evaluate them.",
                "distance": 0.084
            },
            {
                "reference": "Training Recurrent Neural Networks (RNNs) on long sequences often face challenges like slow inference, vanishing gradients, and difficulty in capturing long term dependencies. These issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time.",
                "distance": 0.0852
            },
            {
                "reference": "The problem of effectively training recurrent neural networks (RNNs) for complex and difficult sequence modelling problems, particularly those with long-term dependencies, has been a long-standing issue in the field.",
                "distance": 0.0863
            },
            {
                "reference": "Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, they're difficult to train due to the gradient vanishing and exploding problems and struggle to learn long-term patterns. LSTM and GRU were developed to address these issues, but the use of hyperbolic tangent and sigmoid action functions lead to gradient decay across layers.",
                "distance": 0.0899
            },
            {
                "reference": "Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length.",
                "distance": 0.0903
            }
        ]
    },
    {
        "target": "Hedonic diversity games are a variant of classical hedonic games aimed at modeling diversity and fairness questions. Previous works mainly focused on the case with two diversity classes and provided some initial complexity-theoretic and existential results regarding Nash and individually stable outcomes.",
        "prediction": "Traditional network games cogently represent strategic interactions among agents with sparse dependencies on the actions of others. However, such games struggle to efficiently model real networks that exhibit a multi-scale structure where agents can be grouped into communities, with sparsity in their inter-group interactions.",
        "queries": [
            3.912636,
            -1.52036
        ],
        "log": [
            {
                "reference": "Traditional network games cogently represent strategic interactions among agents with sparse dependencies on the actions of others. However, such games struggle to efficiently model real networks that exhibit a multi-scale structure where agents can be grouped into communities, with sparsity in their inter-group interactions.",
                "distance": 0.0276
            },
            {
                "reference": "Modeling strategic interactions between individuals or organizations as network games is an established approach. However, inferring network structure from observed game outcomes generally relies on methods that require knowledge of the associated utility function. In real-world situations, obtaining this utility function is often unrealistic.",
                "distance": 0.036
            },
            {
                "reference": "Finding out under what conditions the core of a game is guaranteed to be non-empty is significant in cooperative game theory. Linear Programming (LP) duality and convexity are useful tools for this purpose. The notions of conservative core, refined core, and optimistic core in overlapping coalitions (OCF games) is of importance.",
                "distance": 0.0494
            },
            {
                "reference": "In many multi-agent settings, learning inherently involves repeated play, bringing into question the naive application of single play Nash equilibria in multi-agent learning. This suggests the need for bargaining principles which involve mutual give-and-take.",
                "distance": 0.0634
            },
            {
                "reference": "Aggregative games provide a rich abstraction to model strategic multi-agent interactions, often involving players whose actions depend on the aggregate behaviors of other participants. However, learning these games remains challenging, especially when the players' interactions form a connected digraph.",
                "distance": 0.0836
            },
            {
                "reference": "Potential games, where all agent utilities are perfectly aligned with each other via a common potential function, have been widely studied. However, the transposition of this framework into the setting of Markov Games and the understanding of multi-agent coordination with and without state dependence are not clear.",
                "distance": 0.0879
            }
        ]
    },
    {
        "target": "Crowdsourcing is a common method for handling complex tasks using human intelligence, but it often leads to privacy leaks due to the sharing of worker data.",
        "prediction": "Crowdsourcing is a computing paradigm aimed at solving complex problems using human effort. However, achieving a balance between budget and quality is crucially important in crowdsourcing. When faced with a new task, relevant knowledge can greatly aid in its accomplishment.",
        "queries": [
            -5.954379,
            -2.579888
        ],
        "log": [
            {
                "reference": "Crowdsourcing is a computing paradigm aimed at solving complex problems using human effort. However, achieving a balance between budget and quality is crucially important in crowdsourcing. When faced with a new task, relevant knowledge can greatly aid in its accomplishment.",
                "distance": 0.0093
            },
            {
                "reference": "Over the past decade, crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. It has been attracting a lot of attention and numerous related projects in the database community have been undertaken.",
                "distance": 0.0106
            },
            {
                "reference": "Quality control in crowdsourcing is a challenge, as work results from crowd workers who are not necessarily highly skilled or motivated can vary in quality. Current methods to aggregate multiple answers from such tasks are not suitable for more general tasks with unstructured response formats such as article writing or program coding.",
                "distance": 0.0162
            },
            {
                "reference": "Crowdsourcing platforms support job assignments by relying on workers' search capabilities. Recommender systems can support workers' decisions to improve the quality and outcome for both worker and requester. However, creating genuine task recommendations requires identification and analysis of task similarities.",
                "distance": 0.0198
            },
            {
                "reference": "In crowdsourcing, output agreement mechanisms are often used to incentivize participants to provide truthful answers when the correct answer is held by the majority. However, there is a lack of strategies to elicit effort, especially from a population of workers with heterogeneous cost of effort exertion.",
                "distance": 0.0235
            },
            {
                "reference": "Crowdsourcing has become essential in a wide range of Web applications, but one of the biggest challenges is the quality of crowd answers due to wide-ranging levels of expertise and the presence of faulty workers. Various techniques for quality control have been proposed, but a post-processing phase in which crowd answers are validated by experts, who are often limited and incur high costs, is still required.",
                "distance": 0.0236
            },
            {
                "reference": "The majority of popular filter lists used to assist and protect web users are crowd-sourced, and this strategy can falter in regions of the web where few people contribute due to linguistic or economic reasons, such as regions serving languages with few speakers.",
                "distance": 0.025
            },
            {
                "reference": "In the field of micro-task crowdsourcing, tasks are performed by several workers, with measures of agreement among workers typically being used to estimate data reliability. However, existing agreement measures are known to have many issues.",
                "distance": 0.0302
            },
            {
                "reference": "In the crowdsourcing task of learning the answer to simple multiple-choice microtasks, one often needs to ask multiple workers to answer the same microtask. A stopping rule is needed for deciding when to stop and output an answer or ask one more worker.",
                "distance": 0.0313
            },
            {
                "reference": "In crowdsourcing data collection, it is often assumed that crowd workers complete tasks independently without direct communication.",
                "distance": 0.0314
            },
            {
                "reference": "Quality control for crowdsourcing systems has been identified as a significant challenge.",
                "distance": 0.0324
            },
            {
                "reference": "Crowds are increasingly being used to solve complex problems, and they are often characterized by their size and diversity. However, the relationship between these two characteristics and performance is often paradoxical, and not well-understood.",
                "distance": 0.037
            },
            {
                "reference": "Crowdsourcing settings with malicious actors can hinder the effectiveness of crowdsourcing platforms. However, detecting and dealing with these malicious actors can greatly increase their robustness.",
                "distance": 0.0393
            },
            {
                "reference": "Real-time crowdsourcing techniques have increased the potential application areas of crowdsourcing and human computation, but they are still not viable for tasks requiring machine-level speeds due to human perception and response time limitations. Additionally, fundamental bounds of these human limitations prevent addressing problems in milliseconds, instead of seconds.",
                "distance": 0.0403
            },
            {
                "reference": "Crowd work provides solutions to complex problems effectively, but it has been observed that its efficiency can be improved with feedback; particularly correctness feedback. However, feedback, especially when generated by experts, becomes costly and a scaling issue.",
                "distance": 0.0403
            },
            {
                "reference": "Quality insurance for crowd answers is a significant challenge as crowdsourcing involves a heterogeneous population, including many faulty workers. While various techniques for quality control have been developed, a post-processing phase, often requiring expert validation, remains necessary, making the process costly and time-limited.",
                "distance": 0.0407
            },
            {
                "reference": "Current crowdsourcing research concentrates on improving the quality of responses. Identifying high and low quality workers would allow correction of errors in tasks. However, distinguishing between these types is challenging when no measure of individual success exists.",
                "distance": 0.041
            },
            {
                "reference": "Crowdsourcing and human computation are increasingly being used for a variety of applications, helping generate large datasets and contributing to tasks that currently cannot be fully automated.",
                "distance": 0.0414
            },
            {
                "reference": "Crowdsourcing applications are widespread in various scenarios but the quality of the outcome depends on varying design parameters. Conducting experiments to evaluate them is difficult as there are no guidelines and experiments are often conducted in an ad-hoc manner which may not align well with the best possible setting.",
                "distance": 0.0422
            },
            {
                "reference": "Existing crowdsourcing methodologies fail to adequately capture the wide range of opinions and perspectives represented in diverse human interpretations. Traditional methods often discard dissenting votes, resulting in a limited and discrete concept of \u2018truth'.",
                "distance": 0.0422
            }
        ]
    },
    {
        "target": "Information Extraction tasks such as named entity recognition, relation extraction, and event extraction were previously solved with different models due to diverse task output structures.",
        "prediction": "Expense reimbursement is a time-consuming and labor-intensive process. The extraction of relevant named entities from document images with diverse formatting and unconstrained layouts is challenging for image-based data mining and other information retrieval tasks and traditional language modeling techniques aren't effective due to the absence of linguistic context.",
        "queries": [
            -4.231345,
            4.309855
        ],
        "log": [
            {
                "reference": "Expense reimbursement is a time-consuming and labor-intensive process. The extraction of relevant named entities from document images with diverse formatting and unconstrained layouts is challenging for image-based data mining and other information retrieval tasks and traditional language modeling techniques aren't effective due to the absence of linguistic context.",
                "distance": 0.0458
            },
            {
                "reference": "Learning relational facts from text is a core task of information extraction, with diverse applications in several fields. Traditionally, this task has required significant human involvement, such as hand-annotating training instances and formulating extraction rules. While supervised techniques can achieve high performance, they often fail to generalize beyond their training domains and frequently need to be re-engineered for each specific application.",
                "distance": 0.0984
            }
        ]
    },
    {
        "target": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks, but with limited sample numbers, they still suffer from poor accuracy.",
        "prediction": "Large language models (LLMs) are capable of performing high-level analogical reasoning, but struggle in scenarios that require reasoning over multiple objects or facts or making sequences of logical deductions.",
        "queries": [
            -2.653318,
            14.442258
        ],
        "log": [
            {
                "reference": "Large language models (LLMs) are capable of performing high-level analogical reasoning, but struggle in scenarios that require reasoning over multiple objects or facts or making sequences of logical deductions.",
                "distance": 0.0432
            },
            {
                "reference": "Despite their excellent performance, large language models (LLMs) face challenges related to their preference for simple, superficial textual relations over the full semantic complexity of the problem and suffer from a weak ability to generalize out of their training domain.",
                "distance": 0.0602
            },
            {
                "reference": "Recent work has shown that large pretrained Language Models (LMs) perform well on a range of Natural Language Processing (NLP) tasks and have started improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning. However, the underlying capabilities of these LMs are still unclear.",
                "distance": 0.0628
            },
            {
                "reference": "Language models (LMs) have achieved significant performance on many Natural Language Understanding (NLU) tasks, but there are ongoing debates about their reasoning capability in NLU.",
                "distance": 0.0753
            },
            {
                "reference": "Real-world applications often involve complex tasks that cannot be easily handled via a single run of a large language model (LLM). Recent work has shown that chaining multiple LLM runs together can accomplish these tasks with increased transparency and control, yet it is unknown what users need when authoring their own LLM chains.",
                "distance": 0.0994
            }
        ]
    },
    {
        "target": "Applications of machine learning techniques for materials modeling often use functions known to be equivariant or invariant to specific symmetries. Graph neural networks (GNNs) have been successful in such tasks, but enforcing symmetries via the model architecture often reduces their expressivity, scalability, and comprehensibility.",
        "prediction": "In large-scale E-commerce retrieval, Graph Neural Networks (GNNs) have become state-of-the-art due to their feature extraction and relational reasoning capabilities. However, conventional GNN-based methods suffer from low training efficiency in scenarios with billions of entities and tens of billions of relationships, leading to the use of only shallow graph algorithms which limits representational capabilities and retrieval quality.",
        "queries": [
            -9.061019,
            -17.148199
        ],
        "log": [
            {
                "reference": "In large-scale E-commerce retrieval, Graph Neural Networks (GNNs) have become state-of-the-art due to their feature extraction and relational reasoning capabilities. However, conventional GNN-based methods suffer from low training efficiency in scenarios with billions of entities and tens of billions of relationships, leading to the use of only shallow graph algorithms which limits representational capabilities and retrieval quality.",
                "distance": 0.0057
            },
            {
                "reference": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks but their memory complexity has become a major obstacle due to the immense number of nodes, edges, and intermediate activations. Prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs.",
                "distance": 0.008
            },
            {
                "reference": "Most current GNN-based algorithms learn node representations incorporating k-hop neighbors' information for higher-order structural features. However, due to the high time complexity of querying k-hop neighbors, these graph algorithms cannot be deployed in giant dense temporal networks for millisecond-level inference, which limits their application in areas like financial fraud detection.",
                "distance": 0.0121
            },
            {
                "reference": "Graph Neural Networks (GNNs) often struggle with weak generalization due to sparsely labeled data. Data augmentation is a prevalent method to improve model generalization in many domains, but it is challenging to design effective augmentation on graphs due to their non-Euclidean nature and dependencies between samples.",
                "distance": 0.0172
            },
            {
                "reference": "Graph neural networks (GNNs) are prevalent in many web research problems, but current designs struggle with issues like over-smoothing, lack of explanation for their predictions, difficulties in integrating graph topology into the ODEs, and dependency on heuristic steps for numerical stability.",
                "distance": 0.0225
            },
            {
                "reference": "Graph neural networks (GNNs) are powerful representation learning models on graphs, using a global model for information aggregation from neighboring nodes. However, different nodes have different local contexts within the graph, leading to diverse node distributions across the graph.",
                "distance": 0.0227
            },
            {
                "reference": "Graph neural networks (GNNs) have achieved great success in many graph-based applications, yet their high sparsity level and enormous size hinder their applications under industrial scenarios. Existing solutions for large-scale graphs adopt a fixed K-hop neighborhood for each node, which leads to over-smoothing when adopting large propagation depths for nodes within sparse regions.",
                "distance": 0.0236
            },
            {
                "reference": "Graph Neural Networks (GNNs) have been used in multi-hop question answering tasks, but they have limitations such as unnecessary updates and overly simple edge construction that prevent accurate and interpretable answer span extraction.",
                "distance": 0.0263
            },
            {
                "reference": "Graph neural networks (GNNs) have been extensively studied for learning inter-connected data. However, GNNs are known to face issues related to over-squashing, heterophily, handling of long-range dependencies, edge incompleteness, and particularly, the absence of graph structures in data.",
                "distance": 0.0269
            },
            {
                "reference": "Graph neural networks (GNNs) model local graph structures and capture hierarchical patterns by aggregating information from neighbor nodes. However, determining the optimal number of aggregation iterations for each node in complex graphs and with sparse features is challenging.",
                "distance": 0.0269
            },
            {
                "reference": "Graph Neural Networks (GNNs) have been successful in tasks over graph data, but their performance can be significantly degraded by attackers modifying the graph structure. While existing methods use either raw features or representations learned by supervised GNNs to model edge weights, these strategies face problems such as inability to represent various properties of nodes and vulnerability to structural perturbations.",
                "distance": 0.0269
            },
            {
                "reference": "Heterogeneous Graph Neural Networks (HGNNs) have achieved remarkable performance in many tasks, but their success typically relies on the assumption that the original heterogeneous graph structure is reliable. However, in reality, these graphs are often noisy or incomplete.",
                "distance": 0.0277
            },
            {
                "reference": "Graph neural networks (GNNs) are growing in size and complexity, making their training and inference increasingly expensive. Existing network weight pruning algorithms cannot address the space and computational challenges caused by the size and connectivity of the graph.",
                "distance": 0.0287
            },
            {
                "reference": "Graph Neural Networks (GNNs) strive to aggregate information from local neighborhoods for informative node representation. However, the issue of under-adequate aggregation of neighborhood information for nodes with fewer neighbors remains.",
                "distance": 0.0291
            },
            {
                "reference": "Multi-scale Graph Neural Networks (GNNs) are a promising approach for mitigating the over-smoothing problem in GNNs, but little explanation exists as to why it works empirically from the viewpoint of learning theory.",
                "distance": 0.0296
            },
            {
                "reference": "Graph Neural Networks (GNNs) are pivotal in expressive power and generalization but their optimization properties are less well understood. There's scant knowledge about different factors that can affect the GNNs' training speed.",
                "distance": 0.0329
            },
            {
                "reference": "Graph neural networks (GNNs), which have shown remarkable performance in graph analytics tasks, are trained in existing systems such as PyG and DGL using a tensor-centric programming model and manually written operators. This approach has been identified as having poor usability and inferior efficiency due to a large semantic gap between the API and the GNN models, high memory consumption, and massive data movement.",
                "distance": 0.034
            },
            {
                "reference": "Graph neural networks (GNNs) have been proven to be effective in various network-related tasks. Most existing GNNs usually exploit the low-frequency signals of node features, raising questions about the effectiveness of only using low-frequency information in real-world applications.",
                "distance": 0.036
            },
            {
                "reference": "Graph neural networks (GNNs) have been successfully applied to various problems, but the theoretical understanding of their generalization ability, especially in non-IID conditions and their impact on node-level tasks, is limited.",
                "distance": 0.0373
            },
            {
                "reference": "Graphs in many domains follow a long-tailed distribution in their node degrees resulting in many tail nodes with a small degree. Current Graph Neural Networks (GNNs) treat all nodes uniformly and therefore, do not cater effectively for tail nodes which have limited structural information (i.e., links).",
                "distance": 0.0376
            }
        ]
    },
    {
        "target": "Recent Lexical Substitution (LS) methods based on pretrained language models generate potential substitutes for a target word by analysing its contextual surroundings. However, these methods often overlook the preservation of the sentence's meaning when generating the substitutes.",
        "prediction": "Words play a critical role in natural language understanding, encompassing syntactic and semantic processing. For such systems, either already knowing the words that might appear in the text or being able to learn new ones is crucial. The development of a knowledge base of permissible words and morphological rules is an important task in computational linguistics.",
        "queries": [
            -1.414303,
            9.122162
        ],
        "log": [
            {
                "reference": "Words play a critical role in natural language understanding, encompassing syntactic and semantic processing. For such systems, either already knowing the words that might appear in the text or being able to learn new ones is crucial. The development of a knowledge base of permissible words and morphological rules is an important task in computational linguistics.",
                "distance": 0.0673
            },
            {
                "reference": "Complex Word Identification (CWI) aims to detect words within a text that a reader may find difficult to understand. CWI systems can improve text simplification, readability prediction, and vocabulary acquisition modelling. However, the difficulty of a word is a subjective notion that relies on a reader's first language, proficiency, and reading experience.",
                "distance": 0.0765
            },
            {
                "reference": "Identifying complex words (CWs) is a crucial step in lexical simplification, the automatic replacement of complex words with simpler alternatives. Incorrect identification can either lead to a loss of meaning or result in the final text being too complex.",
                "distance": 0.0792
            },
            {
                "reference": "Complex word identification (CWI) is a cornerstone process for proper text simplification, but its application is difficult due to the scarcity of available datasets. These existing datasets vary greatly in terms of domains and languages, making it challenging to develop a robust model that generalizes across a wide array of input examples.",
                "distance": 0.0792
            },
            {
                "reference": "The task of identifying complex words (CWs) is important for lexical simplification, but it is often carried out without any evaluation of success. Prior to this work, there was no standard corpus or evaluation technique for the CW identification task.",
                "distance": 0.0794
            },
            {
                "reference": "The Lexical Substitution task involves selecting and ranking lexical paraphrases for a target word in a given sentential context. There are existing models proposed that aim to solve this task.",
                "distance": 0.0969
            }
        ]
    },
    {
        "target": "Generative pre-trained Transformer (GPT) has demonstrated success in natural language processing and adaptations have been made for molecular modeling. However, the interplay of text and symbolic molecular representations in these systems isn't well-explored.",
        "prediction": "While large pretrained language models have increasingly become efficient at few-shot learning, this is typically achieved by augmenting the input with priming text which includes task-specific descriptions and examples. Presenting examples in the correct order has been identified as significant for generalization, which previous methods may not have addressed adequately.",
        "queries": [
            -2.048218,
            14.791756
        ],
        "log": [
            {
                "reference": "While large pretrained language models have increasingly become efficient at few-shot learning, this is typically achieved by augmenting the input with priming text which includes task-specific descriptions and examples. Presenting examples in the correct order has been identified as significant for generalization, which previous methods may not have addressed adequately.",
                "distance": 0.0577
            },
            {
                "reference": "Pre-trained language models have been successful for a range of natural language processing tasks. However, existing methods either need fine-tuning on downstream labelled datasets or manual construction of prompts.",
                "distance": 0.071
            },
            {
                "reference": "Existing prompt engineering methods for aligning pre-trained language models to specific tasks require significant amounts of labeled data, or access to model parameters, or both.",
                "distance": 0.0736
            },
            {
                "reference": "Pre-training language models (LMs) on large-scale unlabeled text data improves their performance on downstream tasks more than models directly trained on these tasks.",
                "distance": 0.0878
            },
            {
                "reference": "Pre-trained models have been effectively applied to dialogue tasks. However, directly fine-tuning these models for each new task in increasingly diverse online chit-chat scenarios can exhaust the capacity of embedded device systems, causing knowledge forgetting in pre-trained models and interference among different dialogue tasks.",
                "distance": 0.0938
            }
        ]
    },
    {
        "target": "Natural language understanding (NLU) models often suffer from unintended dataset biases. Ensemble-based debiasing methods, particularly the product-of-experts (PoE), have been successful in bias mitigation, but these methods usually impact top-level logits without addressing biased attention patterns, which plays a crucial role in providing robust prediction.",
        "prediction": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has developed methods to address these issues when knowledge of the bias is known.",
        "queries": [
            -3.487933,
            13.407997
        ],
        "log": [
            {
                "reference": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has developed methods to address these issues when knowledge of the bias is known.",
                "distance": 0.0375
            },
            {
                "reference": "Automatically generating compilable programs is a significant task for both computational linguistics and automated software engineering. Prevailing deep-learning approaches, such as CodeGPT, PLBART, and CodeT5, suffer from a lack of consideration for the compilability of the generated programs.",
                "distance": 0.0549
            },
            {
                "reference": "Current models that learn distributed representation of source code use highly structured source code representations like syntax trees and paths. Previous research work has explored various methods of path encoding.",
                "distance": 0.0717
            }
        ]
    },
    {
        "target": "Controlled generation in text-based models often involves predicting desired stylistic or semantic attributes, like reducing content toxicity. However, the performance of such models may falter if there is a distributional disparity between training data and actual use-case scenarios, determined by user prompts.",
        "prediction": "Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-related classifiers or sampling a representation from relevant discrete samples. However, they are not effective enough in modeling both the latent space and the control, leaving controlled text with low quality and diversity.",
        "queries": [
            -3.218296,
            13.279861
        ],
        "log": [
            {
                "reference": "Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-related classifiers or sampling a representation from relevant discrete samples. However, they are not effective enough in modeling both the latent space and the control, leaving controlled text with low quality and diversity.",
                "distance": 0.0304
            },
            {
                "reference": "Controlled text perturbation is useful for evaluating and improving model generalizability. However, current techniques require training a separate model for every desired perturbation, making the process expensive and difficult to generalize.",
                "distance": 0.032
            },
            {
                "reference": "Existing methods for multi-aspect controllable text generation achieve complex multi-aspect control by fusing multiple controllers learned from a single-aspect, but these methods suffer from attribute degeneration caused by the mutual interference of the controllers.",
                "distance": 0.0629
            },
            {
                "reference": "In natural language processing, controlling style by disentangling the latent space is a crucial task, but challenging due to training difficulties and the need for style-specific discriminators when using adversarial training approaches. Additionally, training bias issues arise when different style types have specific style values that consistently occur together.",
                "distance": 0.0704
            }
        ]
    },
    {
        "target": "Radiology report generation, which involves generating free-text descriptions for a set of radiographs, is challenging due to the need to maintain consistency between the images and the lengthy report. Previous research attempted to solve this issue with planning-based methods that generated reports based on high-level plans, but these plans typically only contained major observations and lacked necessary information such as observation characteristics and preliminary diagnoses.",
        "prediction": "Radiology reports play a critical role in communicating medical findings to physicians, however, writing these reports is time-consuming and prone to errors. Thus, automatic impression generation has emerged as a research direction. Previous studies have focused on introducing salient word information to guide the selection of key content in radiology findings.",
        "queries": [
            15.250571,
            -9.752527
        ],
        "log": [
            {
                "reference": "Radiology reports play a critical role in communicating medical findings to physicians, however, writing these reports is time-consuming and prone to errors. Thus, automatic impression generation has emerged as a research direction. Previous studies have focused on introducing salient word information to guide the selection of key content in radiology findings.",
                "distance": 0.0096
            },
            {
                "reference": "Time-sensitive communication of critical imaging findings from radiology reports to referring physicians is crucial for patient safety. However, these findings are often recorded in free-text format, thereby relying on verbal communication, which is not always successful in conveying the information. Presently, manual efforts are used to add new critical findings to a followup list.",
                "distance": 0.0101
            },
            {
                "reference": "Diagnostic radiology reports, while increasingly made available to patients and their family members, are often incomprehensible to lay recipients, resulting in ineffective communication.",
                "distance": 0.0154
            },
            {
                "reference": "The problem of radiology report generation has traditionally involved generating the full report from an image at once.",
                "distance": 0.0177
            },
            {
                "reference": "Automatic impression generation in radiology aims to alleviate the workload of radiologists and reduce repetitive human labor in impression writing. However, most existing methods pay less attention to the radiology images and do not leverage the valuable observations they can provide.",
                "distance": 0.0201
            },
            {
                "reference": "Existing radiology report generation systems aim to automatically describe the findings in medical images, potentially reducing the workload of radiologists. However, they may not guarantee chronological consistency, where images are described in their correct time order, which is crucial for certain diagnostic reports.",
                "distance": 0.026
            },
            {
                "reference": "Automated radiology report generation can reduce clinicians' manual review time and streamline clinical care. However, traditional abstractive methods often generate fluent, but clinically incorrect radiology reports.",
                "distance": 0.0316
            },
            {
                "reference": "Automatically generating the IMPRESSION section of a radiology report, which summarizes a radiologist's reasoning and conclusions, requires a cascade of tasks including the acquisition of salient content from the report. Prior research on radiology report summarization has used single-step end-to-end models for this task.",
                "distance": 0.0376
            },
            {
                "reference": "Automatic medical image report generation has the potential to alleviate radiologists' workload. Existing work on report generation often trains encoder-decoder networks for complete reports. However, these models can be affected by issues such as data bias, label imbalance and text generation problems like repetition.",
                "distance": 0.0433
            },
            {
                "reference": "Existing report generation methods in radiology can struggle with providing clinically accurate reports, particularly when dealing with out-of-distribution data. Efficiency of these systems can also be a challenge.",
                "distance": 0.0448
            },
            {
                "reference": "Existing medical report generation methods are mainly supervised and heavily rely on paired image-report data, which is time-consuming and expensive to produce in the medical field.",
                "distance": 0.047
            },
            {
                "reference": "The impression section of a radiology report is essential for radiologists to communicate findings to physicians. Its manual summarization is time-consuming and can be prone to error, especially for inexperienced radiologists. While previous automatic summaries have leveraged external knowledge (i.e., static pre-defined clinical ontologies) in the encoder-decoder process via a separate encoder, they failed to exploit relations with original findings.",
                "distance": 0.0481
            },
            {
                "reference": "Generating automatic medical reports that describe findings from medical images is challenging due to the imbalances in medical datasets. The incidence rates among diseases vary and the ratios of abnormalities to normalities are significantly imbalanced.",
                "distance": 0.0489
            },
            {
                "reference": "Diagnosing diseases from medical radiographs and writing reports requires professional knowledge and is time-consuming. Automatic medical report generation has gained interest, but identifying diseases, their sizes, locations and other medical description patterns for high-quality reports is challenging.",
                "distance": 0.0557
            },
            {
                "reference": "Automating the generation of medical reports from chest X-ray image inputs is a critical yet time-consuming task for radiologists. Existing medical report generation efforts tend to produce human-readable reports but lack in clinical accuracy.",
                "distance": 0.0568
            },
            {
                "reference": "Automatic generation of ophthalmic reports using data-driven neural networks has potential in clinical practice, but prior methods have not incorporated the clinical knowledge that ophthalmologists use when writing a report.",
                "distance": 0.0585
            },
            {
                "reference": "Management of radiology requests in larger clinical contexts is characterized by a complex and distributed workflow. In many clinics, including our partner hospital, these processes often still rely on exchanging physical papers and forms, which makes case or patient data challenging access, leading to phone calls with long waiting queues.",
                "distance": 0.0594
            },
            {
                "reference": "Generating long and coherent reports to describe medical images is challenging. It involves bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions.",
                "distance": 0.0634
            },
            {
                "reference": "Generating clinical reports from raw recordings such as X-rays and electroencephalogram is a routine task for doctors, but it's often time-consuming to write accurate and detailed reports. Most existing methods have issues like generating errors, not saving time when doctors want additional information included or not being tailored to individual doctors' preferences.",
                "distance": 0.0639
            },
            {
                "reference": "The automatic generation of medical reports from images has the potential to support clinical practice, however, existing medical report generation (MRG) benchmarks lack both explainable annotations and reliable evaluation tools. Existing methods are unable to predict reports with accurate explanation, undermining the trustworthiness of the diagnostic methods, and comparison among the predicted reports from different MRG methods is unreliable using the evaluation metrics of natural language generation.",
                "distance": 0.0657
            }
        ]
    },
    {
        "target": "Most existing text generation models follow the sequence-to-sequence paradigm while Generative Grammar suggests that humans generate natural language texts by learning language grammar.",
        "prediction": "Existing Natural Language Understanding (NLU) models incorporate dataset biases, which results in strong performance on in-distribution test sets but poor performance on out-of-distribution ones.",
        "queries": [
            -3.350983,
            12.754032
        ],
        "log": [
            {
                "reference": "Existing Natural Language Understanding (NLU) models incorporate dataset biases, which results in strong performance on in-distribution test sets but poor performance on out-of-distribution ones.",
                "distance": 0.055
            },
            {
                "reference": "Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models, which often make inferences using information from multiple sources. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied.",
                "distance": 0.0981
            },
            {
                "reference": "Studies have identified that strong natural language understanding (NLU) models tend to rely on unwanted dataset biases, which leads to a failure in generalizing to out-of-domain datasets and poor performance in real-world scenarios.",
                "distance": 0.0997
            }
        ]
    },
    {
        "target": "This paper addresses the SemEval 2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER II) in four monolingual tracks (English, Spanish, French, and Portuguese), which presents a low context setting and fine-grained taxonomy.",
        "prediction": "The task of Japanese Word Sense Disambiguation (WSD) is being addressed by participating systems in SemEval-2.",
        "queries": [
            -1.615757,
            5.537791
        ],
        "log": [
            {
                "reference": "The task of Japanese Word Sense Disambiguation (WSD) is being addressed by participating systems in SemEval-2.",
                "distance": 0.3366
            },
            {
                "reference": "The paper discusses the task of multilingual acronym extraction from documents in scientific and legal domains, a challenge that the authors addressed as part of the SDU@AAAI-22 shared task.",
                "distance": 0.3593
            }
        ]
    },
    {
        "target": "In anonymous social networks, many women experience abuse, discrimination, and sexist expressions. Existing methods which rely on keyword filtering and matching perform poorly in online sexism detection, falling short in identifying implicit stereotypes and discrimination.",
        "prediction": "Toxic language detection systems frequently falsely flag text containing mentions of minority groups as toxic because these groups are often targets of online hate, and also struggle with detecting implicitly toxic language due to over-reliance on spurious correlations.",
        "queries": [
            -23.621777,
            4.704223
        ],
        "log": [
            {
                "reference": "Toxic language detection systems frequently falsely flag text containing mentions of minority groups as toxic because these groups are often targets of online hate, and also struggle with detecting implicitly toxic language due to over-reliance on spurious correlations.",
                "distance": 0.0282
            },
            {
                "reference": "New kinds of abusive language continually emerge in online discussions in response to current events such as COVID-19, and the deployed abuse detection systems should be updated regularly to remain accurate. Existing abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse.",
                "distance": 0.0522
            },
            {
                "reference": "Recognizing offensive text is important for every content management system, particularly for social networks. The majority of prior work approaches this problem as text classification, i.e., if a text excerpt is offensive or not.",
                "distance": 0.0611
            },
            {
                "reference": "With the rising prevalence of offensive language online, especially in Korea noted for its high Internet usage, automatic detection of offensive expressions has become crucial. Yet, the morphological richness and complex syntax of Korean present difficulties in neural model training. Also, most prior studies concentrate on detecting abusive language, neglecting implicit offensiveness and underestimating varied levels of intensity.",
                "distance": 0.0758
            },
            {
                "reference": "Abusive language is a massive problem in online social platforms. The existing techniques for detecting abusive language are ill-suited for comments that contain mixed abusive and non-abusive content, due in part to the lack of datasets that explicitly annotate this mixed content.",
                "distance": 0.0847
            },
            {
                "reference": "Automated hate speech and abuse detection tools are increasingly crucial for ensuring a safer online environment. However, recent studies show these machine learning models may be biased, potentially making accurate decisions based on flawed reasoning.",
                "distance": 0.0892
            },
            {
                "reference": "Profanity detection systems, often list-based, are currently performing poorly due to their inability to adapt to evolving profane slang, identify disguised or partially censored terms, or cope with intentional or unintentional misspellings. They are also subject to assumptions that the definition and use of profane or inappropriate language holds across all contexts, thus providing a one-size-fits-all solution.",
                "distance": 0.0925
            },
            {
                "reference": "Abusive language detection is an emerging field in natural language processing and has received significant attention. However, the automatic detection of implicitly abusive language, abusive language that is not conveyed by abusive words, is not satisfactory.",
                "distance": 0.0959
            }
        ]
    },
    {
        "target": "The AfriSenti SemEval-2023 Task 12: Sentiment Analysis for African Languages consists of three different tasks, covering monolingual, multilingual, and zero-shot sentiment analysis scenarios for African languages.",
        "prediction": "The team participated in SemEval-2016 Task 4 (Sentiment Analysis in Twitter), specifically the Message Polarity Classification subtask.",
        "queries": [
            -22.837734,
            0.789887
        ],
        "log": [
            {
                "reference": "The team participated in SemEval-2016 Task 4 (Sentiment Analysis in Twitter), specifically the Message Polarity Classification subtask.",
                "distance": 0.1422
            },
            {
                "reference": "The UPF-taln team participated in tasks 10 and 11 of SemEval 2015 focusing on 'Sentiment Analysis in Twitter' and 'Sentiment Analysis of Figurative Language in Twitter'.",
                "distance": 0.1458
            }
        ]
    },
    {
        "target": "Designing a student dashboard for intelligent language tutoring systems that transparently offers information on students' progress and performance, while also linking to the overarching functional goal such as foreign language teaching, is challenging; especially when adapting to different learning styles and fulfilling additional requirements for blended learning environments.",
        "prediction": "Peer tutor learning has been effectively adopted in various learning contexts. Recently, it has drawn attention from the CHI community due to its potential to be advanced through computer-mediated online video technologies.",
        "queries": [
            -12.369404,
            9.409719
        ],
        "log": [
            {
                "reference": "Peer tutor learning has been effectively adopted in various learning contexts. Recently, it has drawn attention from the CHI community due to its potential to be advanced through computer-mediated online video technologies.",
                "distance": 0.104
            },
            {
                "reference": "In the e-learning environment, providing personalization to students' activities is essential. Two primary research directions for achieving this personalization are adaptive educational systems and intelligent tutors (agents).",
                "distance": 0.1091
            }
        ]
    },
    {
        "target": "The classic masked language modeling (MLM) pre-training focuses on reconstructing the exact identity of randomly selected masked subwords.",
        "prediction": "Masked language models (MLMs) such as BERT are widely used for learning contextual representations. However, these models often adopt sampled embeddings as anchors to estimate and inject contextual semantics into representations, which can limit their efficiency and effectiveness.",
        "queries": [
            -1.233451,
            15.449299
        ],
        "log": [
            {
                "reference": "Masked language models (MLMs) such as BERT are widely used for learning contextual representations. However, these models often adopt sampled embeddings as anchors to estimate and inject contextual semantics into representations, which can limit their efficiency and effectiveness.",
                "distance": 0.0194
            },
            {
                "reference": "Masked language modeling (MLM) is widely used for pre-training bidirectional representations, but it incurs substantial training costs.",
                "distance": 0.0256
            },
            {
                "reference": "Masked Language Models (MLM) pre-trained on massive datasets often undergo an additional training phase called the domain or task adaptation step before fine-tuning for the final task. However, the adaptation step is generally time-consuming requiring several GPU days.",
                "distance": 0.0263
            },
            {
                "reference": "Whole word masking (WWM) improves the performance of the BERT model for English language by masking all subwords corresponding to a word at once. For Chinese language, the concept of a subword does not exist as each token is an atomic character, leading to a different meaning composition of words as they consist of multiple characters.",
                "distance": 0.0309
            },
            {
                "reference": "Pre-trained Masked Language Models (MLMs) such as BERT and RoBERTa have revolutionized natural language understanding. However, these models often output an anisotropic distribution of token representations that occupies a narrow subset of the representation space, which is not ideal for tasks requiring discriminative semantic meanings of distinct tokens.",
                "distance": 0.0357
            },
            {
                "reference": "Previous work has shown that pretrained Masked Language Models (MLMs) can be expanded to new languages by learning a new set of embeddings, keeping the transformer body frozen. Nonetheless, despite learning a small subset of parameters, this approach is not compute-efficient.",
                "distance": 0.0358
            },
            {
                "reference": "In the pretraining of Masked Language Models (MLMs) like BERT, a common practice is to mask tokens uniformly at random. This approach may lead to an MLM minimizing its training objective by latching onto shallow local signals, thus causing inefficiency in pretraining and suboptimal downstream performance.",
                "distance": 0.0504
            },
            {
                "reference": "Pre-training of masked language models (MLMs) consumes a massive amount of computation and results in a large carbon footprint. The existing MLMs use [MASK]s as placeholders and gather the contextualized information from unmasked tokens to restore the corrupted information.",
                "distance": 0.0642
            },
            {
                "reference": "Language model pretraining often involves correcting corrupted text sequences, and ELECTRA-style pretraining has been a go-to technique in this direction.",
                "distance": 0.0655
            },
            {
                "reference": "BERT and XLNet are two dominant pre-training models that employ masked language modeling (MLM) and permuted language modeling (PLM) respectively, though each has its limitations; BERT neglects the dependency among predicted tokens, and XLNet doesn't fully utilize the position information of a sentence, causing a position discrepancy between pre-training and fine-tuning.",
                "distance": 0.0739
            },
            {
                "reference": "Pretrained language models acquire useful inductive biases through masks used in the unsupervised fashion for prediction, which are believed to act as cloze reductions for downstream tasks. However, the effectiveness of random masking strategies used in practice has come into question.",
                "distance": 0.086
            },
            {
                "reference": "Pretrained Masked Language Models (MLMs) have been previously found to be ineffective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on tasks such as natural language inference, sentence similarity, or paraphrasing using annotated task data.",
                "distance": 0.0886
            },
            {
                "reference": "Training task-specific language models for learning rich representation of keyphrases from text documents is explored, with different masking strategies for pre-training transformer language models in both discriminative and generative settings.",
                "distance": 0.0952
            }
        ]
    },
    {
        "target": "There is ongoing research on how large-scale pre-trained language models can be adapted in an efficient way to particular tasks with minimal changes to parameters. Lightweight parameters like adapters can be stored and shared as capabilities within corresponding models. The challenge lies with transferring these parameters between tasks to improve performance on new tasks, which requires understanding the similarity between tasks.",
        "prediction": "Semi-supervised learning methods combined with pre-trained language models, such as BERT, have recently achieved impressive performance in text classification tasks, making them an attractive option to reduce annotation costs.",
        "queries": [
            -0.945867,
            15.088212
        ],
        "log": [
            {
                "reference": "Semi-supervised learning methods combined with pre-trained language models, such as BERT, have recently achieved impressive performance in text classification tasks, making them an attractive option to reduce annotation costs.",
                "distance": 0.0353
            },
            {
                "reference": "Existing models like BERT are suboptimal in modeling long-distance dependencies, and suffer from substantial memory consumption and training time.",
                "distance": 0.0515
            },
            {
                "reference": "General-purpose pretrained sentence encoders like BERT are computationally heavy, slow, and expensive to train, making them not ideal for real-world conversational AI applications.",
                "distance": 0.0535
            },
            {
                "reference": "Pre-trained language models such as BERT and GPT have achieved success in many NLP tasks using only monolingual data. However, directly connecting these models for machine translation is challenging because GPT-like models lack a component that supports sequence-to-sequence decoding.",
                "distance": 0.0791
            },
            {
                "reference": "Pre-trained language models such as BERT are effective in various tasks but are computationally intensive, which hinders their real-world applications.",
                "distance": 0.0869
            }
        ]
    },
    {
        "target": "When fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions, its generalization performance improves, thereby permitting the construction of a universal language model that can work across task boundaries.",
        "prediction": "Finetuning pretrained language models (PLMs) is essential for their effectiveness in downstream tasks. However, PLMs may overfit the pretraining tasks and data, leading to suboptimal performance on target downstream tasks due to the existing gap.",
        "queries": [
            -2.787077,
            15.775719
        ],
        "log": [
            {
                "reference": "Finetuning pretrained language models (PLMs) is essential for their effectiveness in downstream tasks. However, PLMs may overfit the pretraining tasks and data, leading to suboptimal performance on target downstream tasks due to the existing gap.",
                "distance": 0.0154
            },
            {
                "reference": "Fine-tuning of pretrained language models (PLMs) is prone to overfitting in low resource scenarios due to the huge amount of parameters.",
                "distance": 0.0178
            },
            {
                "reference": "Pre-training language models (PLMs) with in-domain texts is a common approach to enhancing their performance on specific downstream tasks, such as those in the biomedical or scientific domains. However, this requires substantial in-domain data and training resources, and the training process must be repeated when new PLMs are introduced, which is resource-intensive and time-consuming.",
                "distance": 0.0254
            },
            {
                "reference": "Pre-trained language models (PLMs) have improved performances of various Chinese natural language processing (NLP) tasks, however, they mostly use a vocabulary provided by Google Chinese BERT which is based on Chinese characters and the masked language model pre-training is based on a single vocabulary, thus limiting its downstream task performances.",
                "distance": 0.0293
            },
            {
                "reference": "Large Pre-trained Language Models (PLMs) are fundamental to the development of language understanding technology. However, the use of PLMs for Hebrew is limited due to the lack of high-quality training resources compared to English, and the fact that necessary morphological boundaries are not provided by existing PLMs.",
                "distance": 0.034
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have been effectively used in various Natural Language Processing (NLP) tasks with a denoising autoencoder framework being the most successful one, where the models learn to recover the original text from a noise-corrupted version. However, existing studies mainly focus on injecting noises into the input.",
                "distance": 0.0371
            },
            {
                "reference": "Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at language tasks requiring character-level information, even without explicit access to the character composition of tokens.",
                "distance": 0.0394
            },
            {
                "reference": "Pre-trained language models (PLMs) usually incorporate a vast amount of parameters that hinder their deployability across multiple downstream tasks. The parameter-efficient transfer learning methods, which only fine-tune a few parameters, have been proposed to address this.",
                "distance": 0.0459
            },
            {
                "reference": "Pre-trained language models (PLMs) have achieved noteworthy success in Natural Language Processing (NLP), but their large size limits their applicability in practical systems. Knowledge distillation, a technique to compress PLMs by learning a smaller model from a large model, is biased when learning from only a single model.",
                "distance": 0.0472
            },
            {
                "reference": "Semantic parsing, which maps natural language to structured meaning representations, is fine-tuned using a pretrained language model (PLM). However, this approach is inefficient when tasked with multiple downstream tasks, as it needs to store a new set of values for all parameters of the pretrained language models for each separate task. Existing work has explored prefix tuning and bias-term tuning, methods to adapt pretrained language models while keeping most or all of their parameters frozen.",
                "distance": 0.0508
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have proven effective in various Natural Language Processing (NLP) tasks, but they require large amounts of computational resources due to their high number of parameters. Model pruning is a common method for compressing large-scale PLMs; however, most current methods are focused on task-specific knowledge and tend to neglect the essential task-agnostic knowledge during pruning, leading to the catastrophic forgetting problem and poor generalization ability.",
                "distance": 0.0521
            },
            {
                "reference": "Fine-tuning of pretrained language models (PLMs) is utilized in numerous NLP tasks. However, a directly fine-tuned model often contains redundant parameters which contribute less to the downstream task, causing suboptimal performance of the overall model due to the gap between pretraining and downstream tasks.",
                "distance": 0.0546
            },
            {
                "reference": "Domain-specific Pre-trained Language Models (PLMs) have been proposed to enhance task performance within specific domains. However, this Domain Adaptive Pre-training (DAPT) tends to forget the previous general knowledge acquired by general PLMs, which leads to the phenomenon of catastrophic forgetting and sub-optimal performance.",
                "distance": 0.0585
            },
            {
                "reference": "Current large-scale Pre-trained Language Models (PLMs) have two challenges. They can be inefficient in terms of memory and computation, and they can struggle to generalize to out-of-distribution (OOD) data, often relying on data set bias.",
                "distance": 0.0686
            },
            {
                "reference": "Discriminative pre-trained language models (PrLMs) work as denoising auto-encoders with an ennoising process that corrupts texts and a denoising process that restores them. Current studies have focused on optimizing either the ennoising or denoising strategies and treat all training instances equally, ignoring the individual instances' contribution.",
                "distance": 0.0722
            },
            {
                "reference": "Transformer-based pre-trained language models (PLMs) have advanced capacity but also excessive overhead, posing a challenge for resource-constrained devices. Existing statically compressed models do not account for the diverse complexities between input instances, potentially leading to redundancy and inadequacy. Early exit in miniature models also face the issue of balancing between making predictions and serving the deeper layers.",
                "distance": 0.0778
            },
            {
                "reference": "Pretrained text representations, such as context-free word embeddings and contextualized language models, have given rise to the widespread use of the pretrain-finetune paradigm in text mining. However, a significant drawback is the large amount of training data required for fine-tuning pretrained language models (PLMs) on downstream tasks, and the potentially high cost of acquiring human annotations.",
                "distance": 0.0805
            },
            {
                "reference": "Pre-trained language models (PLMs) learn universal language representations through self-supervised training tasks on large-scale corpora, however the quality of word representations tends to depend on their frequency in the corpus. As a result, the embeddings of rare words are often poorly optimized.",
                "distance": 0.0832
            },
            {
                "reference": "Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language.",
                "distance": 0.0839
            },
            {
                "reference": "Pretrained language models (PLMs) have seen remarkable progress in text generation tasks via fine-tuning. However, fine-tuning PLMs in data-scarce situations is challenging and there is a need for a general and lightweight model that can adapt to various text generation tasks.",
                "distance": 0.0848
            }
        ]
    },
    {
        "target": "Deep neural networks, including Transformer-based models, have achieved state-of-the-art performance on various tasks, but their interpretability remains a challenge.",
        "prediction": "Transformer-based models, since 2017, have been widely used in various Natural Language Processing tasks. However, a common limitation is that they cannot automatically capture the information of word order, typically requiring explicit position embeddings.",
        "queries": [
            -0.37873,
            16.414577
        ],
        "log": [
            {
                "reference": "Transformer-based models, since 2017, have been widely used in various Natural Language Processing tasks. However, a common limitation is that they cannot automatically capture the information of word order, typically requiring explicit position embeddings.",
                "distance": 0.0276
            },
            {
                "reference": "Transformer-based models such as Transformer-XL and BERT have achieved success on various natural language processing tasks. However, they suffer from a degeneration problem where contextualized embeddings at the output layer tend to occupy an anisotropic cone in the vector space.",
                "distance": 0.0284
            },
            {
                "reference": "BERTs are ineffective in processing long texts due to quadratically increasing memory and time consumption. Current solutions either use sliding window method or simplify transformers, but these methods fail due to insufficient long-range attentions or they need customized CUDA kernels.",
                "distance": 0.0362
            },
            {
                "reference": "Transformer and its variants have achieved great success in natural language processing. Since Transformer models are large in size, deploying them for real industrial applications presents a challenge.",
                "distance": 0.0427
            },
            {
                "reference": "Language models based on the Transformer architecture are successful, but their representations are observed to have anisotropic properties. Previous studies suggest that these representations occupy a narrow cone.",
                "distance": 0.0483
            },
            {
                "reference": "Extrapolation from short problem instances to longer ones is a key form of out-of-distribution generalization in reasoning tasks, critical in scenarios where longer problem instances are rare, such as theorem proving, solving math problems, and summarizing novels. However, transformer-based language models have shown significant generalization deficiencies when naively fine-tuned on length generalization tasks.",
                "distance": 0.0497
            },
            {
                "reference": "Transformer models have become widely used in natural language processing tasks. However, they require a substantial amount of computing resources to achieve high performance, which limits their applicability for mobile applications, which often have tight constraints on hardware resources and battery usage.",
                "distance": 0.0512
            },
            {
                "reference": "Recent research has shown that increasing the input length or model size can improve the performance of Transformer-based neural models.",
                "distance": 0.0633
            },
            {
                "reference": "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants are not optimal for capturing token distances because the position or distance embeddings used by these methods typically do not hold the precise information of real distances, hindering the modeling of the orders and relations of contexts.",
                "distance": 0.0653
            },
            {
                "reference": "Despite the popularity and success of transformer language models, understanding how they encode latent concepts within learned representations remains a challenge.",
                "distance": 0.0793
            },
            {
                "reference": "Current transformer models face challenges modeling documents that contain large tables, as they are usually limited to 512 tokens. Moreover, more than 20% of relational tables on the web have 20 or more rows, making them rich yet difficult to model for information extraction.",
                "distance": 0.0893
            },
            {
                "reference": "Transformer-based models, although achieving state-of-the-art results in various NLP tasks including document summarization, struggle with memory and compute requirements as the input length grows. This presents challenges when dealing with long document summarization.",
                "distance": 0.0976
            }
        ]
    },
    {
        "target": "The problem of safe reinforcement learning (RL) has gained significant attention, which involves training reward-maximizing agents subject to pre-defined safety constraints. However, the challenge remains in learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining.",
        "prediction": "Safe exploration in reinforcement learning (RL) presents a major problem. During active data collection, partially trained policies must be deployed and these must avoid catastrophically unsafe regions, yet still enable trial and error learning.",
        "queries": [
            9.329752,
            -23.187668
        ],
        "log": [
            {
                "reference": "Safe exploration in reinforcement learning (RL) presents a major problem. During active data collection, partially trained policies must be deployed and these must avoid catastrophically unsafe regions, yet still enable trial and error learning.",
                "distance": 0.0208
            },
            {
                "reference": "There has been interest in studying bandits and reinforcement learning (RL) implementations where the agent is required to perform at least as well as a given baseline policy. This is called a conservative constraint and is especially relevant in real-world applications across various domains.",
                "distance": 0.0247
            },
            {
                "reference": "Recent research has confirmed the feasibility of backdoor attacks in deep reinforcement learning (RL) systems. However, existing attacks require the ability to arbitrarily modify an agent's observation, constraining the application scope to simpler RL systems, such as Atari games.",
                "distance": 0.0268
            },
            {
                "reference": "In safe reinforcement learning (SRL), an agent explores an environment to maximize expected total reward while avoiding violating certain constraints on expected total costs. Existing SRL problems possess nonconvex objective functions with multiple nonconvex constraints, which can be challenging to solve, especially when guaranteed globally optimal policies are desired.",
                "distance": 0.0417
            },
            {
                "reference": "Ensuring that safety constraints are satisfied with probability one is a critical issue in real-world applications of reinforcement learning(RL), yet current RL algorithms may not always meet this requirement.",
                "distance": 0.0422
            },
            {
                "reference": "Safe exploration in model-free reinforcement learning (RL) is a significant challenge, particularly when the safety cost is sparse and unknown, culminating in constraint violations in safety-critical applications.",
                "distance": 0.06
            },
            {
                "reference": "Safe exploration is a key priority in reinforcement learning research. The expected long-term costs of policies are typically constrained, but setting constraints on the expected safety signal without considering the tail of the distribution can be hazardous, especially in safety-critical domains.",
                "distance": 0.069
            },
            {
                "reference": "For many applications of reinforcement learning that physically interact with or around humans, satisfying safety constraints is necessary. Current policy search algorithms have enabled new capabilities in high-dimensional control, but do not consider constraints.",
                "distance": 0.0705
            },
            {
                "reference": "Auto-bidding techniques have become an indispensable method for revenue generation in advertising. Reinforcement learning (RL) algorithms are used to formulate real-time bids for advertisers in complex and ever-changing bidding environments. However, due to safety reasons, the RL training process is often performed in an offline virtual advertising system (VAS), which leads to inconsistency between online and offline environments.",
                "distance": 0.0816
            },
            {
                "reference": "Safe reinforcement learning (RL) aims to deploy policies in safety-critical applications satisfying certain constraints, however, previous primal-dual style approaches have stability issues and lack optimality guarantees.",
                "distance": 0.0824
            },
            {
                "reference": "Training-time safety violations have been a major concern when deploying reinforcement learning algorithms in real world applications, especially in challenging settings where only a safe but trivial-reward initial policy is provided, without any prior knowledge of the dynamics and additional offline data.",
                "distance": 0.09
            },
            {
                "reference": "Facing the outbreak of COVID-19, there is an increasing shortage of medical resources, highlighting the urgent need for efficient medical resource allocation strategies. However, the application of Reinforcement Learning (RL) in this problem faces challenges: complex real-world decision-making scenarios with countless choices, imperfect information availability due to pandemic spread latency and limitations on conducting experiments in the real world.",
                "distance": 0.0944
            },
            {
                "reference": "Safe reinforcement learning algorithms, which aim to learn the optimal policy while satisfying safety constraints, are essential in real-world applications. However, they struggle for efficient policy updates with hard constraint satisfaction.",
                "distance": 0.0949
            },
            {
                "reference": "In Constrained Reinforcement Learning (CRL), safety constraints are critical for real-world problems. However, existing methods constraining discounted cumulative costs lack rigorous definition and guarantee of safety. Some recent studies incorporate feasible sets into CRL with methods like Control Barrier Function (CBF), Safety Index (SI), but use prior conservative estimations of feasible sets which harm the performance of the learned policy.",
                "distance": 0.0971
            },
            {
                "reference": "Adaptive instruction for online education can be beneficial by increasing learning gains and decreasing work for learners, instructors, and course designers. Reinforcement Learning (RL) has been proposed as a tool for developing instructional policies, as RL models can learn complex relationships between course activities, learner actions, and educational outcomes.",
                "distance": 0.0977
            }
        ]
    },
    {
        "target": "Deep Neural Networks and Reinforcement Learning methods are being used to tackle challenging combinatorial problems. In these methods, a deep neural network is used as a solution generator that is trained by gradient-based methods to obtain better solution distributions.",
        "prediction": "Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods exhibit high variance in performance from one run to another using different initializations/seeds, particularly in problems arising in continuous control.",
        "queries": [
            9.263689,
            -22.158634
        ],
        "log": [
            {
                "reference": "Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods exhibit high variance in performance from one run to another using different initializations/seeds, particularly in problems arising in continuous control.",
                "distance": 0.0133
            },
            {
                "reference": "Policy improvement in reinforcement learning (RL) typically oscillates between the greedy approach seen in value-based RL and the full planning approach seen in model-based RL.",
                "distance": 0.0211
            },
            {
                "reference": "Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. They usually distinguish between epistemic and aleatoric uncertainty while learning the model but ignore this differentiation when optimizing the policy, which tends to result in greedy algorithms that do not explore sufficiently.",
                "distance": 0.0376
            },
            {
                "reference": "Reinforcement learning algorithms typically optimize the discounted criterion to accelerate convergence and reduce the variance of estimates. However, many engineering problems treat future rewards equally and prefer a long-run average criterion over the discounted one.",
                "distance": 0.0421
            },
            {
                "reference": "Many recent successful (deep) reinforcement learning algorithms make use of regularization, such as entropy or Kullback-Leibler divergence. These regularization methods often encompass policy iteration and value iteration strategies.",
                "distance": 0.0519
            },
            {
                "reference": "Partially-observable reinforcement learning (PORL) and non-parametric Bayesian statistics (NPB) are two separate fields, and there is a need to address issues of statistical modeling and decision-making in complex, real-world domains.",
                "distance": 0.0565
            },
            {
                "reference": "Most deep reinforcement learning (RL) algorithms distill experience into parametric behavior policies or value functions via gradient updates, which can be computationally expensive, slow to integrate experiences, prone to ignoring not fully integrated experiences, and limited by the model's capacity.",
                "distance": 0.0729
            },
            {
                "reference": "The existing modern deep reinforcement learning (RL) algorithms are based on either the general policy improvement (GPI) or trust-region learning (TRL) frameworks, which prove unscalable. The scalable algorithms violate GPI/TRL assumptions due to the need for regularisation or other heuristics, and their empirical success currently explained as approximate adaptations of theoretically sound methods. However, studies have shown that these algorithms greatly differ from their conceptual ancestors in practice.",
                "distance": 0.0822
            },
            {
                "reference": "Neural combinatorial optimization using reinforcement learning (RL) offers a strong heuristic solver for NP-hard problems but can suffer from issues, such as slow, unstable training and susceptibility to local minima due to high variance. Existing RL solvers are often taught by 'expert guides' with extensive domain knowledge.",
                "distance": 0.0825
            },
            {
                "reference": "Reinforcement learning techniques, specifically deep policy optimization methods, have been successful in complex tasks but have limitations. Their real-world adoption remains limited due to the significant amount of data required for success, with small sample sizes resulting in unstable learning due to their reliance on high-dimensional sample-based estimates.",
                "distance": 0.0889
            },
            {
                "reference": "Standard deep reinforcement learning algorithms use a shared representation for the policy and value function, especially when training directly from images. This shared representation can lead to over-fitting, as more information is needed to accurately estimate the value function than to learn the optimal policy.",
                "distance": 0.0918
            },
            {
                "reference": "Model-free deep reinforcement learning methods have been successful in various domains but they suffer from high sample complexity. Existing methods alternate between stable learning with high variance in batch policy gradient methods and sample-efficient yet biased TD-style methods like off-policy actor-critic and Q-learning.",
                "distance": 0.0936
            },
            {
                "reference": "Model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action Deep Reinforcement Learning (DRL) benchmarks due to a high Update-To-Data (UTD) ratio.",
                "distance": 0.0954
            }
        ]
    },
    {
        "target": "Federated learning is a secure approach for safeguarding local private data in collaborative learning. However, the advent of gradient inversion research has posed significant challenges to this premise by enabling a third-party to recover groundtruth images via gradients, which were mostly focused on low-resolution images and small batch sizes.",
        "prediction": "In federated optimization, heterogeneity in the clients' local datasets and computation speeds result in variations in the local updates performed by each client in each communication round. This leads to objective inconsistency, where the global model converges to a stationary point of a function that differs from the true objective.",
        "queries": [
            3.742744,
            25.66835
        ],
        "log": [
            {
                "reference": "In federated optimization, heterogeneity in the clients' local datasets and computation speeds result in variations in the local updates performed by each client in each communication round. This leads to objective inconsistency, where the global model converges to a stationary point of a function that differs from the true objective.",
                "distance": 0.0011
            },
            {
                "reference": "In federated learning, aggregated model updates may be exploited by an untrusted central server to disaggregate user updates from sums of update across participants, thus compromising privacy.",
                "distance": 0.0016
            },
            {
                "reference": "The existing federated learning techniques usually require readily available labels and do not support desirable properties like robustness to statistical/systems heterogeneity, scalability with the number of participants and communication efficiency. Also, previous work has focused on extending centralized self-supervised learning techniques, which are not designed with the mentioned properties.",
                "distance": 0.0022
            },
            {
                "reference": "Data heterogeneity is a key feature in federated learning, but often overlooked in the lens of robustness to adversarial attacks, particularly backdooring attacks.",
                "distance": 0.0026
            },
            {
                "reference": "Federated learning is being adopted due to its property of privacy preservation, but its implementation encounters the challenge of Non-Independent and Identically Distributed (Non-IID) data across devices.",
                "distance": 0.005
            },
            {
                "reference": "The Federated Averaging (FedAvg) algorithm is widely used in Federated Learning. Despite its simplicity, it performs surprisingly well, particularly after a few fine-tuning steps, but this performance is not fully understood from a theoretical standpoint.",
                "distance": 0.0058
            },
            {
                "reference": "In federated learning, learning a model from decentralized and heterogeneous local data is a challenge, especially in unsupervised learning scenarios where data features are not shared.",
                "distance": 0.0059
            },
            {
                "reference": "Federated learning, despite its potential, is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods have been proposed to reduce this potential backdoor risk, but these methods have been mainly validated in the computer vision (CV) field.",
                "distance": 0.0067
            },
            {
                "reference": "The COVID-19 outbreak highlighted the need for automatic diagnosis using deep learning models. However, these models face the challenge of accessing ample data due to patient privacy concerns. Current Federated Learning (FL) techniques, which allow organizations to cooperatively learn an effective deep learning model without sharing raw data, still lack privacy protection and risk data leakage.",
                "distance": 0.0074
            },
            {
                "reference": "Federated learning has been applied to train machine learning models from decentralized client data on mobile devices. However, the training data distributions periodically shift throughout the day, leading to instability in training and a degradation in model performance.",
                "distance": 0.0079
            },
            {
                "reference": "Federated learning, a method where a neural network is collaboratively trained on a server while users' input data remains on device and only parameter updates (gradients) are shared, is designed to train data efficiently and provide privacy benefits.",
                "distance": 0.0093
            },
            {
                "reference": "Deep neural networks have the ability to extract useful shared feature representations from data for learning tasks. However, this potential has not been fully utilized in federated settings where data is often non-identically and independently distributed (non-i.i.d) across clients and the statistical heterogeneity across clients or tasks is majorly in the labels.",
                "distance": 0.0096
            },
            {
                "reference": "Federated learning systems for streaming data are vulnerable to malicious (Byzantine) attacks that can undermine the accuracy of the learning process.",
                "distance": 0.0097
            },
            {
                "reference": "In federated learning, in each communication round, a subset of clients communicate their model updates back to the server for aggregation. Currently, the optimal size of this subset is unknown, and random selection of this subset has proven inefficient regarding convergence, learning efficiency, and fairness.",
                "distance": 0.01
            },
            {
                "reference": "Current federated learning algorithms mostly operate under the assumption that all participating entities use identical model architectures.",
                "distance": 0.0105
            },
            {
                "reference": "Conventional federated learning involves training customized models for each edge device which is hindered by the inherent data biases of the devices and the requirements imposed by the federated architecture.",
                "distance": 0.0105
            },
            {
                "reference": "Federated learning facilitates collaborative training of machine learning models among multiple clients, while preserving their data privacy. In practice, federated learning faces challenges in achieving fairness among participating clients due to variations in clients' contributions. Also, as machine learning models are increasingly used in important applications, ensuring that a trained model is unbiased against sensitive attributes has become a critical goal.",
                "distance": 0.0108
            },
            {
                "reference": "A critical challenge of federated learning is data heterogeneity and imbalance across clients, which leads to inconsistency between local networks and unstable convergence of global models.",
                "distance": 0.0109
            },
            {
                "reference": "The existing Top-k sparsification method in Federated Learning (FL) uses a lot of the communication cost on the index of the selected parameters which is inefficient for FL training.",
                "distance": 0.0114
            },
            {
                "reference": "Personalized federated learning has been proposed to handle the data heterogeneity problem amongst clients by learning dedicated tailored local models for each user. Existing works are often built in a centralized way, resulting in high communication pressure and vulnerability when a failure or an attack on the central server occurs.",
                "distance": 0.012
            }
        ]
    },
    {
        "target": "Growing leakage and misuse of visual information promote the development of information protection. Existing adversarial perturbations are primarily focused on de-identification against deep learning models, but they don't effectively protect inherent visual information.",
        "prediction": "The vulnerability of deep neural networks to adversarial attacks, whereby slight perturbations on the inputs can lead to incorrect predictions, has been widely reported. This susceptibility is often increased due to the adversarial transferability property, where adversarial examples can fool other models.",
        "queries": [
            -2.463925,
            -21.551981
        ],
        "log": [
            {
                "reference": "The vulnerability of deep neural networks to adversarial attacks, whereby slight perturbations on the inputs can lead to incorrect predictions, has been widely reported. This susceptibility is often increased due to the adversarial transferability property, where adversarial examples can fool other models.",
                "distance": 0.0085
            },
            {
                "reference": "With recent advancements in deep learning, deep clustering models have emerged as state-of-the-art approaches for high-dimensional image datasets. However, unlike traditional clustering approaches, the robustness of deep clustering models to adversarial attacks has not been systematically investigated.",
                "distance": 0.0173
            },
            {
                "reference": "The vulnerability of deep networks to adversarial attacks is a critical issue within deep learning. Current defense methods either train a classifier using adversarial images created during the learning process, or transform or purify the original input to remove adversarial signals before the image is classified.",
                "distance": 0.0201
            },
            {
                "reference": "Deep learning models are vulnerable to adversarial attacks, with existing attacks mostly conducted at a low-level, such as pixels and super-pixels, and rarely leveraging semantic information. In the case of face recognition attacks, traditional methods generating l_p-norm perturbations on pixels suffer from subpar attack transferability and increased vulnerability to denoising defense models.",
                "distance": 0.0213
            },
            {
                "reference": "Recent work has shown that neural networks are vulnerable to adversarial examples, namely, inputs that are almost indistinguishable from natural data but are classified incorrectly by the network.",
                "distance": 0.0225
            },
            {
                "reference": "The study of adversarial examples and their activations in robust deep neural networks (DNNs) learning has attracted significant attention. Existing works have addressed the issue of high activation magnitude of adversarial examples via adversarial training, but the issue of uniform activation of channels by adversarial examples remains.",
                "distance": 0.0292
            },
            {
                "reference": "Deep learning has seen significant advancement in recent years, particularly in perceptual tasks. Yet, it remains vulnerable to adversarial perturbations - minor adjustments to the input designed to deceive the system that are almost imperceptible to humans.",
                "distance": 0.0379
            },
            {
                "reference": "Deep neural networks are vulnerable to adversarial attacks. There has been increasing efforts to defend such attacks using deep generative models, which predict by inverting the deep generative models; these model-based defenses are difficult to attack due to obfuscated gradients caused by inversion.",
                "distance": 0.0415
            },
            {
                "reference": "Deep neural networks are vulnerable to adversarial attacks, where minor perturbations to the input result in altered model predictions. Particularly, adversarial inputs created for one model can often deceive another.",
                "distance": 0.0456
            },
            {
                "reference": "Existing adversarial strategies against deep neural networks (DNN) majorly involve introducing minor perturbations to a digital input or a stationary physical object.",
                "distance": 0.0477
            },
            {
                "reference": "Existing adversarial attacks on deep neural networks typically search for vulnerabilities in the input space, often employing gradient descent. However, due to the non-convexity of these networks, some adversarial examples may be overlooked.",
                "distance": 0.0479
            },
            {
                "reference": "Almost all current adversarial attacks of CNN classifiers rely on information derived from the output layer of the network, which may limit their effectiveness.",
                "distance": 0.0492
            },
            {
                "reference": "Recent research in adversarial perturbations has focused on defeating Deep Neural Networks (DNNs) in machine vision; most of these perturbation-based attacks target object classifiers.",
                "distance": 0.0521
            },
            {
                "reference": "Deep neural networks are vulnerable to adversarial examples, which can lead them to deliver incorrect results. Adversarial patches are a type of adversarial attack that limits noise to small, localized patches, making them potential threats in real-world scenarios. However, existing attack strategies often neglect the network's perceptual sensitivity to the adversarial patch, including its correlations with the image context and visual attention, resulting in patches that lack visual fidelity and strong attack capabilities.",
                "distance": 0.0618
            },
            {
                "reference": "Deep neural networks are known to be vulnerable to adversarial attacks. Numerous attempts have been made to fully understand the existence of adversarial samples and develop effective defense strategies, but the problem remains largely unsolved.",
                "distance": 0.0619
            },
            {
                "reference": "Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems.",
                "distance": 0.0629
            },
            {
                "reference": "Deep learning has been successfully applied in numerous situations, however, it can be vulnerable to malicious inputs with imperceptible perturbations in real applications.",
                "distance": 0.0643
            },
            {
                "reference": "Despite the remarkable success of deep learning and Deep Neural Networks (DNNs) in various machine learning domains, it has been found that DNNs are surprisingly vulnerable to malicious attacks. Adversarial training is an effective defense strategy to train a robust classifier.",
                "distance": 0.0645
            },
            {
                "reference": "Adversarial examples, natural images with visually imperceptible perturbations, have been shown to cause deep networks to fail on image classification. However, extending adversarial examples to semantic segmentation and object detection is much more challenging.",
                "distance": 0.0727
            },
            {
                "reference": "Neural networks are vulnerable to adversarial examples, malicious inputs created to fool models. Adversarial examples often exhibit black-box transfer, meaning that examples created for one model can fool another. However, these examples typically overfit to the architecture and feature representation of one model, leading to less effective attacks on other models.",
                "distance": 0.0737
            }
        ]
    },
    {
        "target": "Large language models (LLMs) are increasingly used as goal-driven agents interacting with external environments, but learning from trial-and-error remains challenging using traditional reinforcement learning methods, as they require extensive training samples and expensive model fine-tuning.",
        "prediction": "Fairness in pre-trained language models and the techniques used to fine-tune them for downstream tasks is a significant concern, especially in legal text processing across different languages and legal jurisdictions.",
        "queries": [
            -2.659332,
            14.592837
        ],
        "log": [
            {
                "reference": "Fairness in pre-trained language models and the techniques used to fine-tune them for downstream tasks is a significant concern, especially in legal text processing across different languages and legal jurisdictions.",
                "distance": 0.0132
            },
            {
                "reference": "Recent work has trained large language models (LLMs) to follow natural language instructions, creating opportunities for natural language interface design. There has been prior success of LLMs in computer-assisted creativity.",
                "distance": 0.0144
            },
            {
                "reference": "Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks.",
                "distance": 0.0272
            },
            {
                "reference": "Large language models are trained on a variety of NLP tasks and converted into a text-to-text format using prompts. Existing research has focused on understanding the effects of input forms and prompts on performance, without addressing whether the input format affects the social biases promoted in outputs.",
                "distance": 0.0376
            },
            {
                "reference": "Large language models (LLMs) have shown human-level performance on many natural language tasks, yet it is unexplored how they can better assimilate knowledge from structured data, such as a knowledge graph, versus from text.",
                "distance": 0.0621
            },
            {
                "reference": "Real-world applications often involve complex tasks that cannot be easily handled via a single run of a large language model (LLM). Recent work has shown that chaining multiple LLM runs together can accomplish these tasks with increased transparency and control, yet it is unknown what users need when authoring their own LLM chains.",
                "distance": 0.0625
            },
            {
                "reference": "Previous work in using language models (LLMs) to act in interactive environments has predominantly focused on learning from explicit step-by-step examples of how to act.",
                "distance": 0.081
            },
            {
                "reference": "Language models (LMs) have achieved significant performance on many Natural Language Understanding (NLU) tasks, but there are ongoing debates about their reasoning capability in NLU.",
                "distance": 0.0851
            },
            {
                "reference": "Investigations into how models of event implications predict entity state-changes have shown that Large Language Models (LLMs), despite being exposed to procedural knowledge about object interactions, fail to reason about the world. Moreover, current benchmarking methods often misrepresent the abilities of LLMs due to improper task encodings.",
                "distance": 0.0878
            }
        ]
    },
    {
        "target": "Backdoor attacks have emerged as a significant problem for NLP systems, where poisoned training data is used to embed a 'backdoor' into the model, causing certain textual patterns to be predicted as a target label of an adversary's choice.",
        "prediction": "Previous off-path TCP-injection attacks assume vulnerabilities such as client-malware or predictable choice of client port or IP-ID.",
        "queries": [
            -3.780404,
            -21.337227
        ],
        "log": [
            {
                "reference": "Previous off-path TCP-injection attacks assume vulnerabilities such as client-malware or predictable choice of client port or IP-ID.",
                "distance": 0.0342
            },
            {
                "reference": "Pre-Trained Models (PTMs) have been widely used, but they are vulnerable to backdoor attacks. PTMs can have weights maliciously poisoned with certain triggers, resulting in inaccurate predefined predictions and security challenges. Existing poisoning methods can have their backdoors erased by changing hyper-parameters during fine-tuning or detected by finding the triggers.",
                "distance": 0.0384
            },
            {
                "reference": "Previous understanding of backdoor data poisoning attacks, where attackers inject mislabeled training examples into a training set, are limited and lack a formal theoretical framework, making predictions and analyses challenging.",
                "distance": 0.049
            },
            {
                "reference": "In the context of poisoned models exploited by backdoor attacks, previous backdoor removal methods often decomposed the task into separate inner and outer problems, but this approach does not account for their interdependence.",
                "distance": 0.0499
            },
            {
                "reference": "In outsourced machine learning training, $backdoor$ $attacks$ are practical where the third party training the model may inject malicious behaviors into an otherwise accurate model. Until now, the method to inject backdoors has been limited to $poisoning$.",
                "distance": 0.0501
            },
            {
                "reference": "Current backdoor attacks on NLP models often involve injecting triggers into selected sentences and changing the original label to a target label. This strategy has a flaw of being easily detected by defense models or manual inspections because of the abnormal language expression and mistaken labels.",
                "distance": 0.0595
            },
            {
                "reference": "A new form of data poisoning known as backdoor attacks has been uncovered recently. These attacks do not affect a network's behavior on typical, benign data but cause deviation only when triggered by an adversary's planted perturbation.",
                "distance": 0.0612
            },
            {
                "reference": "Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process.",
                "distance": 0.065
            },
            {
                "reference": "Membership inference attacks threaten the privacy of machine learning models trained on sensitive private data by offering adversaries the ability to infer whether a particular point was used in model training. Current attacks require access to the model's predicted confidence score.",
                "distance": 0.0657
            },
            {
                "reference": "Membership inference attacks (MIA) aim to detect if data samples were used to train a neural network model, often to detect copyright abuses.",
                "distance": 0.0662
            },
            {
                "reference": "Data poisoning attacks have raised concerns as they can lead to neural backdoors that misclassify certain inputs crafted by an attacker and the sample-targeted backdoor attack is a new challenge in these. The existing backdoor detection schemes fail to detect the sample-targeted backdoor as they depend on reverse-engineering the trigger.",
                "distance": 0.0727
            },
            {
                "reference": "Backdoor attacks manipulate the output of deep neural networks and pose high insidiousness in the field of natural language processing. These attacks are highly successful on multiple popular models, but there is a lack of studies conducted on defending against textual backdoor attacks.",
                "distance": 0.084
            },
            {
                "reference": "Membership inference attacks (MIAs) are a long-term threat to the privacy of training data in machine learning models. The vulnerability of a model to MIAs is strongly connected to the distinguishability of the training and testing loss distributions.",
                "distance": 0.0871
            },
            {
                "reference": "Poisoning-based backdoor attacks present a serious concern for the training of deep models on untrusted data sources.",
                "distance": 0.0875
            },
            {
                "reference": "Previous works have attempted to evaluate privacy loss through poisoning attacks or membership inference for differentially private machine learning systems. However, these methods have been tailored to specific models or have demonstrated relatively low statistical power.",
                "distance": 0.0881
            },
            {
                "reference": "Model inversion attacks (MIAs) aim to create synthetic images that reflect the class-wise characteristics from a target classifier\u2019s private training data by exploiting the model\u2019s learned knowledge. Previous research has developed generative MIAs that use generative adversarial networks (GANs) as image priors tailored to a specific target model. This makes the attacks time- and resource-consuming, inflexible, and susceptible to distributional shifts between datasets.",
                "distance": 0.0962
            }
        ]
    },
    {
        "target": "Modern autonomous driving systems use a modular task sequence: perception, prediction, and planning. For diverse tasks and high-level intelligence, current approaches either use standalone models for individual tasks or design a multi-task paradigm with separate heads. However, these systems may suffer from cumulative errors or insufficient task coordination.",
        "prediction": "Learning driving policies from pre-recorded logs is a complex task, often requiring an innate understanding of the world's dynamics and the ego-vehicle's responses to them.",
        "queries": [
            10.563414,
            2.533982
        ],
        "log": [
            {
                "reference": "Learning driving policies from pre-recorded logs is a complex task, often requiring an innate understanding of the world's dynamics and the ego-vehicle's responses to them.",
                "distance": 0.0482
            },
            {
                "reference": "Vision-based autonomous urban driving in dense traffic is a challenge due to the complicated urban environment and the dynamics of driving behaviors. The widely applied methods either heavily rely on hand-crafted rules or learn from limited human experience, which makes it hard for them to generalize to rare but critical scenarios.",
                "distance": 0.0503
            },
            {
                "reference": "Understanding inter-agent relationships and motion behaviors is crucial for high-quality planning when navigating in complex scenarios, particularly at urban traffic intersections. However, managing the problem of discontinuous dependency in the spatial-temporal space is challenging.",
                "distance": 0.0534
            },
            {
                "reference": "Autonomous systems such as vehicles and robotics require the ability to anticipate and understand future scenarios for effective decision making. However, separately predicting scene parsing and motion dynamics has proven inadequate.",
                "distance": 0.0536
            },
            {
                "reference": "Previous attempts at autonomous driving rely on certain specific road and environmental conditions. The task of road following was not adaptive to various circumstances.",
                "distance": 0.0552
            },
            {
                "reference": "Interpreting complex interactions in urban scenarios and forecasting future positions is crucial for autonomous systems such as self-driving cars or socially compliant robots. Traditional models that do not take into account social interactions or contextual information have proven insufficient.",
                "distance": 0.0582
            },
            {
                "reference": "End-to-end approaches to autonomous driving commonly rely on expert human demonstrations. However, humans can be suboptimal coaches for end-to-end algorithms which require dense on-policy supervision. Automated experts can generate large scale on-policy and off-policy demonstrations, but they often rely heavily on hand-crafted rules and perform suboptimally even on simulators.",
                "distance": 0.0683
            },
            {
                "reference": "In autonomous driving (AD), accurately predicting changes in the environment is very challenging due to the complex interactions among traffic participants, making it hard to achieve accurate prediction for a long horizon.",
                "distance": 0.0745
            },
            {
                "reference": "Existing techniques for learning to drive using AI assume direct access to an instrumented ego-vehicle with fully known observations and expert driver actions. However, such measurements cannot be directly accessed for training AI on non-ego vehicles when learning by watching others. Therefore, a significant amount of potential training data is being discarded in current approaches.",
                "distance": 0.0782
            },
            {
                "reference": "Recent advances in intelligent vehicles have relied on quantitative approaches for autonomous driving, limiting their ability to handle complex situations like city traffic.",
                "distance": 0.0797
            },
            {
                "reference": "For planning safe maneuvers, an autonomous vehicle must accurately perceive its environment, and understand complex urban traffic interactions among participants, an area that requires improvement.",
                "distance": 0.0797
            },
            {
                "reference": "Existing frameworks for predicting future trajectories of navigating participants in road scenes may not adequately consider the relational behavior between road users and their surrounding environment, limiting the effectiveness of the predictions.",
                "distance": 0.0959
            },
            {
                "reference": "Predicting future movement of surrounding traffic actors is crucial for autonomous vehicles. Top-down scene rasterization methods and Generative Adversarial Networks (GANs) have been particularly successful in this regard, achieving state-of-the-art accuracies on traffic movement prediction.",
                "distance": 0.0984
            }
        ]
    },
    {
        "target": "Pre-trained language models (PLMs) are achieving the best performance on a range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to tackle specific domains more effectively.",
        "prediction": "Large-scale Protein Language Models (PLMs) have been successful in various protein prediction tasks and Evoformer, a PLM used in AlphaFold, has remained largely unexplored beyond structure prediction.",
        "queries": [
            -2.890727,
            15.769768
        ],
        "log": [
            {
                "reference": "Large-scale Protein Language Models (PLMs) have been successful in various protein prediction tasks and Evoformer, a PLM used in AlphaFold, has remained largely unexplored beyond structure prediction.",
                "distance": 0.0047
            },
            {
                "reference": "Pre-trained language models (PLMs) show notable prediction performance in various natural language processing (NLP) tasks. However, they often lack reliable indication of the trustworthiness of their predictions, especially in safety-critical applications. Prior studies have only partially addressed this issue without providing a comprehensive analysis on how to optimize a well-calibrated PLM-based prediction pipeline.",
                "distance": 0.0088
            },
            {
                "reference": "Pre-trained language models (PLMs) achieve remarkable performance on many downstream tasks, but may fail in giving reliable estimates of their predictive uncertainty. There's a lack of comprehensive understanding of PLMs' calibration, a topic that has not been thoroughly explored.",
                "distance": 0.0126
            },
            {
                "reference": "Pretrained language models (PLMs) have significantly improved text generation, but can produce unfaithful or inappropriate content. On the other hand, template-based systems offer faithfulness, but lack fluency.",
                "distance": 0.0128
            },
            {
                "reference": "Pretrained language models (PLMs) typically use a fixed number of steps for inference, leading to inefficiencies in certain situations and potential overthinking which can hinder performance.",
                "distance": 0.0151
            },
            {
                "reference": "Pre-trained language models (PLMs) have shown impressive performance by self-supervised training on text, yet they lack visual semantics or commonsense related to properties such as sizes, shapes, and colors of commonplace objects. Existing solutions usually depend on explicit images, which are time-consuming to retrieve or generate, and apply augmentation to the whole input text indiscriminately.",
                "distance": 0.0162
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have been widely used in passage re-ranking tasks due to their natural language understanding capabilities. However, these models often suffer from issues such as vocabulary mismatch and lack of domain-specific knowledge.",
                "distance": 0.0195
            },
            {
                "reference": "Although pre-trained language models (PLMs) are commonly used in NLP and provide generic initialization for finding high-performance minima, little is understood about the connections between the different minima reached under various adaptation configurations.",
                "distance": 0.0277
            },
            {
                "reference": "Pre-trained language models (PLMs) have been effectively used in news recommendation, but they often suffer from the domain shift problem between the pre-training corpus and downstream news texts. Additionally, PLMs are computationally heavy, impacting their usability in low-latency online services.",
                "distance": 0.0287
            },
            {
                "reference": "Recently, Pre-trained Language Models (PLMs) have been improved by adding knowledge facts to enhance language understanding, particularly in the medical domain where terms and relations can be complex.",
                "distance": 0.0292
            },
            {
                "reference": "Pretrained language models (PLMs) have shown success on several downstream tasks due to their ability to model language. While efforts have been made to explore the syntactic structures entailed by PLMs, grounding capabilities of PLMs have been less investigated.",
                "distance": 0.0373
            },
            {
                "reference": "Pre-trained language models (PLMs) improve the generalization of natural language understanding models, but the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods.",
                "distance": 0.0381
            },
            {
                "reference": "Pre-trained Language Models (PLMs) are widely available and used for multi-task fine-tuning across domains. However, they may memorize nontransferable knowledge when applied to tasks from distant domains with different class label sets, leading to negative transfer.",
                "distance": 0.0385
            },
            {
                "reference": "Pre-trained language models (PLMs) learn universal language representations through self-supervised training tasks on large-scale corpora, however the quality of word representations tends to depend on their frequency in the corpus. As a result, the embeddings of rare words are often poorly optimized.",
                "distance": 0.0389
            },
            {
                "reference": "Latency and efficiency issues are often overlooked in the evaluation of Information Retrieval (IR) models based on Pretrained Language Models (PLMs) across various hardware and software testing scenarios. The SPLADE model, which has achieved state-of-the-art zero-shot performance and competitive results on TREC collections, has efficiency controlled via a regularization factor which has proven not to be efficient enough.",
                "distance": 0.0392
            },
            {
                "reference": "The reuse of pre-trained language models (PLMs) has been investigated to reduce computational cost and environmental side-effects. However, the challenge lies in how to combine knowledge from different teacher-PLMs, each specializing in a different classification problem, without available human annotations.",
                "distance": 0.0398
            },
            {
                "reference": "Predominantly, pretrained language models (PLMs) are combined with textual patterns to enhance performance in both the zero and few-shot learning settings. The patterns used either directly resemble the text seen during the PLM's pretraining (for zero-shot tasks) or they are adapted more flexibly to the PLM's unique characteristics (for supervised training tasks).",
                "distance": 0.0415
            },
            {
                "reference": "Domain-specific Pre-trained Language Models (PLMs) have been proposed to enhance task performance within specific domains. However, this Domain Adaptive Pre-training (DAPT) tends to forget the previous general knowledge acquired by general PLMs, which leads to the phenomenon of catastrophic forgetting and sub-optimal performance.",
                "distance": 0.0483
            },
            {
                "reference": "Applying pre-trained language models (PLMs) for search ranking often requires more nuances and training signals. There exist discrepancies in the training objectives and model architectures, as well as the knowledge needed in ranking and that learned during pre-training.",
                "distance": 0.0523
            },
            {
                "reference": "Current large-scale Pre-trained Language Models (PLMs) have two challenges. They can be inefficient in terms of memory and computation, and they can struggle to generalize to out-of-distribution (OOD) data, often relying on data set bias.",
                "distance": 0.0525
            }
        ]
    },
    {
        "target": "Named Entity Recognition (NER) is a common task in natural language processing, but fine-grained multilingual NER, especially in noisy conditions, presents additional challenges.",
        "prediction": "Existing Named Entity (NE) recognition systems often rely on supervised methods which require large volumes of labeled data that is usually expensive and time-consuming to obtain.",
        "queries": [
            -21.597485,
            -4.121071
        ],
        "log": [
            {
                "reference": "Existing Named Entity (NE) recognition systems often rely on supervised methods which require large volumes of labeled data that is usually expensive and time-consuming to obtain.",
                "distance": 0.0035
            },
            {
                "reference": "Named entity recognition (NER) is a critical task that is responsible for identifying and classifying proper names in text as different types of named entities such as people, locations, and organizations. However, specific solutions for certain languages, particularly Arabic, are not prominently mentioned.",
                "distance": 0.004
            },
            {
                "reference": "Named Entity Understanding (NEU) is crucial to the accurate responses of voice assistants to the user's commands. However, it is a challenging task due to the ambiguity of natural language and noise introduced by speech transcription and user errors.",
                "distance": 0.0119
            },
            {
                "reference": "Named Entity Recognition (NER) typically requires training data in the target language and linguistic expertise.",
                "distance": 0.0141
            },
            {
                "reference": "Most existing Nested Named Entity Recognition (N-NER) models focus on English and lack fine-grained classes or large datasets in non-English languages, creating a gap in multilingual Named Entity Recognition.",
                "distance": 0.0143
            },
            {
                "reference": "Named Entity Recognition (NER) is a crucial task in natural language processing, but the creation of high-quality annotated corpora for NER is challenging and resource-intensive.",
                "distance": 0.0166
            },
            {
                "reference": "The task of Named Entity Recognition (NER) is to identify predefined units of information such as person names, organizations and locations in the text. It suffers from challenging problems requiring solutions tailored for specific languages.",
                "distance": 0.0166
            },
            {
                "reference": "Named Entity Recognition, a fundamental task in Natural Language Processing, primarily targets flat entities, neglecting the possibility of nested entities like [Bank of [China]].",
                "distance": 0.0171
            },
            {
                "reference": "Named Entity Recognition (NER) is a key technique in natural language processing and related fields, but it's particularly difficult to implement for Chinese due to a lack of capitalization information and uncertainty in word segmentation.",
                "distance": 0.0173
            },
            {
                "reference": "Pharmaceutical companies often apply biomedical Named Entity Recognition (NER) and linking techniques over their internal and public corpora for assisting the drug discovery/development process. Even though there are numerous algorithms, systems, and datasets from decades of study in the BioNLP field, there isn't a single open-source system that caters to all needs of a modern pharmaceutical company.",
                "distance": 0.0179
            },
            {
                "reference": "In biomedical literature named entity recognition (NER), combined classifier approaches have shown performance improvements as compared to single classifiers. However, selecting appropriate classifiers for an ensemble from a large pool of classifiers remains a critical design issue.",
                "distance": 0.021
            },
            {
                "reference": "Existing name entity recognition (NER) systems require labeled text to accurately identify and categorize named entities, and improving the accuracy of these systems is a key challenge.",
                "distance": 0.0219
            },
            {
                "reference": "Named Entity Recognition (NER) has been traditionally treated as a sequence labeling task, but there has been a recent shift towards formulating NER as a machine reading comprehension task, where entity names or other information are considered as questions, text as the context, and the entity value in text as an answer snippet. However, these approaches consider machine reading comprehension based on a single question or entity at a time.",
                "distance": 0.0242
            },
            {
                "reference": "The Named Entity Recognition (NER) task is crucial as it improves the performance of many natural language processing applications.",
                "distance": 0.0251
            },
            {
                "reference": "Named Entity Recognition (NER) has been traditionally performed without the use of document-specific knowledge base (KB) tags.",
                "distance": 0.0253
            },
            {
                "reference": "Existing named entity recognition (NER) methods for speech recognition results do not utilize confidence on automatic speech recognition (ASR) as a feature.",
                "distance": 0.0267
            },
            {
                "reference": "The importance of named entities (NEs) in the classification of annotated biomedical texts and the potency of integrating Roles into NEs as attributes in the field of public health protection has not been specifically researched.",
                "distance": 0.0271
            },
            {
                "reference": "Named Entity Recognition (NER) is a crucial task in natural language processing and previously machine learning systems employed for this task yield inferior performance.",
                "distance": 0.0271
            },
            {
                "reference": "Named Entity Recognition (NER) traditionally relies on complex linguistic resources and language-dependent tools.",
                "distance": 0.0275
            },
            {
                "reference": "A fundamental challenge in real-world named entity recognition (NER) scenarios is the presence of a large amount of noise from sources like pseudo, weak, or distant annotations, despite recent advancements in deep learning-based NER.",
                "distance": 0.028
            }
        ]
    },
    {
        "target": "Iterative pruning is one of the most effective compression methods for pre-trained language models. However, finding the optimal pruning decision is challenging and generalization at high sparsity levels is poor.",
        "prediction": "Early works in network pruning achieved sparsity by removing less important connections in the network, a process known as skeletonization. While this process has been found to have only a marginal impact on network performance, it does not extend well to pruning levels beyond 95%.",
        "queries": [
            7.893813,
            4.551486
        ],
        "log": [
            {
                "reference": "Early works in network pruning achieved sparsity by removing less important connections in the network, a process known as skeletonization. While this process has been found to have only a marginal impact on network performance, it does not extend well to pruning levels beyond 95%.",
                "distance": 0.0094
            },
            {
                "reference": "Network pruning, an effective method for reducing the computation of neural networks while maintaining high performance, is suitable for resource-limited environments. In large networks, the roles of each channel often overlap with others, necessitating the observation of correlations between features for more effective pruning.",
                "distance": 0.0158
            },
            {
                "reference": "Network pruning is a method to reduce computational resource requirements with little impact on performance. The common belief is that pruning methods utilize training data to find efficient subnetworks and the pruned network's architecture is crucial for satisfactory outcomes.",
                "distance": 0.0201
            },
            {
                "reference": "Network pruning, a regularization method, speeds up training and improves the generalization of neural networks. However, directly using $L_0$ norm as a regularization term is challenging due to its non-differentiable nature.",
                "distance": 0.0202
            },
            {
                "reference": "Network pruning is a widely used technique to reduce computation cost and model size for deep neural networks. However, the typical three-stage pipeline, training, pruning and retraining (fine-tuning) significantly increases the overall training trails.",
                "distance": 0.0222
            },
            {
                "reference": "Network pruning reduces memory footprint and time complexity. However, conventional post-training pruning techniques focus on efficient inference and overlook heavy computation for training. Meanwhile, pre-training pruning at initialization could reduce training cost, but this often leads to noticeable performance degradation.",
                "distance": 0.0225
            },
            {
                "reference": "Recent model pruning methods remove redundant parameters to preserve model performance based on parameter sensitivity, which is a gradient-based measure reflecting the contribution of parameters.",
                "distance": 0.0226
            },
            {
                "reference": "Network pruning is a method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Techniques such as weight rewinding and learning rate rewinding have been shown to improve recovery of lost accuracy in retraining pruned networks, but it is unclear what results in this performance improvement.",
                "distance": 0.0226
            },
            {
                "reference": "Network pruning, which aims to reduce the computational cost of neural networks, typically begins with an over-parameterized model that is then simplified by removing less important units. This process usually starts with training a large, redundant network.",
                "distance": 0.0257
            },
            {
                "reference": "Network pruning is a common method used to lighten and accelerate neural network models. However, structured network pruning, which discards whole neurons or filters, leads to a loss in accuracy.",
                "distance": 0.0286
            },
            {
                "reference": "Network pruning, or removing unimportant weights from a trained network, can improve generalization, simplify networks, reduce hardware or storage requirements, and speed up further training. Existing methods for this often remove the wrong weights and don't permit the removal of many weights without increasing error on the training set.",
                "distance": 0.0303
            },
            {
                "reference": "Automatic design of computationally efficient neural networks usually utilize network pruning or leverage network architecture search methods.",
                "distance": 0.0316
            },
            {
                "reference": "Pruning deep neural networks based on removing parameters according to their magnitude has proved to be a robust method. However, there is a lack of theoretical understanding of why magnitude pruning works, especially pre-convergence, and its relation to other pruning methods such as gradient-based pruning.",
                "distance": 0.0322
            },
            {
                "reference": "Network pruning is a common compression technique that scales down overparameterized models with minimal loss of accuracy.",
                "distance": 0.0337
            },
            {
                "reference": "Current network pruning reduces computation costs of an over-parameterized network without performance damage by pre-defining the width and depth of the pruned networks and then transferring the parameters from the unpruned network to the pruned networks.",
                "distance": 0.0357
            },
            {
                "reference": "Model pruning is used to improve the computational efficiency of deep learning and is particularly useful for resource-limited scenarios. The prevalent assumption in model pruning is that smaller-norm parameters or features play less informative roles during inference.",
                "distance": 0.0363
            },
            {
                "reference": "Weight pruning is an effective method in reducing model size and computation cost, but conventional sparse matrix formats involve irregular index structures with large storage requirement and sequential reconstruction process. These inefficiencies restrict pruning to inference with a batch size of one.",
                "distance": 0.0382
            },
            {
                "reference": "Pruning is a technique used to reduce the inference cost of Transformer models. Previous works on pruning Transformers require retraining the models, which adds high training costs and complexity to model deployment, and hence poses a challenge for practical use.",
                "distance": 0.0434
            },
            {
                "reference": "Network pruning has been traditionally used to alleviate the over-parameterization problem of deep neural networks. However, its value has been recently debated, particularly from the viewpoint of neural architecture search (NAS).",
                "distance": 0.0458
            },
            {
                "reference": "Reducing model redundancy is a key task to deploy complex deep learning models on resource-limited or time-sensitive devices. Pruning methods often directly regularize or modify weight values, making the pruning procedure sensitive to the choice of hyperparameters and requiring prior knowledge.",
                "distance": 0.0465
            }
        ]
    },
    {
        "target": "There has been a surge in identifying suicidal risk and its severity in social media posts using computational intelligence techniques and natural language processing, but there is a lack of explainable research that can impact clinical psychology practice and personalized mental healthcare. Additionally, there is limited availability of datasets for social NLP research community.",
        "prediction": "Recent studies have shown that social media is increasingly used for expressing suicidal thoughts outside the traditional clinical setting. While automated systems can assess suicide risk using Natural Language Processing strategies, they can generate uncertain predictions with severe consequences.",
        "queries": [
            -22.341986,
            8.050074
        ],
        "log": [
            {
                "reference": "Recent studies have shown that social media is increasingly used for expressing suicidal thoughts outside the traditional clinical setting. While automated systems can assess suicide risk using Natural Language Processing strategies, they can generate uncertain predictions with severe consequences.",
                "distance": 0.0385
            },
            {
                "reference": "Estimating the degree of depression in individuals based on their social media activities is an underexplored research topic.",
                "distance": 0.0441
            },
            {
                "reference": "Detecting psychological stress in online posts is a vital but challenging task, with most existing models functioning as black-boxes, lacking transparency and explainability.",
                "distance": 0.0571
            },
            {
                "reference": "The CLPsych 2016 shared task is focused on developing automated systems to assist mental health professionals in triaging posts with ideations of depression or self-harm.",
                "distance": 0.0573
            },
            {
                "reference": "Early intervention using social media data to identify suicide risks has garnered attention. The use of a suicide dictionary created by mental health experts has proven effective in detecting suicidal ideation. However, validation of these dictionaries for languages other than English and Chinese, particularly for low-resource languages like Korean where a knowledge-based suicide dictionary has not been developed, has been lacking.",
                "distance": 0.0719
            },
            {
                "reference": "Current studies on suicide risk assessment on social media require large amounts of user-centric data and often analyze the user's historic timeline and their social network. There is a need for a less intrusive way of assessing such risk.",
                "distance": 0.0788
            },
            {
                "reference": "The utility of screening Twitter users for mental health conditions such as depression and PTSD is a field of interest, with existing work primarily utilizing lexical decision lists.",
                "distance": 0.0816
            },
            {
                "reference": "Psychological stress detection has been a focus in health research. Previous work relied on the stressor dictionary from the stressor-related categories in the LIWC, focusing on stress categories that frequently appear on social media.",
                "distance": 0.0922
            },
            {
                "reference": "The advent of social media has created potential for early detection of depression, but current methods struggle to effectively use both textual and visual information from posts. Additionally, there is difficulty in accurately extracting relevant indicator texts and images due to the variety of content types.",
                "distance": 0.0958
            },
            {
                "reference": "Social media platforms are used for studying psycho-linguistic behaviors to model expressions of suicidal intent in tweets. Most existing methods in suicidal ideation detection don't leverage contextual psychological cues.",
                "distance": 0.0969
            },
            {
                "reference": "Machine learning methods have been used to annotate mental health status in social media, as they can gather large training samples. However, concerns have been raised on the validity of these methods for use in clinical applications.",
                "distance": 0.0999
            }
        ]
    },
    {
        "target": "During language acquisition, children follow a typical sequence of learning stages, from categorizing phonemes, developing their lexicon to mastering increasingly complex syntactic structures. These computational principles leading to this learning trajectory remain largely unknown and it's unclear whether deep language models follow a similar pattern.",
        "prediction": "In the field of natural language processing, traditional systems have struggled to achieve high performance due to the lack of parallelism.",
        "queries": [
            0.207452,
            9.76895
        ],
        "log": [
            {
                "reference": "In the field of natural language processing, traditional systems have struggled to achieve high performance due to the lack of parallelism.",
                "distance": 0.0114
            },
            {
                "reference": "As children learn language, they initially misunderstand reversible passive sentences as if they were active sentences.",
                "distance": 0.0306
            },
            {
                "reference": "Several learnable classes of languages have been previously explored in current literature, however, a unified learning-theoretic analysis has not been conducted.",
                "distance": 0.034
            },
            {
                "reference": "Children learn various levels of linguistic structure concurrently, but most models of language acquisition address only a single level, suggesting a sequential learning process.",
                "distance": 0.0391
            },
            {
                "reference": "For infants, early word learning is a challenge. They can learn word and reference patterns either by observing co-occurrence across situations or inferring the intended referent based on the social context.",
                "distance": 0.0534
            },
            {
                "reference": "In the current literature, the persuasive effect of phonetics in language has received little attention, while lexical, syntactic, semantic and stylistic features have been extensively studied from a computational perspective.",
                "distance": 0.0567
            },
            {
                "reference": "The resource control hypothesis was introduced by the author earlier as a potentially useful framework for understanding certain phenomena in language. However, its application to English articles and their representation in Slavonic languages through word order has been unexplored.",
                "distance": 0.0597
            },
            {
                "reference": "Statistical approaches to language learning typically focus either on short-range syntactic dependencies or long-range semantic dependencies between words.",
                "distance": 0.0657
            },
            {
                "reference": "Current resources for Portuguese multiword expressions (MWE) are insufficient in capturing semantic, syntactic, and pragmatic properties of these expressions.",
                "distance": 0.0846
            },
            {
                "reference": "The processes through which readers evoke mental representations of phonological forms from print constitute a controversial issue in current psycholinguistics.",
                "distance": 0.0946
            }
        ]
    },
    {
        "target": "Argument pair extraction (APE) aims to extract interactive argument pairs from two passages within a discussion. It's challenging due to the effort required to effectively capture the complex context-aware interactive relations of arguments between the two passages.",
        "prediction": "Previous work on Argument pair extraction (APE) studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task which resulted in implicitly obtaining argument pairs. However, these methods were not explicitly modelling the argument-level interactions between argument pairs.",
        "queries": [
            -2.427063,
            4.482875
        ],
        "log": [
            {
                "reference": "Previous work on Argument pair extraction (APE) studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task which resulted in implicitly obtaining argument pairs. However, these methods were not explicitly modelling the argument-level interactions between argument pairs.",
                "distance": 0.0441
            },
            {
                "reference": "Existing studies on argument pair extraction (APE), which aims to automatically mine argument pairs from two interrelated argumentative documents, typically identify argument pairs by predicting sentence-level relations between two documents and neglect the holistic argument-level interactions.",
                "distance": 0.0732
            }
        ]
    },
    {
        "target": "For SemEval-2023 Task 5, the task is to classify spoiler types (passage, phrase, multi) of clickbait web articles. Current models and methods don't take into account the information-placement on webpages which is usually described within the webpages markup, and optimised for ads.",
        "prediction": "SemEval-2015 Task 7 consists of three subtasks that involve automatically identifying the time period when a piece of news was written and determining whether a specific phrase in a sentence is relevant or not for a given period of time.",
        "queries": [
            -23.377132,
            1.785645
        ],
        "log": [
            {
                "reference": "SemEval-2015 Task 7 consists of three subtasks that involve automatically identifying the time period when a piece of news was written and determining whether a specific phrase in a sentence is relevant or not for a given period of time.",
                "distance": 0.3514
            },
            {
                "reference": "The Hyperpartisan News Detection task has attracted interest, and previous studies, such as Potthast et al., 2018, have used text classification methods.",
                "distance": 0.3656
            }
        ]
    },
    {
        "target": "Pre-trained encoder-only and decoder-only language models are commonly used in text classification tasks such as detecting online sexism. However, there's a need for continued analysis and improvement of these models, specifically in their interpretability.",
        "prediction": "Pre-training of masked language models (MLMs) consumes a massive amount of computation and results in a large carbon footprint. The existing MLMs use [MASK]s as placeholders and gather the contextualized information from unmasked tokens to restore the corrupted information.",
        "queries": [
            -1.290597,
            15.377796
        ],
        "log": [
            {
                "reference": "Pre-training of masked language models (MLMs) consumes a massive amount of computation and results in a large carbon footprint. The existing MLMs use [MASK]s as placeholders and gather the contextualized information from unmasked tokens to restore the corrupted information.",
                "distance": 0.0362
            },
            {
                "reference": "Pretraining Natural Language Processing (NLP) models with variants of Masked Language Model (MLM) objectives has led to significant improvements on many tasks.",
                "distance": 0.0418
            },
            {
                "reference": "Pretrained Masked Language Models (MLMs) have been previously found to be ineffective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on tasks such as natural language inference, sentence similarity, or paraphrasing using annotated task data.",
                "distance": 0.0449
            },
            {
                "reference": "Masked Language Models (MLMs) have exhibited outstanding performance in various Natural Language Processing (NLP) tasks. However, these models carry high levels of social biases. Existing evaluation metrics for identifying the social biases in MLMs have demonstrated deficiencies.",
                "distance": 0.0544
            },
            {
                "reference": "Whole word masking (WWM) improves the performance of the BERT model for English language by masking all subwords corresponding to a word at once. For Chinese language, the concept of a subword does not exist as each token is an atomic character, leading to a different meaning composition of words as they consist of multiple characters.",
                "distance": 0.0609
            },
            {
                "reference": "Pre-trained Masked Language Models (MLMs) such as BERT and RoBERTa have revolutionized natural language understanding. However, these models often output an anisotropic distribution of token representations that occupies a narrow subset of the representation space, which is not ideal for tasks requiring discriminative semantic meanings of distinct tokens.",
                "distance": 0.0614
            },
            {
                "reference": "Masked Language Models (MLM) pre-trained on massive datasets often undergo an additional training phase called the domain or task adaptation step before fine-tuning for the final task. However, the adaptation step is generally time-consuming requiring several GPU days.",
                "distance": 0.0667
            },
            {
                "reference": "Masked language models (MLMs) such as BERT are widely used for learning contextual representations. However, these models often adopt sampled embeddings as anchors to estimate and inject contextual semantics into representations, which can limit their efficiency and effectiveness.",
                "distance": 0.0721
            },
            {
                "reference": "Pretrained language models acquire useful inductive biases through masks used in the unsupervised fashion for prediction, which are believed to act as cloze reductions for downstream tasks. However, the effectiveness of random masking strategies used in practice has come into question.",
                "distance": 0.0883
            }
        ]
    },
    {
        "target": "The study is set in the context of the SemEval-2023 NLI4CT task which focuses on multi-evidence natural language inference for clinical trial data.",
        "prediction": "The paper addresses the problem of event detection, which is part of the SemEval-2010 Task.",
        "queries": [
            -2.281697,
            5.748788
        ],
        "log": [
            {
                "reference": "The paper addresses the problem of event detection, which is part of the SemEval-2010 Task.",
                "distance": 0.067
            },
            {
                "reference": "The paper discusses the issue of disorder identification within SemEval 2015's task 14, where existing methods for recognition and normalization are not satisfactory.",
                "distance": 0.0773
            },
            {
                "reference": "This task was a follow-up to the ShARe/CLEF eHealth 2013 shared task, for which it uses a larger test set.",
                "distance": 0.0801
            }
        ]
    },
    {
        "target": "End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. However, the dependence of previous KD approaches on manual audio transcripts limits the applicability to AST.",
        "prediction": "End-to-end speech-to-text translation (E2E-ST) is becoming popular due to its potential for lower error propagation, latency, and fewer parameters. Current high-quality E2E-ST systems leverage the \u3008speech, transcription\u3009 pair to pre-train the model and then the \u3008speech, translation\u3009 pair to further optimize it. However, this process only involves two-tuple data at each stage and doesn't fully use the association between triplet data.",
        "queries": [
            4.305175,
            13.675347
        ],
        "log": [
            {
                "reference": "End-to-end speech-to-text translation (E2E-ST) is becoming popular due to its potential for lower error propagation, latency, and fewer parameters. Current high-quality E2E-ST systems leverage the \u3008speech, transcription\u3009 pair to pre-train the model and then the \u3008speech, translation\u3009 pair to further optimize it. However, this process only involves two-tuple data at each stage and doesn't fully use the association between triplet data.",
                "distance": 0.058
            },
            {
                "reference": "End-to-end (E2E) speech-to-text translation (ST) often relies on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, leading to a substantial drop in translation performance when transcripts are unavailable. The significance of this pretraining for E2E ST has not been widely studied.",
                "distance": 0.0733
            },
            {
                "reference": "End-to-end Speech Translation (E2E ST) traditionally struggles with degraded performance when only limited ST data are available, and performance of these models often correlates with embedding similarity between speech and transcript.",
                "distance": 0.0771
            },
            {
                "reference": "In the field of speech translation (ST), multitask learning has been used to improve ST performance. This typically involves a recognition decoder that generates the source language text and a translation decoder that forms translations based on this output.",
                "distance": 0.0851
            },
            {
                "reference": "End-to-end Speech Translation (ST) traditionally requires parallel ST data for training, which is challenging and costly to obtain. Existing zero-shot methods fail to align the two modalities of speech and text into a shared semantic space, causing them to underperform compared to supervised ST methods.",
                "distance": 0.0873
            },
            {
                "reference": "Existing efforts to boost end-to-end speech translation (E2E-ST) performance lean on using source transcription with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, these efforts face challenges due to differing input modalities in the source language.",
                "distance": 0.0936
            },
            {
                "reference": "End-to-end speech-to-text translation (ST) methods interpret audio in a source language and output text in a target language. However, these methods are limited by the amount of available parallel corpus.",
                "distance": 0.095
            },
            {
                "reference": "Speech-to-text translation (ST), which directly translates the source language speech to target language text, has recently gained attention. The combination of speech recognition and machine translation in a single model poses a heavy burden on direct cross-modal cross-lingual mapping.",
                "distance": 0.0952
            }
        ]
    },
    {
        "target": "In language learning, Single Choice exercises are a central exercise type that has traditionally used distractors derived from static analyses of learner corpora. The process of deriving these distractors, however, has not been dynamically informed by learning analytics or utilization of half-open exercises.",
        "prediction": "In a fill-in-the-blank exercise, a student is presented with a sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. There are limited methods to generate these distractors automatically.",
        "queries": [
            -6.766468,
            10.588033
        ],
        "log": [
            {
                "reference": "In a fill-in-the-blank exercise, a student is presented with a sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. There are limited methods to generate these distractors automatically.",
                "distance": 0.0289
            },
            {
                "reference": "Current evaluation metrics for the automatic generation of Multiple Choice Questions (MCQ) focus on n-gram based similarity of the generated MCQ to the sample in the dataset, without evaluating their educational value or capacity to assess a student's knowledge of the target fact.",
                "distance": 0.0411
            },
            {
                "reference": "The need to generate cloze questions from definitions in the Collins COBUILD English language learner\u2019s dictionary to help second and third grade students learn new vocabulary in an automated reading tutor.",
                "distance": 0.0519
            },
            {
                "reference": "The current state-of-the-art models for Reading Comprehension with Multiple Choice Questions mainly focus on selecting the option that has the maximum similarity with a query-aware representation for the passage.",
                "distance": 0.0603
            },
            {
                "reference": "Previous methods for generating multiple choice cloze questions for testing children's reading comprehension generate general distractors and typically only test comprehension of an individual sentence.",
                "distance": 0.061
            },
            {
                "reference": "Creating distractive choices for open-domain cloze-style multiple-choice questions is a challenging task, especially ensuring that the distractors are plausible and reliable.",
                "distance": 0.0679
            },
            {
                "reference": "All previous works on distractor generation for multiple choice reading comprehension questions from examinations aimed at preparing words or short phrases distractors.",
                "distance": 0.0782
            },
            {
                "reference": "Most existing reading comprehension datasets focus on single-span answers, while multi-span questions, where the answer is a series of multiple discontiguous spans in the text, are common but less studied.",
                "distance": 0.0799
            },
            {
                "reference": "Multi-hop reasoning models must address challenges such as managing a fast-growing search space across the hops, representing complex queries combining multiple information needs, and resolving ambiguity about the best order to hop between training passages.",
                "distance": 0.081
            },
            {
                "reference": "The recent explosion in question answering research has produced a wealth of both factoid reading comprehension (RC) and commonsense reasoning datasets. But these datasets deal with tasks where a decision is made based on whether information is present in the text.",
                "distance": 0.0995
            }
        ]
    },
    {
        "target": "Pre-trained language models (PLMs) are used in various applications but often become overconfident in their wrong predictions, which is undesirable. Previous research introduced extra calibration tasks to mitigate the issue, using abundant extra samples available for calibration. However, these models do not typically consider scenarios where training samples are limited and need to be used effectively.",
        "prediction": "Pre-trained language models (PLMs) achieve remarkable performance on many downstream tasks, but may fail in giving reliable estimates of their predictive uncertainty. There's a lack of comprehensive understanding of PLMs' calibration, a topic that has not been thoroughly explored.",
        "queries": [
            -2.884868,
            15.763243
        ],
        "log": [
            {
                "reference": "Pre-trained language models (PLMs) achieve remarkable performance on many downstream tasks, but may fail in giving reliable estimates of their predictive uncertainty. There's a lack of comprehensive understanding of PLMs' calibration, a topic that has not been thoroughly explored.",
                "distance": 0.0086
            },
            {
                "reference": "Pretrained language models (PLMs) typically use a fixed number of steps for inference, leading to inefficiencies in certain situations and potential overthinking which can hinder performance.",
                "distance": 0.0099
            },
            {
                "reference": "Large-scale Protein Language Models (PLMs) have been successful in various protein prediction tasks and Evoformer, a PLM used in AlphaFold, has remained largely unexplored beyond structure prediction.",
                "distance": 0.0103
            },
            {
                "reference": "Pre-trained Language Models (PLMs) have been widely used in passage re-ranking tasks due to their natural language understanding capabilities. However, these models often suffer from issues such as vocabulary mismatch and lack of domain-specific knowledge.",
                "distance": 0.0121
            },
            {
                "reference": "Pretrained language models (PLMs) have significantly improved text generation, but can produce unfaithful or inappropriate content. On the other hand, template-based systems offer faithfulness, but lack fluency.",
                "distance": 0.0121
            },
            {
                "reference": "Pre-trained language models (PLMs) have shown impressive performance by self-supervised training on text, yet they lack visual semantics or commonsense related to properties such as sizes, shapes, and colors of commonplace objects. Existing solutions usually depend on explicit images, which are time-consuming to retrieve or generate, and apply augmentation to the whole input text indiscriminately.",
                "distance": 0.0138
            },
            {
                "reference": "Pre-trained language models (PLMs) show notable prediction performance in various natural language processing (NLP) tasks. However, they often lack reliable indication of the trustworthiness of their predictions, especially in safety-critical applications. Prior studies have only partially addressed this issue without providing a comprehensive analysis on how to optimize a well-calibrated PLM-based prediction pipeline.",
                "distance": 0.0154
            },
            {
                "reference": "Recently, Pre-trained Language Models (PLMs) have been improved by adding knowledge facts to enhance language understanding, particularly in the medical domain where terms and relations can be complex.",
                "distance": 0.0211
            },
            {
                "reference": "Pretrained language models (PLMs) have shown success on several downstream tasks due to their ability to model language. While efforts have been made to explore the syntactic structures entailed by PLMs, grounding capabilities of PLMs have been less investigated.",
                "distance": 0.0292
            },
            {
                "reference": "Pre-trained language models (PLMs) learn universal language representations through self-supervised training tasks on large-scale corpora, however the quality of word representations tends to depend on their frequency in the corpus. As a result, the embeddings of rare words are often poorly optimized.",
                "distance": 0.0302
            },
            {
                "reference": "Pre-trained language models (PLMs) have been effectively used in news recommendation, but they often suffer from the domain shift problem between the pre-training corpus and downstream news texts. Additionally, PLMs are computationally heavy, impacting their usability in low-latency online services.",
                "distance": 0.0309
            },
            {
                "reference": "Although pre-trained language models (PLMs) are commonly used in NLP and provide generic initialization for finding high-performance minima, little is understood about the connections between the different minima reached under various adaptation configurations.",
                "distance": 0.036
            },
            {
                "reference": "Pre-trained Language Models (PLMs) are widely available and used for multi-task fine-tuning across domains. However, they may memorize nontransferable knowledge when applied to tasks from distant domains with different class label sets, leading to negative transfer.",
                "distance": 0.0417
            },
            {
                "reference": "Current large-scale Pre-trained Language Models (PLMs) have two challenges. They can be inefficient in terms of memory and computation, and they can struggle to generalize to out-of-distribution (OOD) data, often relying on data set bias.",
                "distance": 0.0441
            },
            {
                "reference": "The reuse of pre-trained language models (PLMs) has been investigated to reduce computational cost and environmental side-effects. However, the challenge lies in how to combine knowledge from different teacher-PLMs, each specializing in a different classification problem, without available human annotations.",
                "distance": 0.0442
            },
            {
                "reference": "Domain-specific Pre-trained Language Models (PLMs) have been proposed to enhance task performance within specific domains. However, this Domain Adaptive Pre-training (DAPT) tends to forget the previous general knowledge acquired by general PLMs, which leads to the phenomenon of catastrophic forgetting and sub-optimal performance.",
                "distance": 0.0455
            },
            {
                "reference": "Latency and efficiency issues are often overlooked in the evaluation of Information Retrieval (IR) models based on Pretrained Language Models (PLMs) across various hardware and software testing scenarios. The SPLADE model, which has achieved state-of-the-art zero-shot performance and competitive results on TREC collections, has efficiency controlled via a regularization factor which has proven not to be efficient enough.",
                "distance": 0.0456
            },
            {
                "reference": "Pre-trained language models (PLMs) improve the generalization of natural language understanding models, but the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods.",
                "distance": 0.0459
            },
            {
                "reference": "Predominantly, pretrained language models (PLMs) are combined with textual patterns to enhance performance in both the zero and few-shot learning settings. The patterns used either directly resemble the text seen during the PLM's pretraining (for zero-shot tasks) or they are adapted more flexibly to the PLM's unique characteristics (for supervised training tasks).",
                "distance": 0.0474
            },
            {
                "reference": "Pre-trained models (PTMs) are used extensively in natural language processing and computer vision tasks, and have been applied to geographic tasks on Baidu Maps with some success. However, a performance plateau was observed due to these generic PTMs not possessing inherent geographical knowledge.",
                "distance": 0.0495
            }
        ]
    },
    {
        "target": "Non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than Autoregressive translation (AT), which can lead to performance issues in Neural Machine Translation.",
        "prediction": "Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the decoder inputs, yield significantly faster inference but produce less accurate results compared to autoregressive translation (AT) models. The quality of decoder inputs holds significant impact on model accuracy.",
        "queries": [
            1.976118,
            15.959039
        ],
        "log": [
            {
                "reference": "Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the decoder inputs, yield significantly faster inference but produce less accurate results compared to autoregressive translation (AT) models. The quality of decoder inputs holds significant impact on model accuracy.",
                "distance": 0.0156
            },
            {
                "reference": "Non-autoregressive translation (NAT) achieves faster inference speed but sacrifices accuracy compared to autoregressive translation (AT). AT is an easier task than NAT due to the explicit dependency on previous target-side tokens, but transitioning the model training directly from AT to NAT is challenging.",
                "distance": 0.0171
            },
            {
                "reference": "Non-Autoregressive machine Translation (NAT) methods offer high efficiency but suffer from issues such as repeated translations, due to indistinguishable adjacent decoder hidden states, and incomplete translations, due to incomplete transfer of source side information via the decoder hidden states.",
                "distance": 0.0195
            },
            {
                "reference": "Non-autoregressive translation (NAT) models can predict all tokens simultaneously during inference, resulting in faster speed compared to autoregressive translation (AT) models. However, NAT models often suffer from lower translation quality compared to AT and current methods to improve NAT models do not fully utilize its potential.",
                "distance": 0.0228
            },
            {
                "reference": "Non-autoregressive translation (NAT) can significantly accelerate the inference process by predicting the entire target sequence at once. However, it struggles with learning high-mode knowledge, such as one-to-many translations, which limits its effectiveness.",
                "distance": 0.0246
            },
            {
                "reference": "Non-autoregressive translation (NAT) models are typically trained with the cross-entropy loss, which penalizes small shifts in word positions. Existing latent alignment models allow for word reordering but only in a monotonic fashion, which is a limitation given the global word reordering nature of machine translation.",
                "distance": 0.0271
            },
            {
                "reference": "Non-autoregressive translation (NAT) models face challenges in capturing the multi-modal distribution of target translations, known as the 'multi-modality problem'. This problem includes two aspects: lexical and syntactic multi-modality. While lexical multi-modality has been studied, syntactic multi-modality remains understudied.",
                "distance": 0.0316
            },
            {
                "reference": "Non-autoregressive machine translation (NAT) approaches, which are faster due to their parallelizable processes, have a bottleneck in the decoder layers. Unlike autoregressive models, decoder layers in NAT models significantly influence accuracy.",
                "distance": 0.0317
            },
            {
                "reference": "Non-autoregressive Transformers (NATs) significantly reduce decoding latency by generating all tokens in parallel, but they struggle to capture dependencies between the tokens for generating multiple possible translations.",
                "distance": 0.0416
            },
            {
                "reference": "Current autoregressive approaches for translation suffer from high latency, and non-autoregressive translation (NAT) models, which are based on iterative editing, struggle to handle low-frequency constraints.",
                "distance": 0.0454
            },
            {
                "reference": "Non-autoregressive machine translation (NAT) offers low latency but suffers severe performance deterioration due to the naive independence assumption. The independence assumption is further reinforced by the cross-entropy loss which requires a strict match between the hypothesis and the reference token by token.",
                "distance": 0.0479
            },
            {
                "reference": "Non-Autoregressive machine Translation (NAT) models are faster but less accurate than Autoregressive machine Translation (AT) models. A common way to improve NAT models is by using knowledge distillation, which involves transferring knowledge from AT to NAT models.",
                "distance": 0.0504
            },
            {
                "reference": "Non-autoregressive Transformer (NAT) is a family of text generation models designed to reduce decoding latency by predicting entire sentences in parallel. However, this approach sacrifices the ability to capture left-to-right dependencies, making NAT learning very challenging.",
                "distance": 0.0545
            },
            {
                "reference": "Non-autoregressive (NAR) models are showing promise for machine translation but are limited by their dependence on knowledge distillation from autoregressive models. It is suggested that distilled training data is less complex than manual translations.",
                "distance": 0.0702
            },
            {
                "reference": "Recent research in machine translation often favor non-autoregressive (NAR) models due to the promise of faster translation, while also maintaining interest in optimizing autoregressive models as part of the WMT shared task on efficient translation.",
                "distance": 0.0733
            },
            {
                "reference": "Non-autoregressive (NAR) neural machine translation models have garnered attention due to their efficient parallel decoding. However, they fall short in characterizing human language data due to requiring conditional independence assumption on target sequences, resulting in less informative learning signals.",
                "distance": 0.0778
            },
            {
                "reference": "Fully non-autoregressive neural machine translation (NAT) has been developed to predict all the tokens simultaneously in a single forward pass of neural networks. However, this model significantly reduces the inference latency, causing a quality drop compared to traditional Transformer models.",
                "distance": 0.0828
            },
            {
                "reference": "Non-autoregressive approaches improve the inference speed of translation models by requiring only a single forward pass to generate the output sequence but their translation quality tends to lag behind their autoregressive counterparts due to output token interdependence.",
                "distance": 0.0857
            },
            {
                "reference": "Non-autoregressive neural machine translation (NAT) has a multi-modality problem where the source sentence can have multiple correct translations, but the loss function is only calculated according to the reference sentence. Sequence-level knowledge distillation was used to make the target more deterministic but the multi-modality problem is still substantial. Also, learning from a specific teacher limits the capacity of the model and restricts the potential of NAT models.",
                "distance": 0.0875
            },
            {
                "reference": "The current problem that arises in training fully non-autoregressive translation (NAT) models is the effect of word reordering; a common source of the critical multimodality problem.",
                "distance": 0.0952
            }
        ]
    },
    {
        "target": "Knowledge graphs (KGs) are far from complete, making KG completion (KGC) a crucial task in KG research. Recently, Message Passing (Graph) Neural Networks (MPNNs), which include a message passing (MP) component, have been attributed with the success in achieving powerful embeddings for this task, superior to simpler multi-layer perceptron (MLP) models.",
        "prediction": "Despite the increasing popularity of Graph convolutional networks (GCNs) in knowledge graph completion (KGC), many GCN-based KGC models fail to outperform state-of-the-art knowledge graph embedding (KGE) models and introduce additional computational complexity.",
        "queries": [
            -6.925444,
            -16.910292
        ],
        "log": [
            {
                "reference": "Despite the increasing popularity of Graph convolutional networks (GCNs) in knowledge graph completion (KGC), many GCN-based KGC models fail to outperform state-of-the-art knowledge graph embedding (KGE) models and introduce additional computational complexity.",
                "distance": 0.0899
            },
            {
                "reference": "Graph Neural Network (GNN) approaches are typically used for question answering (QA) tasks based on knowledge graphs (KG), but a significant challenge is effectively utilizing interactions between the QA context and KG. Existing models adopt the same QA context representation for all KG layers, resulting in limited interaction.",
                "distance": 0.2841
            }
        ]
    },
    {
        "target": "Pre-trained Language Models have demonstrated exceptional performance on many text generation tasks. However, they endure inefficient computation and memory handling due to their large-scale parameters and the universal autoregressive decoding paradigm.",
        "prediction": "Chinese pre-trained language models, such as BERT, BERTwwm and ERNIE 1.0, primarily exploit contextual character information to learn representations, while largely neglecting linguistic knowledge like word and sentence information.",
        "queries": [
            -1.361705,
            14.880607
        ],
        "log": [
            {
                "reference": "Chinese pre-trained language models, such as BERT, BERTwwm and ERNIE 1.0, primarily exploit contextual character information to learn representations, while largely neglecting linguistic knowledge like word and sentence information.",
                "distance": 0.0291
            },
            {
                "reference": "Pre-trained language models like BERT have achieved success in a variety of natural language processing tasks, and researchers have adapted these models for different application domains. However, these domain-oriented models often struggle to accurately understand domain phrases. Also, the semantic learning of pre-training models could be significantly improved by using entity-level association knowledge, but this could introduce noise due to the lack of ground truth word-level alignment.",
                "distance": 0.031
            },
            {
                "reference": "Pre-trained language models such as BERT and its variants have significantly improved various NLP tasks, but a theoretical framework for studying their relationships is lacking.",
                "distance": 0.0512
            },
            {
                "reference": "Existing approaches for fine-tuning pre-trained language models have shown to be unstable across hyper-parameter settings, inspiring recent work on trust region methods.",
                "distance": 0.0658
            },
            {
                "reference": "Pre-trained language models such as BERT and ERNIE have proven successful in natural language understanding tasks, including Information Retrieval models. However, most of these models focus solely on character-level relevance matching, neglecting other key granularity information like words and phrases which may lead to query understanding ambiguity and inaccurate matchings in web searches.",
                "distance": 0.0691
            },
            {
                "reference": "Many studies, known as BERTology, have analyzed components of pre-trained language models, yet the understanding of discourse information in these models remains limited.",
                "distance": 0.0712
            },
            {
                "reference": "State-of-the-art pretrained contextualized models (PCM) like BERT are evaluated using tasks such as WiC and WSD, assuming that performance in these tasks reflects how well a model represents the coupled word and context semantics.",
                "distance": 0.074
            },
            {
                "reference": "Pre-trained language models like BERT have achieved state-of-the-art performance on natural language inference (NLI). However, these models can be tricked by variations in surface patterns such as syntax.",
                "distance": 0.0759
            },
            {
                "reference": "Several studies have been carried out on revealing linguistic features captured by BERT, usually done by training a diagnostic classifier on representations from different layers and interpreting subsequent classification accuracy as representation of linguistic property knowledge. However, these studies have not considered the potential role of token representations.",
                "distance": 0.0782
            },
            {
                "reference": "ELMo and BERT have improved word representation by modeling context information and have shown effectiveness on Named Entity Recognition. However, they do not incorporate the prior knowledge of entities from an external knowledge base.",
                "distance": 0.0888
            },
            {
                "reference": "BERT models have been used widely for various NLP tasks, however, they do not incorporate factual knowledge about entities. Previous work such as ERNIE and KnowBert have tried to imbue entity knowledge into BERT but required expensive pre-training of the BERT encoder.",
                "distance": 0.0914
            }
        ]
    },
    {
        "target": "Contemporary methods for document ranking either use more efficient transformers or divide long documents into passages to handle the long input. Some recent works propose cascade document ranking models where a selector is used to extract relevant passages before ranking. However, these models have the issues of the selection and ranking modules being almost independently optimized and deployed that lead to selecting error reinforcement and sub-optimal performance.",
        "prediction": "The scarcity of ground truth relevance labels has limited the application of supervised methods to ad-hoc retrieval; as a result, unsupervised scoring methods like BM25 often outperform deep learning methods. Recent works have attempted to use the performance of unsupervised methods to generate training data for learning-to-rank models, but typically require an impractically large amount of data to supersede the performance of the original unsupervised method.",
        "queries": [
            -9.42723,
            7.354749
        ],
        "log": [
            {
                "reference": "The scarcity of ground truth relevance labels has limited the application of supervised methods to ad-hoc retrieval; as a result, unsupervised scoring methods like BM25 often outperform deep learning methods. Recent works have attempted to use the performance of unsupervised methods to generate training data for learning-to-rank models, but typically require an impractically large amount of data to supersede the performance of the original unsupervised method.",
                "distance": 0.0401
            },
            {
                "reference": "Ad-hoc retrieval models with implicit feedback often have problems like imbalanced classes in the data set, which can lead to poor generalization ability due to too few clicked documents, and may harm effectiveness and efficiency due to too many non-clicked documents. Furthermore, recent neural network-based models are vulnerable to adversarial examples due to their linear nature.",
                "distance": 0.0858
            }
        ]
    },
    {
        "target": "Probabilistic modeling is a central task in machine learning. Zhang et al. (ICML 2021) recently proposed a new model, probabilistic generating circuits, and raised the question whether every strongly Rayleigh distribution can be efficiently represented by such circuits.",
        "prediction": "Graphical models are fundamental tools in machine learning and data mining, combining probability and graph theories. However, traditional algorithms for learning graphical models lack scalability, making them less usable as data becomes more voluminous and higher-dimensional.",
        "queries": [
            0.139555,
            -14.835479
        ],
        "log": [
            {
                "reference": "Graphical models are fundamental tools in machine learning and data mining, combining probability and graph theories. However, traditional algorithms for learning graphical models lack scalability, making them less usable as data becomes more voluminous and higher-dimensional.",
                "distance": 0.034
            },
            {
                "reference": "Logic-based probabilistic modeling incorporates Bayesian inference techniques with generative models like Bayesian networks and PCFGs.",
                "distance": 0.0581
            },
            {
                "reference": "The problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting is persistent, with existing techniques, including convex-optimization based algorithms, presenting limitations in terms of sample complexity and underlying assumptions.",
                "distance": 0.0711
            },
            {
                "reference": "Graphical models of probabilistic dependencies have been extensively used in contexts where classical uncertainty is present. Nonetheless, in domains such as computational physics and quantum computing where the uncertainty is non-classical, the laws of classical probability are surpassed by those of quantum mechanics.",
                "distance": 0.0856
            }
        ]
    },
    {
        "target": "Large-scale text-to-image generative models like diffusion models can synthesize convincing images by following a text prompt. However, current image editing techniques often have unintended modifications of regions outside the targeted area due to inaccurate cross-attention maps.",
        "prediction": "Text-to-image generative models are a new and powerful tool to produce visual artwork based on textual prompts, but it can be hard to predict the quality of the generated output due to the open-ended nature of the text input.",
        "queries": [
            8.977325,
            7.071667
        ],
        "log": [
            {
                "reference": "Text-to-image generative models are a new and powerful tool to produce visual artwork based on textual prompts, but it can be hard to predict the quality of the generated output due to the open-ended nature of the text input.",
                "distance": 0.0269
            },
            {
                "reference": "Text-to-Image generation in the general domain has long been an open problem, which requires a powerful generative model and cross-modal understanding.",
                "distance": 0.0332
            },
            {
                "reference": "Image generation models have been developed, but few focus specifically on the concept of domain transfer at both the semantic and pixel levels, with an emphasis on producing realistic images",
                "distance": 0.0342
            },
            {
                "reference": "Generating and editing images from open domain text prompts is a challenging task that traditionally required expensive and specially trained models. Previous approaches like DALL-E, GLIDE, and Open-Edit offered less flexibility.",
                "distance": 0.0377
            },
            {
                "reference": "Prior work on text-to-image (T2I) generation has treated the problem as a uni-directional task and utilized pre-trained language models to establish image-text consistency.",
                "distance": 0.0394
            },
            {
                "reference": "Existing text-to-image synthesis methods are generally only applicable to words in the training dataset, which is limiting when it comes to highly variable objects like human faces.",
                "distance": 0.0402
            },
            {
                "reference": "Existing text-to-image generation strategies have traditionally centered on finding better modeling assumptions to train on a fixed dataset, which may involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks.",
                "distance": 0.0429
            },
            {
                "reference": "Prior data augmentation methods have improved the performance of few-shot text classification, but they lack intricate compositional structure and thus fail to generate samples with plausible and diverse sentence structures.",
                "distance": 0.0454
            },
            {
                "reference": "Current models for text-to-image generation struggle to control specific features of the images synthesized based on language descriptions.",
                "distance": 0.0515
            },
            {
                "reference": "Text-to-image generative models have made significant progress in generating high-quality images from natural language descriptions, but it is shown that these models are biased toward specific social groups with neutral text descriptions.",
                "distance": 0.0623
            },
            {
                "reference": "Existing methods for text-to-image synthesis typically learn a direct mapping from text to image, which can lead to outputs that lack meaningful semantic alignment with the input text.",
                "distance": 0.0652
            },
            {
                "reference": "Large-scale diffusion neural networks have marked a significant development in text-to-image generation but their interpretability is limited.",
                "distance": 0.0883
            },
            {
                "reference": "Recent advancements in diffusion models allow users to generate high-quality images from text prompts, however, generating images with desired details requires proper prompts, and it's unclear how a model reacts to different prompts and how to determine the best prompts.",
                "distance": 0.0907
            },
            {
                "reference": "Conditional text-to-image generation is an active area of research. Existing research, however, has primarily focused on generating a single image based on available conditioning information in just one step.",
                "distance": 0.0947
            }
        ]
    },
    {
        "target": "Sequence-level knowledge distillation reduces the size of Seq2Seq models for more efficient abstractive summarization, but it often leads to a loss of abstractiveness in summarization.",
        "prediction": "The pyramid method for content evaluation of automated summarizers produces scores that correlate well with manual scores used in educational assessment of students\u2019 summaries.",
        "queries": [
            -5.769851,
            8.717417
        ],
        "log": [
            {
                "reference": "The pyramid method for content evaluation of automated summarizers produces scores that correlate well with manual scores used in educational assessment of students\u2019 summaries.",
                "distance": 0.0181
            },
            {
                "reference": "Existing summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but these methods are rough proxies for true summary quality.",
                "distance": 0.025
            },
            {
                "reference": "State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, the underlying characteristics of these datasets (such as data noise and summarization complexity) and their effects on system performance and the reliability of automatic metrics like ROUGE are relatively unknown.",
                "distance": 0.0251
            },
            {
                "reference": "In controllable text generation, structure-controlled summarization has recently become an interesting research direction. However, current structure-controlling methods have shown limited effectiveness in enforcing the desired structure.",
                "distance": 0.0287
            },
            {
                "reference": "Current progress in abstractive text summarization primarily depends on large pre-trained sequence-to-sequence Transformer models, which have significant computational demands. Methods based on pseudo-labeling are popular in sequence-to-sequence model distillation.",
                "distance": 0.0313
            },
            {
                "reference": "Current sequence-to-sequence learning frameworks for abstractive summarization struggle with a gap between the learning objective and evaluation metrics.",
                "distance": 0.0315
            },
            {
                "reference": "Typically, the quality of a summary is evaluated by comparing its measures with quality scores given by human annotators, with a higher correlation implying a better measure.",
                "distance": 0.0341
            },
            {
                "reference": "Traditional approaches to summary quality scoring rely on human annotators and recently proposed approaches such as BLANC which is based on the performance of the language model with and without the help of a summary.",
                "distance": 0.0345
            },
            {
                "reference": "Neural abstractive summarization models have high overlap with human references, but they are not optimized for factual correctness, a crucial attribute in real-world applications such as summarizing radiology reports.",
                "distance": 0.0353
            },
            {
                "reference": "Abstractive neural summarization models have become better at generating summaries, evidenced by improved ROUGE scores. However, there is a lack of understanding about the strategies such models employ and how these strategies relate to their understanding of language. A need thus exists to characterize popular abstractive models, like the pointer-generator model, and understand how their mechanisms work.",
                "distance": 0.0364
            },
            {
                "reference": "Current abstractive summarization systems often generate content that isn't faithful to the original document, leading to misinformation. Current training methods like maximum likelihood training are not able to distinguish between factual errors and other model errors.",
                "distance": 0.0366
            },
            {
                "reference": "The original beam search algorithm for neural abstractive summarization suffers from a local optimality problem.",
                "distance": 0.0372
            },
            {
                "reference": "The surge in online audio-visual content has necessitated summarization systems. However, these systems often fall short because their summaries are generated based on heuristics that are optimized for traditional evaluation metrics, which merely score summaries against subjective gold standards without considering the summary's usability for a specific user goal or coherence.",
                "distance": 0.0395
            },
            {
                "reference": "In the domain of text summarization, acknowledges the scarcity and ambiguity of ground-truth summaries, and the need for human preferences to guide the output generation. It also highlights the requirement for dynamic feedback exchanges between humans and AI in practical settings, wherein feedback is provided online and in small quantities.",
                "distance": 0.0401
            },
            {
                "reference": "Language models for tasks like summarization are currently trained on predicting human reference summaries and evaluated using metrics like ROUGE, though these approaches are acknowledged to be rough proxies for true summary quality.",
                "distance": 0.0432
            },
            {
                "reference": "Sequence-to-sequence models for abstractive summarization are frequently used but often generate summaries that contain inaccurate content and are similar to the source text (near-extractive).",
                "distance": 0.0456
            },
            {
                "reference": "Sentence summarization, which compresses a long sentence while keeping the main gist, is a critical task with many real-world applications like headline generation. Though past research has developed various approaches to improve ROUGE score as an evaluation metric for summarization, the control of summary length has been largely overlooked.",
                "distance": 0.0464
            },
            {
                "reference": "Current models of abstractive summarization, even with the benefit of pre-trained language models and large-scale datasets, suffer from generating factually inconsistent summaries. Attempts have been made to handle this issue with models that identify factual inconsistencies, but they focus only on English, which has abundant resources.",
                "distance": 0.0482
            },
            {
                "reference": "Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors.",
                "distance": 0.049
            },
            {
                "reference": "The evaluation of automatic summaries of business meetings has been a concern in related research.",
                "distance": 0.0492
            }
        ]
    },
    {
        "target": "ReParameterization (RP) Policy Gradient Methods have been widely adopted for continuous control tasks in robotics and computer graphics. However, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance when applied to long-term reinforcement learning problems, leading to slow convergence.",
        "prediction": "Policy gradient methods have performed well in solving complex reinforcement learning challenges but still suffer from large variance issues on policy gradient estimation, which leads to poor sample efficiency during training.",
        "queries": [
            9.472511,
            -21.294401
        ],
        "log": [
            {
                "reference": "Policy gradient methods have performed well in solving complex reinforcement learning challenges but still suffer from large variance issues on policy gradient estimation, which leads to poor sample efficiency during training.",
                "distance": 0.0075
            },
            {
                "reference": "Actor-critic algorithms are commonly used for reinforcement learning but require computation of compatible features which can become prohibitively difficult. Modularization of computation, each having only local information, can increase biological plausibility of reinforcement learning methods.",
                "distance": 0.008
            },
            {
                "reference": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation, but the usual individual gradient approach may lead to issues with cycling and slower convergence.",
                "distance": 0.0207
            },
            {
                "reference": "Actor-critic is one of the most popular families of reinforcement learning algorithms. However, existing works on actor-critic primarily focus on two-timescale updates where the actor and critic are updated in separate steps.",
                "distance": 0.0268
            },
            {
                "reference": "The problem of temporal credit assignment in reinforcement learning is typically resolved using Q-learning and other algorithms based on the methods of temporal differences (TD(\u03bb)). However, using TD-based algorithms with \u03bb > 0 often allows faster credit propagation, it also involves certain implementational problems such as lack of generality and computational inefficiency.",
                "distance": 0.0285
            },
            {
                "reference": "Existing actor-critic algorithms are designed based on the Bellman optimality equation, in which actor (policy) and critic (value function) often have different objectives and their updates are separated.",
                "distance": 0.0298
            },
            {
                "reference": "Policy gradient-based methods used in continuous control are limited by the need for the agent's underlying probability distribution which limits policy representation to parametric distribution classes. This limitation results in local movement in the action space and can lead to the convergence of sub-optimal solutions.",
                "distance": 0.0322
            },
            {
                "reference": "Policy Optimization (PO) algorithms have been used widely to handle the high-dimensionality of real-world continuous control tasks. Trust Region Policy Optimization methods, which use the Kullback-Leibler (KL) divergence to limit policy changes, are popular. The Wasserstein distance is considered as an alternative, however, current works either use approximations or don't provide algorithms for continuous state-action spaces.",
                "distance": 0.0445
            },
            {
                "reference": "Modern policy gradient algorithms, such as Proximal Policy Optimization (PPO), use a number of heuristics including loss clipping and gradient clipping to ensure successful learning. These techniques, however, often resemble methods from robust statistics which are used in outlier-rich environments.",
                "distance": 0.0483
            },
            {
                "reference": "Q-learning, a popular reinforcement learning algorithm, has always included an extension to eligibility traces for more rapid learning and improved asymptotic performance on non-Markov problems. The \u03bb parameter in off-policy algorithms such as Q(\u03bb), GQ(\u03bb), and off-policy LSTD(\u03bb) was intended to smoothly shift the learning from bootstrapping to Monte Carlo, but fails to approximate Monte Carlo learning when \u03bb = 1.",
                "distance": 0.0495
            },
            {
                "reference": "Actor-critic algorithms for reinforcement learning have been re-popularized due to their good convergence properties in situations where other approaches tend to fail, such as when function approximation is involved. Additionally, there is a growing body of evidence that phasic dopamine signals-based actor-critic approaches play a key role in biological learning.",
                "distance": 0.0508
            },
            {
                "reference": "Deep Deterministic Policy Gradients (DDPG), a commonly used actor-critic reinforcement learning algorithm for continuous control, suffers from overestimation that can hinder performance. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm mitigates overestimation but introduces a large underestimation bias.",
                "distance": 0.0523
            },
            {
                "reference": "Policy gradient (PG) reinforcement learning algorithms, despite their strong (local) convergence guarantees, are plagued by a high variance in the estimation of the gradient that limits their learning performance.",
                "distance": 0.0547
            },
            {
                "reference": "Actor-Critic (AC) methods are extensively used in reinforcement learning and are known to be closely related to policy gradient (PG) methods, but the exact relationship and differences between these two approaches have not been fully explored or characterized.",
                "distance": 0.0548
            },
            {
                "reference": "Policy gradient methods in deep reinforcement learning have been successful but suffer from high variance of gradient estimates, which is particularly severe in problems with long horizons or high-dimensional action spaces.",
                "distance": 0.0614
            },
            {
                "reference": "Policy gradient methods are widely used in control and reinforcement learning. However, existing convergence analysis relies on non-intuitive, impractical, and often opaque conditions and is achieved under strict regularity conditions in limited settings.",
                "distance": 0.0639
            },
            {
                "reference": "Policy Optimization (PO) is a widely used approach to address continuous control tasks, but traditional methods do not reuse samples generated by one policy to estimate the performance of other policies.",
                "distance": 0.0686
            },
            {
                "reference": "Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, performing at state-of-the-art levels. However, wide-scale adoption in real-world domains is hampered by poor sample efficiency.",
                "distance": 0.0695
            },
            {
                "reference": "Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. The statistically efficient estimation of policy gradients from off-policy data is a challenging task.",
                "distance": 0.0698
            },
            {
                "reference": "Policy gradient approaches are a widely used learning method for deciding how to interact with environments. However, existing methods focus on propositional and continuous domains only and struggle to handle structured domains with a varying number of objects and relations without substantial feature engineering.",
                "distance": 0.0729
            }
        ]
    },
    {
        "target": "Graph-based Collaborative Filtering (GCF) is widely used in personalized recommendation systems. However, GCF suffers from a fundamental problem where features tend to occupy the embedding space inefficiently, thereby resulting in a popularity bias or the Matthew effect where popular items are highly recommended and less popular ones are ignored.",
        "prediction": "Collaborative Filtering (CF) methods like PinSage and LightGCN have offered the state-of-the-art performance in parameterizing users and items into latent representation spaces from interaction data. However, existing solutions have failed to address the over-smoothing effect in deep graph-based CF architectures and the scarceness and skewed distribution of supervision signals.",
        "queries": [
            11.462492,
            -16.336065
        ],
        "log": [
            {
                "reference": "Collaborative Filtering (CF) methods like PinSage and LightGCN have offered the state-of-the-art performance in parameterizing users and items into latent representation spaces from interaction data. However, existing solutions have failed to address the over-smoothing effect in deep graph-based CF architectures and the scarceness and skewed distribution of supervision signals.",
                "distance": 0.005
            },
            {
                "reference": "Collaborative filtering (CF) is a common technique in recommendation systems, with Graph Neural Network (GNN)-based CF models like NGCF, LightGCN, and GTN achieving significant advances. However, item recommendation essentially being a link prediction problem, existing works tend to focus more on aggregating information from user and item nodes, rather than on direct link modeling.",
                "distance": 0.0135
            },
            {
                "reference": "Collaborative filtering (CF) algorithms often face challenges due to matrix sparsity, which can hurt performance. Currently, most CF methods are limited due to not incorporating auxiliary and social information properly.",
                "distance": 0.0203
            },
            {
                "reference": "Collaborative filtering (CF) relies on encoders to embed users and items into a single representation space, with the Bayesian personalized ranking (BPR) loss commonly employed to learn these encoders. Existing studies primarily focus on designing advanced encoders, such as graph neural networks, to improve representations, but little research investigates the vital properties of representations in CF.",
                "distance": 0.022
            },
            {
                "reference": "Collaborative filtering (CF) is a common recommendation approach that often uses user-item rating data. However, this approach encounters problems with the intrinsic sparsity of user-item ratings and can often lack transparency and explainability due to the use of latent features.",
                "distance": 0.0224
            },
            {
                "reference": "Collaborative filtering (CF) used in recommender systems often suffers from degrading performance due to sparse user-item matrices common in real-world applications. While some CF methods utilize side information to mitigate issues of data sparsity and cold start, these latent factors may not be effective due to the inherent data sparsity.",
                "distance": 0.0227
            },
            {
                "reference": "Widespread deployment of recommender systems is affected by the quality of user feedback, leading to noise interference and potential vulnerabilities to malicious attacks. Despite previous attempts to ensure the robustness of Collaborative Filtering (CF) approaches, current solutions like Neighbor Selection algorithms, Association Rules, and Robust Matrix Factorization have produced sub-par results.",
                "distance": 0.0231
            },
            {
                "reference": "Collaborative filtering is a known technique for recommending documents to users based on the similarity of their tastes, and the generalized vector space model (GVSM) is a method in information retrieval that represents a document by a vector of its similarities to all other documents.",
                "distance": 0.0235
            },
            {
                "reference": "Collaborative filtering (CF) for recommender systems has been the focus of many studies. Most previous works concentrate on improving the rating prediction formula, with modifications such as adding user and item biases, implicit feedback and time-aware factors. However, incorporating additional regularization to the objective function within CF has not been adequately explored.",
                "distance": 0.0238
            },
            {
                "reference": "Existing collaborative filtering (CF) approaches usually model interactions solely within the Euclidean space. However, user-item interactions inherently present highly non-Euclidean anatomy with various types of geometric patterns, making Euclidean-based models potentially inadequate.",
                "distance": 0.0239
            },
            {
                "reference": "Existing item-based collaborative filtering (ICF) methods only use the relation of collaborative similarity, evidenced by user interactions, to generate recommendations. In real-world scenarios, multiple relations between items exist such as shared attributes or complementary functions.",
                "distance": 0.0245
            },
            {
                "reference": "Collaborative filtering (CF) methods are employed for personalized recommendations based on historical user-item interactions. While deep learning has improved the performance of CF methods, these deep learning-based CF methods require intense computation, making real-time end-to-end neural recommendations costly.",
                "distance": 0.0254
            },
            {
                "reference": "Existing Collaborative Filtering (CF) methods focus on matching user and item embeddings to capture associative relevance patterns in data. Yet, recommendation, as a cognitive task, requires not only pattern recognition but also cognitive reasoning from data.",
                "distance": 0.0256
            },
            {
                "reference": "While Collaborative Filtering (CF) has proven successful in providing personalized products and services, dealing with the increasing sparseness of user-item matrix is a challenge. Hybrid CFs have been extensively researched to enhance performance, often using hand-crafted feature engineering which can be noise-prone and biased by the scheme used for feature extraction and selection.",
                "distance": 0.0267
            },
            {
                "reference": "Current embedding functions in Collaborative Filtering (CF) methodologies model user-item relationships in a uniform manner, leading to the neglect of the diversity of user intents when interacting with items. This results in suboptimal representations and fails to model diverse relationships and disentangle user intents.",
                "distance": 0.0287
            },
            {
                "reference": "Binary coding techniques for Collaborative Filtering (CF) have been growing in interest due to their potential for fast user-item affinity computation in the Hamming space. However, the limited representation capability of binary codes, which naturally suffer from low accuracy, hinders it from modeling complex data structure.",
                "distance": 0.0288
            },
            {
                "reference": "Existing neural graph-based Collaborative Filtering (CF) models learn user and item embeddings based on the user-item bipartite graph structure which does not include unobserved behaviors as possible positive links. Moreover, previous graph learning models relied mostly on node features and localized optimization goals, leaving some global properties of the graph inadequately modelled.",
                "distance": 0.031
            },
            {
                "reference": "Nearest-neighbor collaborative filtering (CF) algorithms are gaining acceptance in recommender systems and e-commerce applications. However, they face performance issues when dealing with volume of data that is extremely big and sparse.",
                "distance": 0.0318
            },
            {
                "reference": "Collaborative Filtering (CF) is traditionally used to learn user and item embeddings from user-item interaction history for building recommender systems, but its performance suffers due to the sparseness of user behavior data. Social recommender systems have been proposed to utilize user's local neighbors' preferences to alleviate data sparsity but these models have been static, not simulating the recursive diffusion in the global social network.",
                "distance": 0.0318
            },
            {
                "reference": "Most Collaborative Filtering (CF) algorithms optimize using a dataset of isolated user-item tuples and are designed to recommend isolated items. Traditional CF approaches do not account for interactions between items in a list, the variation in click propensity, or other factors like item fatigue when items are recommended as a list.",
                "distance": 0.0343
            }
        ]
    },
    {
        "target": "Out-of-Distribution (OOD) detection is important for document understanding tasks, especially for large-scale pre-training models. However, most existing OOD detection methods focus on single-modal inputs such as images or texts, and the usage of multi-modal information in documents for OOD detection is underexplored.",
        "prediction": "Although several out-of-distribution (OOD) detection scores have been recently proposed for deep generative models, the direct use of the likelihood threshold for OOD detection has been found to be problematic.",
        "queries": [
            -4.862969,
            -19.434765
        ],
        "log": [
            {
                "reference": "Although several out-of-distribution (OOD) detection scores have been recently proposed for deep generative models, the direct use of the likelihood threshold for OOD detection has been found to be problematic.",
                "distance": 0.0022
            },
            {
                "reference": "As AI systems continue to proliferate, there's a necessity to implement efficient control mechanisms to ensure their safe and proper functioning. Reliable out-of-distribution (OOD) detection, which seeks to identify test samples that significantly deviate from the training distribution, serves as a critical control mechanism to prevent system failures.",
                "distance": 0.0032
            },
            {
                "reference": "Detecting out-of-distribution (OOD) instances is significant for the safe deployment of NLP models. Several prevalent methods for textual OOD detection, based on pretrained language models (PLMs), use a distance-based approach, by estimating sample distance scores in the last-layer CLS embedding space. However, these don't utilize the full spectrum of linguistic information in the PLMs.",
                "distance": 0.0053
            },
            {
                "reference": "Miller et al. established the 'accuracy-on-the-line' phenomenon where a model's in-distribution (ID) accuracy strongly correlates with its out-of-distribution (OOD) accuracy. Although helpful for model selection, it doesn't help estimate actual OOD performance without a labeled OOD validation set.",
                "distance": 0.0068
            },
            {
                "reference": "Existing Out-of-Distribution (OOD) detection methods typically rely on discriminative softmax metrics or energy methods and train with outlier exposure.",
                "distance": 0.0073
            },
            {
                "reference": "Detecting out-of-distribution (OOD) data has become a critical component in ensuring the safe deployment of machine learning models in the real world. Existing OOD detection approaches primarily rely on the output or feature space for deriving OOD scores and overlook information from the gradient space.",
                "distance": 0.0079
            },
            {
                "reference": "Unsupervised out-of-distribution (OOD) detection is essential for machine learning reliability. Studies have shown that hierarchical VAEs can help detect OOD instances, but they suffer from an inherent issue known as 'posterior collapse' which limits their performance.",
                "distance": 0.0092
            },
            {
                "reference": "Deep probabilistic generative models can model high dimensional data likelihoods and are expected to play a key role in out-of-distribution (OOD) detection. However, a problematic issue is observed when certain types of OOD samples are assigned higher likelihoods by these models, causing likelihood threshold-based OOD detection rules to fail.",
                "distance": 0.0098
            },
            {
                "reference": "The problem of recognizing out-of-distribution (OOD) inputs, inputs unrelated to the in-distribution task, is a crucial part of trustworthy machine learning. Numerous OOD detection methods have been proposed in recent years.",
                "distance": 0.0105
            },
            {
                "reference": "Detection of out-of-distribution (OOD) data using dropout Bayesian neural networks (BNNs) has been attempted before, but previous methods leveraging randomized embeddings can fail due to the distance metric used.",
                "distance": 0.0106
            },
            {
                "reference": "Out-of-distribution (OOD) detection is a crucial aspect when deploying machine learning models. Recently, many methods are using auxiliary outlier data to regularize models for better OOD detection. These approaches assume that the auxiliary outlier data is completely separable from the in-distribution data. ",
                "distance": 0.0117
            },
            {
                "reference": "The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. Existing methods often fail to adequately differentiate between in-distribution (ID) and OOD features, which is a critical aspect in ensuring the robustness and reliability of machine learning models.",
                "distance": 0.0122
            },
            {
                "reference": "Out-of-distribution (OOD) detection and lossless compression are two problems that can be solved by training probabilistic models on a first dataset with subsequent likelihood evaluation on a second dataset, where data distributions differ.",
                "distance": 0.0131
            },
            {
                "reference": "Out-of-distribution (OOD) detection is a critical aspect of deploying machine learning models in the real world, where data from shifted distributions can occur. Despite numerous algorithmic approaches emerging for OOD detection, there is a lack of theoretical understanding in this area.",
                "distance": 0.0133
            },
            {
                "reference": "Out-of-distribution (OOD) detection is a realistic setting studied by researchers where test data may come from unknown classes during training, and good generalization is crucial due to the unavailability and diversity of OOD data. The probably approximately correct (PAC) learning theory of OOD detection was proposed as an open problem.",
                "distance": 0.0134
            },
            {
                "reference": "Out-of-distribution (OOD) detection attempts to discern outliers from the intended data distribution, which is vital for ensuring high reliability and a good user experience. Most recent techniques in OOD detection use information from a single representation located in the penultimate layer to determine if an input is anomalous.",
                "distance": 0.0144
            },
            {
                "reference": "Recognizing out-of-distribution (OOD) samples is critical for machine learning systems, but most OOD detection methods are based on a single modality (either vision or language), not utilizing the rich information in multi-modal representations.",
                "distance": 0.0156
            },
            {
                "reference": "Out-of-distribution (OOD) detection benchmarks usually define one dataset as in-distribution (ID) and all others as OOD, setting impractical goals such as perfect distinction between very similar images from different datasets, which limits the usefulness of these models in real world settings.",
                "distance": 0.0157
            },
            {
                "reference": "Out-of-distribution (OOD) detection is critical for ensuring the reliability and safety of deep neural networks in real-world scenarios. Most previous OOD detection methods focus on designing OOD scores or introducing diverse outlier examples to retrain the model.",
                "distance": 0.0158
            },
            {
                "reference": "Detecting out-of-distribution (OOD) objects is critical for safely deploying object detectors in the wild, but distance-based OOD detection methods remain largely unexplored at the object level.",
                "distance": 0.0167
            }
        ]
    },
    {
        "target": "Generative artificial intelligence (AI) systems, which can produce new and creative content including images, texts, music, video, code, and other forms of design, are emerging. However, understanding how humans interact and make sense of these systems or outcomes, control them ethically, and collaborate with them creatively is not yet clear.",
        "prediction": "Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with humans in co-creating novel content. However, less research has been done on the role AI can play in human-human collaborations during co-creation.",
        "queries": [
            -10.710335,
            11.62228
        ],
        "log": [
            {
                "reference": "Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with humans in co-creating novel content. However, less research has been done on the role AI can play in human-human collaborations during co-creation.",
                "distance": 0.0095
            },
            {
                "reference": "Artificial Intelligence (AI) plays an important role in improving Human-computer interaction (HCI) and user experience (UX), but many challenges persist in designing and innovating human-AI interactions.",
                "distance": 0.0117
            },
            {
                "reference": "In the field of Human-Computer Interaction (HCI), artificial intelligence (AI) has been explored as a design material. Recent studies suggest that enterprise applications present opportunities for AI innovation to enhance user experience. There are questions about how designers of enterprise applications, working in cross-functional AI teams, engage with AI.",
                "distance": 0.0313
            },
            {
                "reference": "Artificial intelligence (AI) and Human Computer Interaction (HCI) have begun to diverge after sharing common roots, with modern AI techniques beginning to significantly impact areas outside of core-AI, including new ways for machines and humans to interact.",
                "distance": 0.0474
            },
            {
                "reference": "Vernor Vinge's proposition indicates the potential significance and profound nature of network and interface research in Artificial Intelligence.",
                "distance": 0.0578
            },
            {
                "reference": "AI-empowered co-creative tools are used in various forms in the present but their involvement in various stages of the creative process and how they address common challenges in human-AI interaction (HAII) are not well understood.",
                "distance": 0.0648
            },
            {
                "reference": "The growth of AI techniques has led to improvements in understanding and using implicit human inputs to enhance future AI-infused systems. However, these capabilities have also instigated new interactions with AI, necessitating Human-Computer Interaction (HCI) techniques to improve AI usage through design and evaluation.",
                "distance": 0.0719
            },
            {
                "reference": "Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades. However, advances in AI and the growing use of AI technologies in human-facing applications necessitate further study and innovation.",
                "distance": 0.0895
            },
            {
                "reference": "While the opportunities for AI and machine learning (ML) are vast in current interactive system development, there's limited understanding about how the system behind the interface is designed, and how design methodologies such as user-centered design have influence.",
                "distance": 0.098
            }
        ]
    },
    {
        "target": "Quantitatively profiling a scholar's scientific impact is important, and current practices with bibliometric indicators, lists, and networks perform well at scholar ranking but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding.",
        "prediction": "The need for a method to automatically link references in online scholarly literature was the motivation for this study, with the existing tools lacking in effectiveness.",
        "queries": [
            -8.313098,
            3.73878
        ],
        "log": [
            {
                "reference": "The need for a method to automatically link references in online scholarly literature was the motivation for this study, with the existing tools lacking in effectiveness.",
                "distance": 0.0186
            },
            {
                "reference": "Assessing academic productivity based on publication outputs is a challenge. Citation-based metrics, despite their wide adoption, can be improved upon.",
                "distance": 0.0191
            },
            {
                "reference": "The escalation in the number of research papers necessitates effective article-level metrics to help researchers select papers. Traditional metrics, however, typically rely on a single factor, which limits their ability to accurately assess papers over various periods after publication. Furthermore, the advent of web 2.0 introduces new factors to paper assessment.",
                "distance": 0.0215
            },
            {
                "reference": "The challenging process of tracing back data, facts, and citations used in documents to their original sources is a recognized problem, which leaves open questions about the legitimacy, date, origin, and collection methods of these sources.",
                "distance": 0.0243
            },
            {
                "reference": "Citation analysis is currently used to evaluate the impact of scientific collections, publications, and scholars. Some awards, like the 'VLDB 10 Year Award' and 'SIGMOD Test of Time Award', are granted based on assessments in top database conferences.",
                "distance": 0.0267
            },
            {
                "reference": "Research papers are increasingly being published on the Internet in various forms and mediums. Ties between these papers can be traced through citations; however, finding the relationship between journals published on the Internet requires a system that can automatically produce a relationship between articles from different journals hosted on different websites.",
                "distance": 0.0289
            },
            {
                "reference": "Citations serve as an important metric for identifying experts and opinion leaders in academic communities, yet understanding the dynamics of citation rankings remain a challenge.",
                "distance": 0.0289
            },
            {
                "reference": "Evaluating a scientist's impact is critical in decisions related to recruitment and funding, and it is increasingly linked to publication citation counts. Identifying valuable work with great potential before they are widely recognized and become highly cited papers is beneficial for both readers and authors.",
                "distance": 0.0323
            },
            {
                "reference": "Evaluating the impact of scholarly papers is crucial for various purposes including recruitment decisions, funding allocation, and promotions. However, the influence of geographic distance on the impact of scholarly papers has not been thoroughly studied.",
                "distance": 0.0344
            },
            {
                "reference": "When developing grant proposals for funding agencies like NIH or NSF, it is crucial to determine whether a research topic is gaining momentum. However, prior to this study, there has not been a method to quantify the momentum of research topics.",
                "distance": 0.036
            },
            {
                "reference": "Writing and sourcing relevant bibliographic information from various online sources can be challenging for authors while writing a document.",
                "distance": 0.0362
            },
            {
                "reference": "The authors stress the importance of self-knowledge in a scientific community for its evolution and maturity. They note that the citation profile of a scientific event can reflect its quality, research areas, knowledge niches, and collaboration profiles.",
                "distance": 0.0366
            },
            {
                "reference": "Academic publication is a crucial indicator of a scholar's scientific productivity and can impact their future career. Previous studies identified a positive association between the number of collaborators and academic productivity. However, existing approaches for tracking and predicting potential collaborators are less effective for junior scholars due to their insufficient publication record.",
                "distance": 0.0367
            },
            {
                "reference": "Scholarly entities such as articles, journals, authors, and institutions are primarily ranked based on expert opinion and citation data.",
                "distance": 0.044
            },
            {
                "reference": "The proliferation of scientific literature in digital libraries and social media along with the complexity of citation relationships existing in papers have brought challenges to the management, analysis and extraction of knowledge from scientific literature. Current data mining techniques utilized for scientific literature analysis lack flexibility and smartness, and typically require expert input and guidance.",
                "distance": 0.0485
            },
            {
                "reference": "Understanding the qualitative patterns of research endeavor of scientific authors in terms of publication count and their impact (citation) has been a complex problem.",
                "distance": 0.0489
            },
            {
                "reference": "Citation is a key activity in the scientific community and efficient flexible querying methods are essential for researchers to effectively follow trends within related topics in their research fields. Academic search engines such as Google Scholar and Microsoft Academic Search are common tools utilized for this purpose.",
                "distance": 0.0516
            },
            {
                "reference": "Many measures and metrics have been proposed to provide indications over researchers' academic productions. However, most of these measures do not consider the collaborations among researchers and the impact that each co-author has on the scientific production of another, failing to distinguish co-authors from their pure publication history.",
                "distance": 0.0519
            },
            {
                "reference": "Predicting high impact academic papers is beneficial to a variety of stakeholders, but current methods are not listed or stated to be insufficient.",
                "distance": 0.0607
            },
            {
                "reference": "The growing volume of academic publications has made it challenging for researchers to effectively identify the most crucial prior works related to their research topic.",
                "distance": 0.0655
            }
        ]
    },
    {
        "target": "Redirected walking (RDW) is a VR technology which allows users to move in a large virtual space within a limited real space but suffers from visual-vestibular inconsistencies, disrupting seamless virtual reality experience.",
        "prediction": "New virtual reality (VR) experiences allow users to walk extensively in virtual spaces with solutions like larger tracking spaces, treadmills and redirected walking, but the connection between the user's movement and locomotion feedback is often neglected in VR setups.",
        "queries": [
            -19.054195,
            17.194361
        ],
        "log": [
            {
                "reference": "New virtual reality (VR) experiences allow users to walk extensively in virtual spaces with solutions like larger tracking spaces, treadmills and redirected walking, but the connection between the user's movement and locomotion feedback is often neglected in VR setups.",
                "distance": 0.0145
            },
            {
                "reference": "Locomotion in Virtual Reality (VR) poses a challenge due to the mismatch between the size of a Virtual Environment and the physically available tracking space. Current research in VR locomotion techniques have not reached a conclusion as yet.",
                "distance": 0.02
            },
            {
                "reference": "Professionals in domains like film, theater, or architecture often use physical models to visualize spaces. With virtual reality (VR), new tools provide immersive experiences with corrected perceptions of depth and scale. Yet, these VR tools lack the tangibility of physical models, which is a gap that using tangible objects in VR could potentially fill. However, this introduces the challenges of producing suitable objects and interacting with them when only the virtual ones are visible.",
                "distance": 0.0212
            },
            {
                "reference": "Redirected walking (RDW) visually manipulates the virtual environment (VE) such that the real environment (RE) and VE are not matched 1:1. However, when the intensity of visual manipulations increases, people notice the RDW manipulation due to visual-vestibular inconsistency, leading to discomfort, such as motion sickness.",
                "distance": 0.0218
            },
            {
                "reference": "Previous virtual reality (VR) locomotion techniques aim to mitigate the spatial limitation of tracking. However, these 'unrealistic' methods like teleportation decrease the potential benefits of more realistic techniques such as walking.",
                "distance": 0.0253
            },
            {
                "reference": "The use of vection, the illusion of self-movement, has recently been explored as a novel way to immerse observers in mediated environments without physically moving. This provides advantages over existing systems that employ costly, cumbersome, and potentially hazardous motion platforms, which are often surprisingly inadequate to provide life-like motion experiences.",
                "distance": 0.026
            },
            {
                "reference": "Physical and virtual dimensions in Virtual Reality (VR) often differ, rendering traditional locomotion methods unfeasible. Existing artificial locomotion solutions such as teleportation, treadmills, and redirected walking either require the user's hands, complex hardware, or large physical spaces.",
                "distance": 0.0339
            },
            {
                "reference": "Today's virtual reality (VR) systems use chaperone rendering techniques to prevent user-object collisions. However, these techniques are limited in their ability to provide advanced compositing between the real and virtual world due to the lack of a detailed geometric model of the real world.",
                "distance": 0.0365
            },
            {
                "reference": "Current locomotion user interfaces (LUIs) for seated virtual reality (VR) experiences primarily rely on established techniques such as teleportation and joystick-based navigation.",
                "distance": 0.0373
            },
            {
                "reference": "Natural locomotion in room-scale virtual reality is constrained by the user's immediate physical space. Researchers have used the impossible space design mechanic to overcome these constraints.",
                "distance": 0.0375
            },
            {
                "reference": "Locomotion in virtual reality (VR) is one of the biggest problems for large scale adoption of VR applications. There is a lack of studies conducted in-the-wild to understand performance metrics and general user preference for different mechanics.",
                "distance": 0.0405
            },
            {
                "reference": "Virtual reality (VR) is excellent in showing spatial relationships, such as viewing medical 3D data. However, the utilization of tangible controllers to manipulate these data hasn't been explored extensively.",
                "distance": 0.041
            },
            {
                "reference": "The Virtual Reality (VR) user interface style allows the manipulation of virtual objects in a 3D environment using 3D input devices, and is best suited to application areas where traditional two dimensional styles are insufficient. However, current programming efforts required to produce a VR application are substantially large.",
                "distance": 0.0484
            },
            {
                "reference": "Increased levels of interactivity and multi-sensory stimulation can enhance the immersion of Virtual Reality experiences but often suffer from limitations such as motion artifacts.",
                "distance": 0.0509
            },
            {
                "reference": "Virtual reality (VR) experiences currently require dedicated infrastructure like infrared cameras for tracking, or they provide a limited 3 Degrees of Freedom (DoF) tracking which compromises the user experience.",
                "distance": 0.0524
            },
            {
                "reference": "In VR, the user interacts with the virtual world through a virtual avatar. Existing physics engines in VR do not consider the direction in which the user perceives a collision, leading to inaccuracies and imprecision in applications such as VR sports games.",
                "distance": 0.0539
            },
            {
                "reference": "Virtual Reality has created a demand for realistic and cheap reconstruction of real-life products. This has raised challenges surrounding the use of consumer-grade scanning devices which often result in objects with seams and misaligned textures.",
                "distance": 0.0545
            },
            {
                "reference": "Interacting with out-of-reach or occluded Virtual Reality (VR) objects can be challenging. Users can change their position and orientation, such as via teleporting, to observe and select, but doing so frequently may cause loss of spatial orientation or motion sickness.",
                "distance": 0.0558
            },
            {
                "reference": "The challenge of natural navigation in VR exists due to spatial limitations. Current solutions either involve teleportation (which requires minimal space and doesn't cause motion sickness, but reduces presence and spatial awareness) or redirected walking (RDW) which lets users walk within a finite large physical space.",
                "distance": 0.0567
            },
            {
                "reference": "Room-scale virtual reality games allow natural navigation through physical walking, but physical tracking space is limited. Current solutions for traversing larger distances in virtual environments, such as teleportation, can be exhausting or potentially reduce presence.",
                "distance": 0.0634
            }
        ]
    },
    {
        "target": "Planning and organizing user studies can be a challenge for novices and student researchers in human-computer interaction (HCI).",
        "prediction": "The authors observe a need for clearer and more reliable ways of writing about experiments and statistical practices in HCI.",
        "queries": [
            -16.907316,
            10.835048
        ],
        "log": [
            {
                "reference": "The authors observe a need for clearer and more reliable ways of writing about experiments and statistical practices in HCI.",
                "distance": 0.0779
            },
            {
                "reference": "The existing methods for continuing education in HCI lack comprehensive learning support systems.",
                "distance": 0.0827
            },
            {
                "reference": "When designing an HCI experiment, planning the sample size with a priori power analysis is often skipped due to the lack of reference effect sizes. This can lead to a false-negative result or overspending of resources.",
                "distance": 0.0875
            },
            {
                "reference": "Research on human factors in computer systems has mostly focused on supporting individuals, rather than groups or whole organizations.",
                "distance": 0.0889
            },
            {
                "reference": "The Engineering Psychology program at the University of Illinois is an interdisciplinary entity consisting of faculty members, undergraduate, graduate, and post-doctoral students from multiple departments.",
                "distance": 0.095
            },
            {
                "reference": "Experiment design is challenging for both novice and advanced HCI researchers.",
                "distance": 0.0955
            }
        ]
    },
    {
        "target": "Answering complex queries on knowledge graphs is critical yet challenging due to data incompleteness. Previous algorithms dealing with this issue focus on specific forms of embeddings and simulate logical reasoning with set operators, but the scoring functions between these embeddings remain largely unexplored. Existing scoring functions are either based on local comparison or global transport.",
        "prediction": "Entities in most knowledge graphs (KGs) lack succinct noun compound descriptions, and existing methods to generate such descriptions either overlook grammatical structure or make factual errors in the generated texts.",
        "queries": [
            -6.449115,
            -16.482445
        ],
        "log": [
            {
                "reference": "Entities in most knowledge graphs (KGs) lack succinct noun compound descriptions, and existing methods to generate such descriptions either overlook grammatical structure or make factual errors in the generated texts.",
                "distance": 0.0402
            },
            {
                "reference": "The surge in multi-modal knowledge graphs necessitates multi-modal entity alignment techniques for integrating disparate MMKGs. Existing methods typically resort to heuristic merging of uni-modal feature embeddings, which might miss inter-modal nuances present in multi-modal knowledge.",
                "distance": 0.0458
            },
            {
                "reference": "Most prior work in knowledge base (KB) completion has focused on the task of relation extraction. Inferring missing entity type instances in a KB, a critical task for completion, has received little attention.",
                "distance": 0.0523
            },
            {
                "reference": "Textual corpora, such as financial documents, contain a wealth of knowledge. Knowledge graphs have recently become a popular approach for capturing structured knowledge of entities and their relationships.",
                "distance": 0.0533
            },
            {
                "reference": "Querying knowledge graphs (KGs) to explore entities and discover facts can be difficult and tedious, even for users with substantial programming or data analytics skills.",
                "distance": 0.0552
            },
            {
                "reference": "Current knowledge bases (KBs) are usually incomplete and over-specified, failing to provide some queries that have real-world answers. Query embedding techniques have been proposed to represent KB entities and KB queries jointly in an embedding space to support inference.",
                "distance": 0.0562
            },
            {
                "reference": "Answering complex logical queries on incomplete knowledge graphs (KGs) is a challenging task due to missing edges. Existing query embedding methods use a single, concentrated query representation to retrieve answers, which is not suitable for scenarios where the embeddings of the answer entities may not always follow a uni-modal distribution in the embedding space.",
                "distance": 0.0592
            },
            {
                "reference": "Path queries on a knowledge graph are used to answer compositional questions. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces, but they suffer from cascading errors when applied recursively to answer path queries.",
                "distance": 0.0592
            },
            {
                "reference": "Knowledge graph inference has been approached by traditional logical rule reasoning and the recent knowledge graph embedding (KGE) methods. Attempts to combine KGE and logical rules have resulted in treating rules as additional KGE constraints or approximating exact logical inference using probabilistic models, but these approaches have scalability issues due to the intractability of the total number of ground rules.",
                "distance": 0.0603
            },
            {
                "reference": "Human-curated knowledge graphs are incomplete, necessitating their auto-completion. Prevalent approaches like graph embeddings and textual encoding have drawbacks: embeddings are hardly generalizable to elements not visited in training and are vulnerable to graph incompleteness, while textual encoding has high overheads due to costly scoring and lacks structured knowledge.",
                "distance": 0.0634
            },
            {
                "reference": "Approaches based on Knowledge Graph (KG) embeddings have been used for reasoning over incomplete KGs but struggle to handle the rising focus on complex logical queries. These queries comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities.",
                "distance": 0.0637
            },
            {
                "reference": "The automatic construction of knowledge graphs (KGs) from unstructured text has resulted in KGs with millions of entities (nodes) and facts (edges). These KGs are sparse in terms of facts known for a given entity, making it difficult to use in real-world applications.",
                "distance": 0.0665
            },
            {
                "reference": "Existing multi-hop reasoning approaches over knowledge graphs infer missing relationships between entities with a multi-hop rule, which corresponds to a chain of relationships.",
                "distance": 0.0681
            },
            {
                "reference": "Existing Query Embedding (QE) methods, which aim to embed entities and first-order logical queries in a vector space for answering queries on knowledge graphs, handle complex queries by dividing them into mini-queries and perform logical operations on the answer sets. However, these methods usually assume that answer sets satisfy certain individual distributions, which is often inaccurate in real applications and limits their performance.",
                "distance": 0.0682
            },
            {
                "reference": "Knowledge graphs encapsulate entities and relationships, but the vast amount of information describing an entity could overwhelm users. This has stimulated research on entity summarization, where compact summaries for entities are automatically generated. Various techniques ranging from ranking and mining to machine and deep learning have been proposed, but the continuous improvements have made it challenging for newcomers and the community to stay current.",
                "distance": 0.0715
            },
            {
                "reference": "Existing multi-hop reasoning approaches in graph walking work well on short reasoning paths but tend to miss the target entity with the increasing path length, which is a challenge when short paths connecting source and target entities are not available in incomplete Key Graphs (KGs).",
                "distance": 0.072
            },
            {
                "reference": "Knowledge graph (KG) entity typing, which aims at inferring possible missing entity type instances in KG, is a significant but still under-explored subtask of knowledge graph completion.",
                "distance": 0.0741
            },
            {
                "reference": "Quantum Embedding (QE), which allows for Boolean logic style deductive reasoning directly over embedding vectors, was previously proposed for knowledge base embeddings. However, the original QE is limited to the transductive setting and is slow for real applications involving millions of entities.",
                "distance": 0.0762
            },
            {
                "reference": "Knowledge graphs, while popular, often suffer from incompleteness due to multiple semantic issues where a relation or an entity can have multiple meanings. This makes multi-hop reasoning, a common method for knowledge graph completion, challenging.",
                "distance": 0.0778
            },
            {
                "reference": "Large knowledge graphs require reasoning for development, especially for completion to infer new triples based on existing ones. Both rules and embeddings can be used for knowledge graph reasoning, each with their own strengths and limitations. Rule-based reasoning is accurate and explainable but suffers from efficiency issues due to huge search space, whereas embedding-based reasoning is scalable and efficient, but struggles with learning good representations for sparse entities due to heavy reliance on data richness.",
                "distance": 0.078
            }
        ]
    },
    {
        "target": "Existing fact-checking systems estimate truthfulness using numerical scores, leading to a lack of human interpretation. Despite common practices of fact-checking websites to categorize facts into different logical aspects, contemporary systems miss out on providing aspect-based explainability that can assist human fact-checkers in verifying facts.",
        "prediction": "Automatic fact-checking systems are used to detect misinformation by selecting check-worthy sentences for fact checking and inferring their factuality. Past research mainly uses hand-crafted features to select check-worthy sentences and often ignores the fact that top-weighted terms often overlap between check-worthy and non-check-worthy sentences.",
        "queries": [
            -23.247513,
            3.031616
        ],
        "log": [
            {
                "reference": "Automatic fact-checking systems are used to detect misinformation by selecting check-worthy sentences for fact checking and inferring their factuality. Past research mainly uses hand-crafted features to select check-worthy sentences and often ignores the fact that top-weighted terms often overlap between check-worthy and non-check-worthy sentences.",
                "distance": 0.0055
            },
            {
                "reference": "While there's significant interest in developing general purpose fact-checking models, constructing a large-scale fact verification dataset with realistic real-world claims is challenging due to biases introduced by crowd-workers or expensive verification by professional fact-checkers.",
                "distance": 0.0141
            },
            {
                "reference": "Fact-checking is an important part of journalism to fight misinformation, and it is often structured around certain factors including the claim, claimant, and verdict. Recently, the ClaimReview schema was introduced to standardize this process but annotating fact-checks is a tedious process and less than half of the fact-checkers worldwide had adopted it by mid-2019.",
                "distance": 0.0272
            },
            {
                "reference": "Verifying complex political claims is a challenging task, and current automatic fact-checking systems have limitations in determining the veracity of such claims. Their predictions like 'half-true' are not very informative by themselves since we have no clarity on which parts of the claim are true and which are not.",
                "distance": 0.0285
            },
            {
                "reference": "Most existing automated fact checking systems focus on predicting the veracity of claims based on various factors, such as metadata, social network spread, and language used in claims. However, these systems lack the ability to generate justifications for verdicts on claims, which is a crucial part of the fact-checking process.",
                "distance": 0.0302
            },
            {
                "reference": "Claim check-worthiness detection is the first step of automatic fact checking and has been studied in different contexts like political speeches, Twitter rumors, and in Wikipedia. However, there hasn't been any structured comparison of these tasks to understand their relatedness or a unified approach addressing them.",
                "distance": 0.0385
            },
            {
                "reference": "The rise of false information online has been addressed by manual fact-checking and automatic fact-checking methods. However, manual fact-checking is slow and non-scalable while automatic fact-checking lacks explainability and credibility. Recently, the approach of checking whether a given claim has been previously fact-checked has emerged.",
                "distance": 0.0458
            },
            {
                "reference": "Fact-checking is a task requiring considerable effort, and there's a need for real-world data sets that can effectively facilitate natural language processing tasks in this context.",
                "distance": 0.0506
            },
            {
                "reference": "Manually or programmatically verifying a scientific claim by providing supporting or refuting evidence rationales is challenging even for domain experts. This is exacerbated by the proliferation of misinformation on social media or news websites, highlighting the crucial need for an automated fact-verification tool.",
                "distance": 0.0507
            },
            {
                "reference": "Fact-checking in news organizations is an important yet time-consuming and costly process. A model that allows crowdsourced fact-checking with professional oversight and automation is yet to be realized.",
                "distance": 0.0539
            },
            {
                "reference": "Traditional fact checking methods cannot keep pace with the modern influx of information, necessitating an approach that can computationally verify information.",
                "distance": 0.058
            },
            {
                "reference": "Automated fact-checking on a large scale is difficult due to the noisy document collections like the web or news articles, and comprehensive studies on this task have been scarce.",
                "distance": 0.0679
            },
            {
                "reference": "Fact verification has gained significant attention as a method for tackling misinformation. Current benchmarks focus mostly on textual sources (unstructured information) and have largely ignored structured information, such as tables.",
                "distance": 0.071
            },
            {
                "reference": "The increasing concern with misinformation has stimulated research efforts on automatic fact checking. The FEVER dataset introduced a benchmark fact-verification task, where a system is asked to verify a claim using evidential sentences from Wikipedia documents.",
                "distance": 0.0855
            },
            {
                "reference": "Existing fact checking datasets do not introduce the complexity of requiring models to extract evidence from multiple hops and sources.",
                "distance": 0.0964
            }
        ]
    },
    {
        "target": "The main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy, but this approach is rarely applicable for low-resource languages where traditional handcrafted features are still widely used.",
        "prediction": "The current state of album storytelling models struggle to fully exploit the structure information of the photos within an album and might struggle to generate a coherent narration.",
        "queries": [
            -4.651742,
            9.835334
        ],
        "log": [
            {
                "reference": "The current state of album storytelling models struggle to fully exploit the structure information of the photos within an album and might struggle to generate a coherent narration.",
                "distance": 0.0195
            },
            {
                "reference": "The story rewriting task involves pupils rewriting an exemplar story in their own words. This allows them to practice language skills without worrying about content creation. There exists the problem of the pupil improperly recalling the story.",
                "distance": 0.0329
            },
            {
                "reference": "Existing methods for visual storytelling, which involve generating a story from an image stream, tend to represent images directly with the extracted high-level features. These representations are not intuitive and are often difficult to interpret.",
                "distance": 0.0367
            },
            {
                "reference": "Story visualization is a field focused on generating image sequences to narrate multi-sentence stories. The challenge is to preserve global consistency of characters and scenes across different story images, a task made difficult by the sparse signals provided for generating images by story sentences.",
                "distance": 0.0417
            },
            {
                "reference": "Existing computational systems lack sufficient understanding of human actions in stories due to limitations in their knowledge about these actions.",
                "distance": 0.0455
            },
            {
                "reference": "Visual storytelling is the task of generating a short story to describe an ordered image stream. Existing approaches largely fail to generate coherent, diverse, and informative stories due to lack of proper reasoning and absence of imaginative concepts.",
                "distance": 0.0506
            },
            {
                "reference": "Story visualization aims to generate a sequence of images to narrate each sentence in a multi-sentence story, but current works face the problem of semantic misalignment due to their fixed architecture and the diversity of input modalities.",
                "distance": 0.0599
            },
            {
                "reference": "Existing generative models for the task of story ending generation, which aims at generating a reasonable and coherent ending for a given story context, have not adequately captured the hidden logic information, which is essential for comprehending the context sufficiently.",
                "distance": 0.0642
            },
            {
                "reference": "Story visualization is a complex task requiring machines to understand long text inputs and produce a globally consistent image sequence that illustrates the contents of the story. Preserving characters within these stories is a challenging aspect of consistent story visualization.",
                "distance": 0.0657
            },
            {
                "reference": "Image narrative generation involves creating a story from an image with a subjective viewpoint. It is important for methods to consider human emotion due to its significance in storytelling.",
                "distance": 0.0724
            },
            {
                "reference": "Current visual storytelling methodologies involve generating relevant and interesting stories for given image sequences, but lack diversity and ample informative content.",
                "distance": 0.0771
            },
            {
                "reference": "Story visualization, where given a series of natural language captions composing a story, an agent generates a corresponding sequence of images, is an under-explored task. Prior work has introduced recurrent generative models that outperform text-to-image synthesis models but the resultant images could be enhanced in terms of visual quality, coherence and relevance.",
                "distance": 0.0884
            },
            {
                "reference": "The task of generating a reasonable ending for a given story context, which is a strong indication of story comprehension, has been a challenge, requiring understanding of context clues and handling of implicit knowledge to build a coherent narrative.",
                "distance": 0.0892
            },
            {
                "reference": "STRIPS-style planning has been adopted for narrative generation; however, traditional assumptions about its application, including the importance of the initial and goal states and the sequence of actions in the context of a story, may hinder the creation of engaging narratives.",
                "distance": 0.0943
            },
            {
                "reference": "The existing computational models for Interactive Narrative may not be achieving the full potential of narrative interactivity.",
                "distance": 0.0958
            },
            {
                "reference": "Most visual storytelling models produce stories with fixed lengths, typically five sentences, due to the domination of such length stories in the training data. Such fixed-length stories often carry limited details and provide ambiguous textual information to the readers.",
                "distance": 0.0985
            }
        ]
    },
    {
        "target": "Automatic code generation tools, such as Copilot, are becoming popular. These tools present potential hazards tied to social biases, particularly at the stage of code generation models validation.",
        "prediction": "Github Copilot is trained on billions of lines of public code, but its usage has raised ethical and security concerns regarding the use of copyleft licensed code or insecure code for training. These problems pose impacts on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models.",
        "queries": [
            -3.582353,
            13.764012
        ],
        "log": [
            {
                "reference": "Github Copilot is trained on billions of lines of public code, but its usage has raised ethical and security concerns regarding the use of copyleft licensed code or insecure code for training. These problems pose impacts on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models.",
                "distance": 0.0619
            },
            {
                "reference": "Existing natural language processing (NLP) models are known to inherit social biases that exist in human-generated corpora used for training. These biases, particularly gender biases, can lead to discriminative results in downstream tasks.",
                "distance": 0.1207
            }
        ]
    },
    {
        "target": "Current state-of-the-art systems for dialogue state tracking use a sequence-to-sequence approach that takes the full dialogue history as input and generates the entire state from scratch at each dialogue turn. This approach becomes inefficient especially when the number of slots is large and the conversation is long.",
        "prediction": "In the field of natural language understanding, user intent extraction from spoken or written language is typically modeled as a classification task assigning single intent labels to utterances based on a pre-established set of categories.",
        "queries": [
            -6.501219,
            13.781882
        ],
        "log": [
            {
                "reference": "In the field of natural language understanding, user intent extraction from spoken or written language is typically modeled as a classification task assigning single intent labels to utterances based on a pre-established set of categories.",
                "distance": 0.0152
            },
            {
                "reference": "Current multi-domain dialogue state trackers are only about 55% accurate, often mistakenly placing full confidence in an incorrect dialogue state. Belief trackers, which maintain a distribution over possible dialogue states, aren't as accurate as dialogue state trackers and their distributions aren't well calibrated.",
                "distance": 0.0198
            },
            {
                "reference": "Existing reinforcement learning methods for dialog policy learning involve the training of a centralized agent selecting a predefined joint action combining domain name, intent type, and slot name. However, this centralized agent approach is costly in terms of user-agent interactions and designing concatenated actions can be laborious and struggle with edge cases.",
                "distance": 0.0268
            },
            {
                "reference": "Dialogue state tracking has expanded from single to multiple domains, characterized by shared slots between domains. As scenarios get more complex, the out-of-vocabulary problem worsens. Current models struggle with ontology integration between domains and out-of-vocabulary issues.",
                "distance": 0.0331
            },
            {
                "reference": "Training the Dialog Management module in task-oriented dialog systems using Reinforcement Learning (RL) suffers from low sample efficiency and slow convergence speed due to sparse rewards in RL. Existing strategies to provide rewards for RL training lack interpretability and cannot accurately estimate the distribution of state-action pairs in real dialogs.",
                "distance": 0.037
            },
            {
                "reference": "Existing models in dialogue state tracking use a consistent dialogue history throughout the state tracking, regardless of which slot is being updated. This approach often provides either insufficient or redundant information, negatively affecting overall performance.",
                "distance": 0.0473
            },
            {
                "reference": "Frame-based state representation is used to model user intentions and slot values in modern task-oriented dialog systems, but a fixed domain ontology design makes it hard to extend to new services. Recently, the use of natural language descriptions to define domain ontology has been proposed, offering a dynamic set of schema.",
                "distance": 0.0596
            },
            {
                "reference": "Reinforcement learning methods are popular for training effective dialogue policy, but they struggle with sparse and unstable reward signals returned by a user simulator only when the dialogue finishes. Adversarial learning methods have been proposed to learn the reward function and the dialogue policy, but they have limitations and can often result in local optima or mode collapse.",
                "distance": 0.094
            },
            {
                "reference": "Designing dialog policies for voice-enabled interfaces often requires the expertise of a natural language processing experts and usually needs to be done for every new dialog task as cross-domain transfer is not possible. Traditional reinforcement learning (RL) methods for dialog policy optimization require the assessment of more or less random changes in the policy on users, known as on-policy learning, which can result in unacceptable system behaviors.",
                "distance": 0.0944
            }
        ]
    },
    {
        "target": "Language models often generate false, toxic, or irrelevant outputs. The concept of reinforcement learning from human feedback, where human preference judgments on language model outputs are transformed into a learning signal, has been used to address these issues, but it provides limited information on long text outputs.",
        "prediction": "There has been growing interest in using human feedback during robot operation to incorporate non-expert human expertise while learning complex tasks, primarily using reinforcement learning where human feedback is converted into rewards through different modalities such as speech, graphical interfaces, gestures.",
        "queries": [
            8.72553,
            -23.821539
        ],
        "log": [
            {
                "reference": "There has been growing interest in using human feedback during robot operation to incorporate non-expert human expertise while learning complex tasks, primarily using reinforcement learning where human feedback is converted into rewards through different modalities such as speech, graphical interfaces, gestures.",
                "distance": 0.0708
            },
            {
                "reference": "Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a stable alternative, it requires access to demonstrations from a human supervisor.",
                "distance": 0.117
            }
        ]
    },
    {
        "target": "Multilingual pre-trained language models have shown strong cross-lingual transfer abilities, but performance can drop when the target language is greatly different from source languages or when there is limited pre-training data.",
        "prediction": "Numerous modern NLP models, trained with resource-rich English language datasets, have achieved high performance in various tasks. The idea of applying such learning to low-resource languages via zero-shot or few-shot cross-lingual transfer has gained attention. However, most efforts to achieve this involve the use of parallel corpora for sentence alignment training.",
        "queries": [
            -0.449544,
            14.02135
        ],
        "log": [
            {
                "reference": "Numerous modern NLP models, trained with resource-rich English language datasets, have achieved high performance in various tasks. The idea of applying such learning to low-resource languages via zero-shot or few-shot cross-lingual transfer has gained attention. However, most efforts to achieve this involve the use of parallel corpora for sentence alignment training.",
                "distance": 0.0016
            },
            {
                "reference": "The introduction of pretrained cross-lingual language models has significantly improved multilingual NLP tasks. Nonetheless, the lack of labelled task data still necessitates a variety of methods aiming to equalize performances between resource-rich and resource-poor languages, such as zero-shot methods using translated task data.",
                "distance": 0.0192
            },
            {
                "reference": "Massively multilingual models that operate over a broad range of languages pose significant challenges to multi-task optimization. The common practice involves a language-agnostic procedure optimizing a joint multilingual task objective, but how to improve optimization efficiency by utilizing the underlying problem structure has not been extensively explored.",
                "distance": 0.0282
            },
            {
                "reference": "Pre-trained multilingual language models underpin many NLP approaches, including cross-lingual transfer solutions. However, languages with small available monolingual corpora are often not well-supported by these models, leading to poor performance.",
                "distance": 0.0394
            },
            {
                "reference": "Pre-trained multilingual models are used for various tasks in low-resource languages via cross-lingual transfer. English is often seen as the most natural choice for a transfer language, but it's not always the best choice. The factors influencing the success of cross-lingual transfer are currently difficult to explain.",
                "distance": 0.0414
            },
            {
                "reference": "Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. However, quantitative analyses of their successes attributed to shared representations are limited.",
                "distance": 0.0457
            },
            {
                "reference": "Cross-lingual models trained on source language tasks possess the capability to transfer to target languages directly. However, due to word order variances in different languages, such models that overfit into the word order of the source language could have sub-optimal performance in the target languages.",
                "distance": 0.05
            },
            {
                "reference": "Multilingual models jointly pretrained on multiple languages have shown impressive performance on multilingual downstream tasks. However, these tasks need to align gradients between them to maximize knowledge transfer and minimize negative transfer. Despite its importance, current methods either are not suitable, neglect inter-task alignment, or solve continual learning problems inefficiently.",
                "distance": 0.0555
            },
            {
                "reference": "Existing work in multilingual pretraining relies on shared vocabulary and bilingual contexts to encourage correlation across languages, using a unified Transformer encoder. This approach is loose and implicit for aligning the contextual representations between languages.",
                "distance": 0.0574
            },
            {
                "reference": "Multilingual pre-trained language models have shown to perform well on cross-lingual downstream tasks, but it has remained unclear to what extent they learn language-neutral representations (representations that encode similar phenomena across languages), and how such representations affect the performance of cross-lingual transfer.",
                "distance": 0.0577
            },
            {
                "reference": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks.",
                "distance": 0.068
            },
            {
                "reference": "Many of the most-spoken low-resource languages are creoles which have a distinctive place in exploring the effectiveness of transfer from large pre-trained models. Previous work shows that transfer from these models to low-resource languages that are unrelated to languages in their training set is not very effective.",
                "distance": 0.0736
            },
            {
                "reference": "Recent studies have shown that multilingual pretrained language models can align languages well. However, this alignment can potentially be improved.",
                "distance": 0.0738
            },
            {
                "reference": "Previous studies have shown that cross-lingual knowledge distillation can improve the performance of pre-trained models for cross-lingual similarity matching tasks. However, a large student model is required, which makes it impractical for deployment on memory-limited devices.",
                "distance": 0.0811
            },
            {
                "reference": "Recent work on multilingual language models has shown the ability for cross-lingual zero-shot transfer on downstream tasks, yet there is uncertainty around what shared properties between languages enable this high-level performance. Traditional analyses between pairs of natural languages often lead to contradictory results due to languages having many differing characteristics simultaneously.",
                "distance": 0.0818
            },
            {
                "reference": "Pretraining cross-lingual language models on unlabeled texts has shown significant performance improvements for various cross-lingual and low-resource tasks. These models leverage high-resource languages to improve low-resource language processing.",
                "distance": 0.0821
            },
            {
                "reference": "Two prevalent approaches exist when supporting multiple languages for a given problem: training a model for each language with annotations divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages.",
                "distance": 0.0876
            },
            {
                "reference": "Although the development of corpora in English has led to significant advances in training semantic parsers, data in other languages is limited, impacting the performance of parsers in these languages. While pretrained multilingual models have been proven useful for zero-shot cross-lingual transfer in many NLP tasks, the requirements for applying a parser trained in English to other languages for zero-shot cross-lingual semantic parsing are still unclear.",
                "distance": 0.0952
            }
        ]
    },
    {
        "target": "Current generative models used for aspect-based sentiment analysis require large-scale computing resources and they do not explicitly model the structure between sentiment elements.",
        "prediction": "Traditional sentiment analysis approaches tackle problems like ternary (3-category) and fine-grained (5-category) classification by learning the tasks separately.",
        "queries": [
            -21.468393,
            0.432737
        ],
        "log": [
            {
                "reference": "Traditional sentiment analysis approaches tackle problems like ternary (3-category) and fine-grained (5-category) classification by learning the tasks separately.",
                "distance": 0.0105
            },
            {
                "reference": "Neural models used for sentiment classification over constituent trees learn phrase composition automatically by encoding tree structures, but they do not explicitly model sentiment composition, which requires encoding sentiment class labels.",
                "distance": 0.0402
            },
            {
                "reference": "Target-based sentiment analysis has two sub-components: opinion target extraction and target sentiment classification. However, most existing works study these two tasks individually, which limits their practical application.",
                "distance": 0.0488
            },
            {
                "reference": "Aspect-Based Sentiment Analysis is commonly tackled using high-performing machine learning approaches, but these methods do not always exploit high level linguistic information.",
                "distance": 0.054
            },
            {
                "reference": "Aspect-level sentiment classification is challenging task and the existing models generally do not mimic human reading cognitive processes (pre-reading, active reading, post-reading).",
                "distance": 0.0544
            },
            {
                "reference": "Existing machine learning approaches for sentence-level sentiment analysis often struggle with the modeling of complex linguistic structures across sentences. They frequently fail to capture nonlocal contextual cues necessary for sentiment interpretation.",
                "distance": 0.0555
            },
            {
                "reference": "Various topic models have been developed for sentiment analysis tasks, but they lack fine-grained dependency analysis between topical aspects and sentiments due to a simple topic-sentiment mixture assumption.",
                "distance": 0.0565
            },
            {
                "reference": "Traditionally, sentiment analysis models used are based on Recurrent Neural Networks (RNN) without a capsule-based approach.",
                "distance": 0.0569
            },
            {
                "reference": "The importance of sentiment classification has grown due to the exponential increase in the availability of online reviews and recommendations. However, reviews span different domains making it difficult to gather annotated training data for all of them. Thus domain adaptation for sentiment classifiers becomes a significant problem to study.",
                "distance": 0.0579
            },
            {
                "reference": "Sentiment classification typically involves text representation based on features like bag-of-words and semantic structures, but this often cannot capture complex patterns or correlations in the linguistic annotation graphs.",
                "distance": 0.0645
            },
            {
                "reference": "Existing models for fine-grained sentiment analysis rely on significant amounts of manually crafted sentence labels, which can be resource-intensive.",
                "distance": 0.0713
            },
            {
                "reference": "Continual learning (CL) techniques have been proposed for document sentiment classification, but not for aspect sentiment classification (ASC) tasks.",
                "distance": 0.0825
            },
            {
                "reference": "Previous methods for aspect-based sentiment analysis models the detection of aspects and the classification of their polarity separately.",
                "distance": 0.0887
            },
            {
                "reference": "Existing work in fine-grained sentiment analysis focuses mostly on sentences and phrases, overlooking the contribution of individual words and their grammatical connections. This is due to a lack of annotated data at the word level, and algorithms that can effectively use syntactic information.",
                "distance": 0.0916
            },
            {
                "reference": "Existing sentiment classification approaches work with document-level training data where each document has a sentiment label or aspect-level training data where each aspect has a sentiment label.",
                "distance": 0.0984
            },
            {
                "reference": "Traditional sentiment classification tasks often rely on training data from a single domain, which could limit the performance due to potential lack of diverse or generalizable sentiment indicators.",
                "distance": 0.0992
            }
        ]
    },
    {
        "target": "Knowledge-aware recommender systems currently struggle to identify informative knowledge connections effectively, and there is a need for a method that can rationalize these connections.",
        "prediction": "Knowledge graph-to-text (KG-to-text) generation methods phrase the task as a sequence-to-sequence generation task with linearized KG input and ensure consistency between generated texts and the KG through a simple selection between decoded sentence words and KG node words at each time step. However, this approach does not optimize the knowledge description order prediction with data-driven methods.",
        "queries": [
            -6.213735,
            -16.791183
        ],
        "log": [
            {
                "reference": "Knowledge graph-to-text (KG-to-text) generation methods phrase the task as a sequence-to-sequence generation task with linearized KG input and ensure consistency between generated texts and the KG through a simple selection between decoded sentence words and KG node words at each time step. However, this approach does not optimize the knowledge description order prediction with data-driven methods.",
                "distance": 0.0559
            },
            {
                "reference": "Previous works on knowledge base question answering (KBQA) have used subgraph retrieval for streamlined reasoning. However, these techniques can be either heuristic or intertwined with the reasoning process. This leads to partial subgraph reasoning, causing biases, especially when intermediate supervision is not present.",
                "distance": 0.0585
            },
            {
                "reference": "Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts and neglects the n-ary facts, which contain more than two entities. Moreover, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA.",
                "distance": 0.0692
            },
            {
                "reference": "Knowledge graph reasoning is pivotal in many applications, with knowledge graph completion (KGC) and multi-hop question answering over knowledge graph (Multi-hop KGQA) being two principal reasoning tasks. However, existing works typically consider these two tasks separately using different models or algorithms.",
                "distance": 0.0749
            },
            {
                "reference": "Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, but has been limited by the heterogeneous nature of its inputs and outputs, resulting in different SKG tasks being investigated by different communities in isolation.",
                "distance": 0.0842
            },
            {
                "reference": "Knowledge Graphs (KGs) are fundamental for search and question answering (QA) tasks, but research on deep or neural-based representation of KGs and deep QA has mostly transitioned to AI, ML, and NLP communities.",
                "distance": 0.0898
            },
            {
                "reference": "Answering complex natural language questions on knowledge graphs (KGQA) is a challenging task due to the need for reasoning with the input natural language questions and a massive, incomplete heterogeneous KG. Prior methods obtain an abstract structured query graph/tree from the input question and traverse the KG for answers. However, they cannot deal with missing links in the KG.",
                "distance": 0.0928
            },
            {
                "reference": "Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge lies in learning to reason over KG facts relevant to the question that traverse through KG entities and lead to the question answers. The existing approach decodes the question into instructions used to guide KG traversals, but can lead to reasoning under irrelevant context if they do not exactly match the underlying KG information.",
                "distance": 0.0965
            },
            {
                "reference": "Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG to answer natural language questions. These questions often present three challenges that previously have not been satisfactorily addressed: questions often do not specify exact timestamps; there are subtle lexical differences in time relations; off-the-shelf temporal KG embeddings previously used disregard the temporal order of timestamps.",
                "distance": 0.0973
            },
            {
                "reference": "Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. However, most approaches focus on leveraging knowledge graphs for more accurate recommendation, without explicit reasoning with knowledge for decision making.",
                "distance": 0.0996
            }
        ]
    },
    {
        "target": "Transforming narrative structure to implicit discourse relations in long-form text is challenging and inconsistent. Summarizing lengthy biographical discourse is beneficial to readers but is currently plagued by factual inconsistencies with respect to the inputs.",
        "prediction": "Generic sentence fusion is a common task in Natural Language Processing, but its definition and performance are subjects of ongoing discussion.",
        "queries": [
            -4.561284,
            9.052039
        ],
        "log": [
            {
                "reference": "Generic sentence fusion is a common task in Natural Language Processing, but its definition and performance are subjects of ongoing discussion.",
                "distance": 0.1036
            },
            {
                "reference": "Storytelling involves the communication of related events that form a process. Extracting the event chains that represent these processes from natural language text is a challenging problem due to the interleaving of principal events with background information, opinions, and other elements.",
                "distance": 0.1333
            }
        ]
    },
    {
        "target": "Task 4: Human Value Detection of SemEval 2023, called for models that can interpret data and detect human values.",
        "prediction": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
        "queries": [
            -1.912017,
            5.345277
        ],
        "log": [
            {
                "reference": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
                "distance": 0.2295
            },
            {
                "reference": "A recent challenge introduced by the SpaceEval shared task on spatial relation extraction is the identification of MOVELINKs, a type of spatial relation in which up to eight spatial elements can participate. This adds complexity to the task.",
                "distance": 0.2717
            }
        ]
    },
    {
        "target": "The SemEval 2023 Task 2: MultiCoNER II Multilingual Complex Named Entity Recognition, Track 9-Chinese focuses on detecting fine-grained named entities, representing a realistic challenge for Named Entity Recognition (NER).",
        "prediction": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
        "queries": [
            -1.672928,
            5.455747
        ],
        "log": [
            {
                "reference": "The ISO-Space annotation scheme is used to capture spatial information. However, there hasn't been a system that can automatically recognize spatial information following this scheme and can report results for all three evaluation configurations in SpaceEval.",
                "distance": 0.3262
            },
            {
                "reference": "The task of Japanese Word Sense Disambiguation (WSD) is being addressed by participating systems in SemEval-2.",
                "distance": 0.4285
            }
        ]
    },
    {
        "target": "Sentiment composition, which is the key to sentiment analysis, has been widely studied in the form of hierarchical trees, including untagged and sentiment ones. These existing approaches to sentiment composition are seen as intrinsically suboptimal.",
        "prediction": "Existing methods for sentence-level sentiment classification are mainly based on supervised learning. However, obtaining sentiment labels of sentences is difficult since manual annotation is expensive and time-consuming.",
        "queries": [
            -21.440863,
            0.517609
        ],
        "log": [
            {
                "reference": "Existing methods for sentence-level sentiment classification are mainly based on supervised learning. However, obtaining sentiment labels of sentences is difficult since manual annotation is expensive and time-consuming.",
                "distance": 0.0296
            },
            {
                "reference": "Existing models for fine-grained sentiment analysis rely on significant amounts of manually crafted sentence labels, which can be resource-intensive.",
                "distance": 0.032
            },
            {
                "reference": "Aspect-level sentiment classification is challenging task and the existing models generally do not mimic human reading cognitive processes (pre-reading, active reading, post-reading).",
                "distance": 0.035
            },
            {
                "reference": "In supervised and semi-supervised sentiment classification, a single perspective, often not distinguishing between personal and impersonal views, is typically employed.",
                "distance": 0.0371
            },
            {
                "reference": "Existing sentiment analysis models infer sentiments by naively aggregating sentiments from aspects and targets in opinionated text, which can impair performance due to the subtle and often conflicting interrelationships between targets and aspects.",
                "distance": 0.038
            },
            {
                "reference": "Current methods in visual sentiment analysis predominantly leverage object classification models trained on large datasets like ImageNet, however, these objects are sentiment neutral which hinders the transfer learning for sentiment analysis tasks.",
                "distance": 0.0421
            },
            {
                "reference": "Sentiment classification typically involves text representation based on features like bag-of-words and semantic structures, but this often cannot capture complex patterns or correlations in the linguistic annotation graphs.",
                "distance": 0.0454
            },
            {
                "reference": "For sentiment classification, embedding based on the distributional hypothesis is often found weak in capturing sentiment contrast, as contrasting words may have similar local contexts.",
                "distance": 0.046
            },
            {
                "reference": "Sentiment classification is the task of labeling review documents according to their polarity of opinion. In this process, model builders have three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language.",
                "distance": 0.0472
            },
            {
                "reference": "Most existing sentiment analysis work either uses prior lexical knowledge or treats the task as a text classification problem using only labeled training data. These approaches may not effectively utilize all available information for sentiment analysis.",
                "distance": 0.0531
            },
            {
                "reference": "In machine learning-based sentiment classification, extracting complex features that outperform simple features is a challenging problem. Several studies have focused primarily on character or word Ngrams features, but never considered substring-group features in the sentiment classification area.",
                "distance": 0.0574
            },
            {
                "reference": "Existing machine learning approaches for sentence-level sentiment analysis often struggle with the modeling of complex linguistic structures across sentences. They frequently fail to capture nonlocal contextual cues necessary for sentiment interpretation.",
                "distance": 0.061
            },
            {
                "reference": "Existing sentiment classification algorithms typically split a sentence into a sequence of words, and struggle to effectively handle inconsistent sentiment polarity between a phrase and the words it contains, like \u201cnot bad\u201d and \u201ca great deal of \u201d.",
                "distance": 0.0643
            },
            {
                "reference": "The problem of sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks has not been efficiently solved in the existing literature.",
                "distance": 0.0755
            },
            {
                "reference": "Existing sentiment analysis approaches often rely on binary polarity, but there is a need for more fine-grained sentiment analysis that can compare adjectives sharing a common semantic property.",
                "distance": 0.0782
            },
            {
                "reference": "Document sentiment classification is a task to classify a document according to the positive or negative polarity of its opinion.",
                "distance": 0.0835
            },
            {
                "reference": "Existing approaches to sentiment lexicon construction often utilize manual methods or automatic methods that may not achieve optimal performance.",
                "distance": 0.0844
            },
            {
                "reference": "Sentiment analysis, the task of determining the polarity (positive or negative) of documents, for multiple domains is challenging as the polarity of some words cannot be determined without domain knowledge. Previous approaches have used transfer learning techniques, but they are not feasible for multiple source domains and multiple target domains.",
                "distance": 0.0845
            },
            {
                "reference": "Traditional sentiment analysis approaches tackle problems like ternary (3-category) and fine-grained (5-category) classification by learning the tasks separately.",
                "distance": 0.0849
            },
            {
                "reference": "Current sentiment analysis methods struggle with complex scenarios such as subclause embeddings, negation, and polarity conflicts. Additionally, conventional methods overlook the polar effects and expectations that some verbs impose on their semantic roles.",
                "distance": 0.0853
            }
        ]
    },
    {
        "target": "Pretrained language models do not successfully map human-written definition and its usage in example sentences, suggesting a gap in how these models represent words and their meanings.",
        "prediction": "In language modeling, backing off to a shorter history and class-based generalization are usually used as two complementary mechanisms to use a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation.",
        "queries": [
            -1.742875,
            13.273772
        ],
        "log": [
            {
                "reference": "In language modeling, backing off to a shorter history and class-based generalization are usually used as two complementary mechanisms to use a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation.",
                "distance": 0.0281
            },
            {
                "reference": "Neural language models exhibit impressive performance on a variety of tasks, but understanding their internal reasoning is challenging. Existing methods use probes to uncover meaningful properties within model representations, but it is unclear how faithfully such probes portray information that the models actually use.",
                "distance": 0.0482
            },
            {
                "reference": "Probing experiments investigate how neural representations encode properties like part-of-speech. Existing methods only look at whether probing a representation produces higher accuracy than a baseline such as non-contextual word embeddings, without really assessing what extra information the representation might hold beyond the baseline.",
                "distance": 0.0756
            }
        ]
    },
    {
        "target": "Access to a representative sample from the population is an assumption that underpins all of machine learning. Unfortunately, selection effects can cause observations to instead come from a subpopulation, causing potential bias in inferences.",
        "prediction": "Current standard test sets for supervised learning only evaluate in-distribution generalization, which can be misleading when a dataset has systematic gaps such as annotation artifacts. Typically, a model may learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test.",
        "queries": [
            1.210308,
            -9.694158
        ],
        "log": [
            {
                "reference": "Current standard test sets for supervised learning only evaluate in-distribution generalization, which can be misleading when a dataset has systematic gaps such as annotation artifacts. Typically, a model may learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test.",
                "distance": 0.0274
            },
            {
                "reference": "In machine vision, the assumption is often made that the training and test samples come from the same distribution. However, in bio-medical applications, this assumption can be grossly violated due to changing experimental conditions, and is even more pronounced with 3D data, where annotation is very time-consuming, limiting the amount of data that can be labeled for training new acquisitions.",
                "distance": 0.0354
            },
            {
                "reference": "Traditional machine learning assumes that training and test data should be from the same distribution. However, this assumption may not hold true in many cases, especially if a task from a new domain arrives, but only labeled data from a similar old domain is available.",
                "distance": 0.0402
            },
            {
                "reference": "Supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets quantify this phenomenon by perturbing test samples in a minimal way to change output, but most are created manually, requiring intensive annotation effort.",
                "distance": 0.0455
            },
            {
                "reference": "Deep learning algorithms can perform poorly when the training dataset suffers from severe class-imbalance, but the testing criterion requires good generalization on less frequent classes.",
                "distance": 0.0533
            },
            {
                "reference": "Much of statistics and machine learning rely on random sampling and designed experiments. However, sometimes the only available data were obtained through different means, such as transaction logs or advertisement data. This can lead to badly biased estimates, even with perfect random sampling.",
                "distance": 0.0583
            },
            {
                "reference": "In the context of CNN classifiers, the problem addressed is that the class priors of the training and test set can be different.",
                "distance": 0.0681
            },
            {
                "reference": "Deep image classifiers often underperform when training data are heavily class-imbalanced. Existing regularization techniques such as Mixup, Manifold Mixup, and CutMix also fail to handle class imbalance effectively.",
                "distance": 0.095
            }
        ]
    },
    {
        "target": "The problem of agnostic PAC reinforcement learning (RL) concerning how many rounds of interaction with an unknown MDP are required to learn an $\\epsilon$-suboptimal policy with respect to a given policy class $\\Pi$, especially in large state and action space has been posed.",
        "prediction": "Information-directed sampling (IDS) has shown promise as a data-efficient reinforcement learning algorithm, but its optimization when contextual information is available remains unclear.",
        "queries": [
            8.420734,
            -20.852077
        ],
        "log": [
            {
                "reference": "Information-directed sampling (IDS) has shown promise as a data-efficient reinforcement learning algorithm, but its optimization when contextual information is available remains unclear.",
                "distance": 0.0309
            },
            {
                "reference": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism often couples the dynamics of the environment and the learning objective, complicating generalization of the task specification and obfuscating connections between different task settings.",
                "distance": 0.0527
            },
            {
                "reference": "In the context of reinforcement learning, the interruption of episodes by an external non-Markovian observer, such as a human, is not comfortably accounted for in the traditional Markov Decision Process (MDP) framework.",
                "distance": 0.0611
            },
            {
                "reference": "Reinforcement Learning traditionally uses Markov decision problems (MDPs) that have discrete state spaces and continuous control spaces, and often involve complex minimization problems and require state-action values for off-policy learning.",
                "distance": 0.0653
            },
            {
                "reference": "Reinforcement learning in an environment modeled by an episodic, finite, stage-dependent Markov decision process is a well-studied problem. The open problems raised by Agrawal and Jia [2017b] for the episodic setting have not yet been answered.",
                "distance": 0.0866
            },
            {
                "reference": "Previous work in reinforcement learning (RL) in episodic MDPs with adversarial full-information reward feedback and unknown fixed transition kernels has focused primarily on static regret, which does not account for the non-stationarity of environments.",
                "distance": 0.0935
            }
        ]
    },
    {
        "target": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, affecting both node features and graph topology.",
        "prediction": "Graph Neural Networks (GNNs), extensions of neural networks for graph-structured data, are vulnerable to adversarial attacks, with minor perturbations in the structure leading to significant performance degradation.",
        "queries": [
            -9.119196,
            -17.28023
        ],
        "log": [
            {
                "reference": "Graph Neural Networks (GNNs), extensions of neural networks for graph-structured data, are vulnerable to adversarial attacks, with minor perturbations in the structure leading to significant performance degradation.",
                "distance": 0.0311
            },
            {
                "reference": "Heterogeneous Graph Neural Networks (HGNNs) have achieved outstanding performance in many tasks. However, their robustness against adversarial attacks especially the vulnerability caused by adversarial edges between a target node and large-degree nodes (i.e., hubs) has not been well studied.",
                "distance": 0.0332
            },
            {
                "reference": "Recent studies have revealed that Graph Neural Networks (GNNs) are highly susceptible to adversarial attacks on both the graph structure and node attributes, resulting in unreliable outcomes.",
                "distance": 0.0353
            },
            {
                "reference": "Most graph neural networks (GNNs) use the message passing paradigm, where node features are propagated on the input graph. Previous work has pointed out that information distortion from distant nodes can limit the efficiency of message passing for tasks relying on long-distance interactions, a phenomenon referred to as 'over-squashing'.",
                "distance": 0.0478
            },
            {
                "reference": "Graph Neural Networks (GNNs) are commonly used for collaborative filtering but struggle to capture long-range dependencies effectively due to reliance on propagating and aggregating messages between local neighbors. Training deep GNNs to improve this can suffer from issues like over-fitting and over-smoothing.",
                "distance": 0.0498
            },
            {
                "reference": "Graph Neural Networks (GNNs), which generally follow some variant of Message-Passing (MP) with repeated aggregation, face the oversmoothing phenomenon: by performing too many rounds of MP, the node features tend to converge to a non-informative limit. Existing analyses do not exhibit both beneficial 'finite' smoothing and oversmoothing in the limit.",
                "distance": 0.0502
            },
            {
                "reference": "Graph Neural Networks (GNN) are effective at node classifications across various domains. However, they are vulnerable to adversarial attacks impacting their classification performance. Existing adversarial attacks on GNNs typically involve manipulating the connectivity between existing nodes.",
                "distance": 0.0519
            },
            {
                "reference": "Current Graph Neural Networks (GNN) architectures rely on two main components: node features embedding through message passing and aggregation through pooling. The structural (or topological) information is only implicitly considered in these steps.",
                "distance": 0.0537
            },
            {
                "reference": "Graph Neural Networks (GNNs) are powerful tools in learning node representations, but their performance significantly decreases when more convolutional layers are added. Previous studies attribute this limitation to over-smoothing, where node embeddings converge into indistinguishable vectors.",
                "distance": 0.0541
            },
            {
                "reference": "Graph Neural Networks (GNNs) have excelled by extending traditional convolution to learning on non-Euclidean data. These networks typically adopt a message-passing paradigm with two stages: aggregation and update. While the topology information is considered in the aggregation stage, all nodes share the same updating function in the updating stage, treating each node embedding as independent random variables, disregarding the inherent relationships between neighborhoods.",
                "distance": 0.0548
            },
            {
                "reference": "The convergence property of graph neural networks (GNN), specifically Invariant Graph Networks (IGN), is an underexplored topic in the existing literature, despite significant research into their expressive power and over-smoothing properties.",
                "distance": 0.0556
            },
            {
                "reference": "Scaling arbitrary message-passing Graph Neural Networks(GNNs) to large graphs is often challenging due to constraints on GPU memory usage. Existing solutions often involve sub-sampling of edges or non-trainable propagations which significantly reduce the expressive power of the GNN.",
                "distance": 0.0569
            },
            {
                "reference": "Although attention mechanisms in graph neural networks have been thoughtfully designed to give more weight to critical neighbor nodes, the understanding of what these mechanisms learn is limited, especially in the context of noisy graphs.",
                "distance": 0.0599
            },
            {
                "reference": "Recommender systems use historical interactions to personalized recommendations. Recently, Graph Neural Networks (GNNs) have emerged as an effective representation learning technique from structured graph data. Many existing graph-based recommender systems have been shown to achieve state-of-the-art performance when employing GNNs. However, these approaches have limitations, preventing GNNs from achieving their full potential.",
                "distance": 0.0609
            },
            {
                "reference": "Self-attention mechanism in graph neural networks (GNNs) has led to state-of-the-art performance on many graph representation learning tasks. However, current models compute attention independently for each neighbor of a node and do not consider nodes not connected by an edge, which can provide important network context information.",
                "distance": 0.0615
            },
            {
                "reference": "Graph Neural Networks (GNNs) are powerful architectures effective for graph-based collaborative filtering. However, GNNs are known to be vulnerable to adversarial perturbations and many studies inject adversarial perturbations into either node features or hidden layers of GNNs. Yet, perturbing graph structures in recommendations is less studied.",
                "distance": 0.0619
            },
            {
                "reference": "Graph Neural Networks (GNNs), which naturally integrate node information and topological structure, have been demonstrated to be effective in learning on graph data. Despite their potential to advance social recommendation, building social recommender systems using GNNs presents several challenges due to the complexity of encoding user-item interactions, heterogeneous social relation strengths, and users participating in multiple graphs.",
                "distance": 0.062
            },
            {
                "reference": "Graph Neural Networks (GNNs) use a message-passing scheme to learn representation of graph-structured data. To refine the representation, attention mechanism has been used to assign trainable weights to node's neighbors in the aggregate, although these methods have gaps in their discriminative capacities.",
                "distance": 0.0622
            },
            {
                "reference": "Graph Neural Networks (GNNs) often use average weight aggregation, which fails to model the differences in importance among different nodes in the neighborhood. GAT-based models address this by introducing an attention mechanism, but they neglect rich structural information and can suffer from over-smoothing.",
                "distance": 0.0626
            },
            {
                "reference": "The scalability of Graph Neural Networks (GNNs) is a major challenge in graph machine learning due to the exponential growth of receptive fields. Various approaches, such as sampling-based methods and techniques based on pre-computation of graph filters, have been proposed to address this issue.",
                "distance": 0.0634
            }
        ]
    },
    {
        "target": "Heterogeneous image fusion (HIF) techniques aim to enhance image quality by merging information from images captured by different sensors. Deep unfolding network (DUN)-based methods are relatively successful but still suffer from two issues: they lack a degradation-resistant-oriented fusion model and struggle with the structural properties of DUNs, making them vulnerable.",
        "prediction": "Color filter array (CFA) has been essential for modern photography and recently multispectral filter array (MSFA) has found wide application. However, the technology currently lacks a deep learning network capable of joint demosaicking and denoising for both CFA and MSFA raw images.",
        "queries": [
            22.206791,
            -8.09462
        ],
        "log": [
            {
                "reference": "Color filter array (CFA) has been essential for modern photography and recently multispectral filter array (MSFA) has found wide application. However, the technology currently lacks a deep learning network capable of joint demosaicking and denoising for both CFA and MSFA raw images.",
                "distance": 0.0711
            },
            {
                "reference": "The demosaicing process converts single-CCD color representations of one color channel per pixel into full per-pixel RGB. Current techniques may face limitations such as color fringing artifacts and aliasing.",
                "distance": 0.079
            },
            {
                "reference": "Color images are usually captured by a sensor with a color filter array (CFA), requiring a demosaicing process to generate a full color image. The issue is that the captured images generally have a low signal-to-noise ratio, and the demosaicing step further corrupts the image, which results in visually objectionable random noise patterns (splotches).",
                "distance": 0.08
            }
        ]
    },
    {
        "target": "A continuing challenge in neural networks is the ability to continually train from sequentially streamed few-shot data without significant loss of past knowledge, also known as 'catastrophic forgetting'.",
        "prediction": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning (learning tasks sequentially), especially with the rapid progress in computer vision and natural-language processing using large-scale pretrained models.",
        "queries": [
            -2.048959,
            -6.702015
        ],
        "log": [
            {
                "reference": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning (learning tasks sequentially), especially with the rapid progress in computer vision and natural-language processing using large-scale pretrained models.",
                "distance": 0.0052
            },
            {
                "reference": "Continual Learning (CL), which refers to learning multiple tasks sequentially without forgetting previous knowledge, remains a long-standing challenge for neural networks. Existing methods typically rely on adding network capacity or data replay.",
                "distance": 0.0099
            },
            {
                "reference": "Continual Learning (CL) is a problem of modeling sequences of related tasks, which involve challenges such as catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. Until now, solution approaches to CL have been task- and/or model-specific.",
                "distance": 0.011
            },
            {
                "reference": "Deep neural networks struggle with catastrophic forgetting when learning new categories, and previous attempts to solve this problem either succumb to the stability-plasticity dilemma or require significant computational or storage resources.",
                "distance": 0.0155
            },
            {
                "reference": "Catastrophic forgetting is a detrimental issue in training deep neural networks. Existing solutions often require additional network components and find it difficult to scale to a large number of tasks.",
                "distance": 0.017
            },
            {
                "reference": "Continual learning (CL) aims to learn a sequence of tasks without forgetting previously acquired knowledge. However, recent advances in CL are limited to supervised continual learning (SCL) scenarios, which are not scalable to real-world applications because data in such scenarios is often biased and unannotated.",
                "distance": 0.0171
            },
            {
                "reference": "The catastrophic forgetting problem in sequential learning of multiple tasks is a significant challenge. Previous models addressing this often require source task information and lead to increments in network size.",
                "distance": 0.018
            },
            {
                "reference": "Continual learning methods are needed that learn from data, while minimizing memory footprint and power consumption. While memory replay techniques have been promising in addressing this task, the best method for deciding which buffered images to replay is still a debated question.",
                "distance": 0.0199
            },
            {
                "reference": "Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks, but designing CL algorithms that perform reliably and avoid catastrophic forgetting has proven to be a persistent challenge.",
                "distance": 0.0209
            },
            {
                "reference": "Continual learning is a recent challenge in machine learning where models train well on the most recent data but face catastrophic forgetting of previous data due to shifting distributions, a problem that current replay-based methods attempt to solve with a small historical replay buffer.",
                "distance": 0.022
            },
            {
                "reference": "Backpropagation networks are known for their susceptibility to catastrophic forgetting, where they tend to forget previously learned skills upon learning new ones. Existing efforts to address this issue mainly focus on minimizing empirical risk through parameter regularization terms and episodic memory, without much exploration of the weight loss landscape.",
                "distance": 0.022
            },
            {
                "reference": "Catastrophic forgetting is a challenge in neural networks where the system 'forgets' previously learned knowledge when learning new tasks. Existing methods have primarily focused on overcoming this issue in convolutional neural networks (CNNs), which handle grid-based data like images, while largely overlooking graph neural networks (GNNs) that deal with non-grid data.",
                "distance": 0.0225
            },
            {
                "reference": "Continual learning (CL) is a crucial problem in the field of machine learning, where agents have to learn from a sequentially incoming stream of data, solving new problems over time while retaining previous knowledge. However, existing solutions face challenges of catastrophic forgetting (CF), high memory costs, and lack of theoretical understanding of neural networks' behavior when learning new tasks.",
                "distance": 0.0232
            },
            {
                "reference": "Catastrophic forgetting in neural networks is a performance issue observed in deep learning models when learning new tasks, leading them to forget previously learned tasks.",
                "distance": 0.0238
            },
            {
                "reference": "Catastrophic forgetting is a common problem in neural networks, where the network loses information about the first task after being trained on a second task.",
                "distance": 0.0265
            },
            {
                "reference": "The catastrophic forgetting problem in neural networks has been a primary focus in continual learning research with many solutions being proposed in the form of new algorithms. However, the underlying properties of such networks that contribute to this forgetting phenomenon are yet not well-understood.",
                "distance": 0.0271
            },
            {
                "reference": "Training a comprehensive artificial intelligence capable of multiple tasks is impeded by a problem known as catastrophic forgetting. Replay of all past data can mitigate this problem, but it requires substantial memory and is often unfeasible in real-world scenarios where past data access is limited.",
                "distance": 0.0283
            },
            {
                "reference": "Artificial neural networks, when learning tasks over time, suffer from a problem known as Catastrophic Forgetting (CF). This happens when the weights of a network are overwritten during the training of a new task causing forgetting of old information.",
                "distance": 0.029
            },
            {
                "reference": "Continual Learning (CL) primarily focuses on Bayesian inference over the parameters of a deep neural network. However, this traditional approach may lead to forgetting the previous tasks over time.",
                "distance": 0.0291
            },
            {
                "reference": "Continual Learning (CL) entails sequentially learning a set of tasks while preserving acquired knowledge. Existing methods often assume the data stream is explicitly divided into known contexts or tasks which helps in knowledge transfer between contexts. However, in real-world CL scenarios, there may not be clear task or context boundaries, leading to the need for task-agnostic CL.",
                "distance": 0.0297
            }
        ]
    },
    {
        "target": "Learning with noisy labels (LNL) is a challenging problem in weakly-supervised learning. Existing approaches use sample selection strategies and small-loss criteria to select clean samples, but this fails to reflect the complex feature landscape of various samples, leading to potential classification errors during sample selection.",
        "prediction": "In learning with noisy labels, the sample selection approach is commonly used, which considers small-loss data as correctly labeled during training. However, large-loss data, generated on-the-fly by the model being trained with noisy labels, are likely but not certain to be incorrect, presenting two possibilities.",
        "queries": [
            1.404276,
            -8.632571
        ],
        "log": [
            {
                "reference": "In learning with noisy labels, the sample selection approach is commonly used, which considers small-loss data as correctly labeled during training. However, large-loss data, generated on-the-fly by the model being trained with noisy labels, are likely but not certain to be incorrect, presenting two possibilities.",
                "distance": 0.0096
            },
            {
                "reference": "Current methods for learning with noisy labels show good performance by exploiting a small clean dataset and by correcting noisy labels on the fly using model-agnostic meta-learning. However, these approaches have problems such as potential label miscorrection, leading to degraded performance, and requiring at least three back-propagations per training step which slows training speed considerably.",
                "distance": 0.0173
            },
            {
                "reference": "Saliency detection models often suffer from noisy training labels, which are generated by unsupervised handcrafted feature-based methods.",
                "distance": 0.0227
            },
            {
                "reference": "Modern deep learning systems typically require large data sets to achieve robust performance, yet there is little guidance regarding how much or what type of data to collect. Existing strategies often risk either over-collecting, incurring unnecessary present costs, or under-collecting, potentially incurring future costs and delaying workflows.",
                "distance": 0.0312
            },
            {
                "reference": "With the increasing availability of pre-trained and regularly improving state-of-the-art models, there are challenges when a new ML model becomes available. Two main issues arise: deciding which data points should be re-evaluated given a limited budget (compute cost), and whether to update predictions if they differ from the current ones (maintaining consistency), especially to avoid negative flips.",
                "distance": 0.0372
            },
            {
                "reference": "The issue of robustness to label noise in training data presents a significant challenge in the development and accuracy of classification models.",
                "distance": 0.0438
            },
            {
                "reference": "Supervised learning under label noise has had many advances recently. Most current theoretical and empirical results are largely based on the class-conditional noise (CCN) assumption where the noise is considered independent of input features given the true label.",
                "distance": 0.0487
            },
            {
                "reference": "There has been interest in developing learning algorithms that can learn accurate classifiers from data with noisy labels. A widely-studied noise model is that of class-conditional noise (CCN), wherein a label y is flipped to a label with some associated noise probability. Current algorithms under the CCN model involve changing the training process by introducing a \u2018noise-correction\u2019 to the surrogate loss to be minimized over the noisy training examples.",
                "distance": 0.0555
            },
            {
                "reference": "Learning image classification models with label noise is a challenge. Existing solutions either rely on time-consuming manual identification of correct or incorrect labels, or non-human supervised methods that are scalable but less effective.",
                "distance": 0.0583
            },
            {
                "reference": "Collecting large scale annotated data often introduces label noise, and many methods for dealing with this noise rely on noisy classifiers to assess label trustworthiness. However, there is not a clear theoretical understanding of why these noisy classifiers work effectively.",
                "distance": 0.0781
            },
            {
                "reference": "In many visual recognition tasks, the ability to learn from noisy labels is essential as a vast amount of data with noisy labels are easy to obtain. Traditional methods treat label noise as statistical outliers and employ techniques such as importance re-weighting and bootstrapping to overcome this issue.",
                "distance": 0.0813
            },
            {
                "reference": "Label noise is prevalent in real-world visual learning applications and can lead to significant performance degeneration. Also, confidence estimates in active label correction (ALC) can be unreliable as the classifier is initially trained on noisy data, and passively selecting low-confidence examples can result in redundant labeling.",
                "distance": 0.084
            },
            {
                "reference": "Deep Learning with noisy labels is a practically challenging problem in weakly-supervised learning. The state-of-the-art approaches such as 'Decoupling' and 'Co-teaching+' use the 'disagreement' strategy to mitigate the problem of learning with noisy labels.",
                "distance": 0.0869
            },
            {
                "reference": "Learning with noisy labels is a common problem in supervised learning. Existing approaches require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem.",
                "distance": 0.0881
            },
            {
                "reference": "Machine learning models often need to be retrained as dataset changes, including additions or deletions of data points. This retraining is applicable in various areas including privacy and bias reduction, among others; however, it is costly to retrain models from scratch.",
                "distance": 0.0924
            },
            {
                "reference": "Deep learning requires data obtained from various sources, often resulting in noisy labels.",
                "distance": 0.0971
            },
            {
                "reference": "Learning with instance-dependent label noise is challenging because it is hard to model such real-world noise and annotators are more likely to annotate instances based on parts rather than whole instances.",
                "distance": 0.0993
            }
        ]
    },
    {
        "target": "Generative Adversarial Networks (GANs) are widely used for generating photo-realistic images. However, controlling and interpreting the contents of these generated images is challenging due to the entanglement in the feature spaces of GANs (e.g., StyleGAN).",
        "prediction": "Generative adversarial networks (GANs) have been successful in producing high-quality images in various domains. The synthesized images from GANs suggest that they maintain informative, disentangled, and explainable image representations. However, their use and transfer for downstream tasks is largely unexplored.",
        "queries": [
            4.498052,
            -25.873989
        ],
        "log": [
            {
                "reference": "Generative adversarial networks (GANs) have been successful in producing high-quality images in various domains. The synthesized images from GANs suggest that they maintain informative, disentangled, and explainable image representations. However, their use and transfer for downstream tasks is largely unexplored.",
                "distance": 0.0051
            },
            {
                "reference": "Generative adversarial networks (GAN) have found success in various domains, including information retrieval. However, embedding each node of a signed network as a low-dimensional vector is a challenge that needs to be addressed.",
                "distance": 0.0061
            },
            {
                "reference": "Recent advancements in Generative Adversarial Networks (GANs) have led to the generation of highly realistic images. However, the significant reduction of underlying artifacts and specific patterns in these GAN-generated images poses a challenge for their detection and for transferring knowledge to identify other types of GAN-images.",
                "distance": 0.0061
            },
            {
                "reference": "Generative Adversarial Networks (GANs) are widely used for image synthesis, but creating images from sparse sketches specifically for novice users in a wide array of object classes has not been widely addressed.",
                "distance": 0.0072
            },
            {
                "reference": "Co-generation, the task of inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones, is computation-intensive for complex settings. While extensive exploration has occurred around traditional distribution modeling such as structured prediction, co-generation with emerging models like generative adversarial nets (GANs) is relatively unexplored.",
                "distance": 0.0088
            },
            {
                "reference": "Generative Adversarial Networks (GANs) have been widely used in modeling diverse image distributions, but their latent space remains a black-box, making controllable generation an open problem, particularly in the presence of spurious correlations between different semantic attributes in the image distributions.",
                "distance": 0.014
            },
            {
                "reference": "Prior research in GAN evaluation relies heavily on the usage of embeddings from CNNs pretrained on Imagenet classification for assessing fidelity, precision, and recall measures. Despite criticism for their usage in non-Imagenet domains, these embeddings continue to be preferred in much of the GAN literature.",
                "distance": 0.0141
            },
            {
                "reference": "Generative Adversarial Networks (GANs) have made significant advancements in image generation, but there is a lack of understanding on how a realistic image is generated by the deep representations of GANs from a random vector.",
                "distance": 0.0161
            },
            {
                "reference": "Generative Adversarial Networks (GANs) have opened up exciting possibilities for image synthesis, but controlling the specifics of the generated images, such as viewpoint, aging and time of day, in an interpretable manner is still a challenge.",
                "distance": 0.0163
            },
            {
                "reference": "Generative modeling over natural images is one of the most fundamental machine learning problems. However, modern generative models, including Wasserstein Generative Adversarial Nets (WGANs), have not been extensively studied on manifold-valued images, despite their frequent occurrence in real-world applications.",
                "distance": 0.0195
            },
            {
                "reference": "Real-world complex systems require realistic simulations, which have been achieved by using generative models like GANs. However, these models often don't adequately model interactions in multi-agent systems.",
                "distance": 0.0196
            },
            {
                "reference": "Generative Adversarial Networks (GANs) can generate near photo-realistic images in narrow domains such as human faces, but have found it challenging to model complex distributions of datasets such as ImageNet and COCO-Stuff in unconditional settings.",
                "distance": 0.0199
            },
            {
                "reference": "Recent advances in Generative Adversarial Networks (GANs) have allowed for the generating of photorealistic images, but have also presented challenges to visual forensics and model attribution.",
                "distance": 0.0214
            },
            {
                "reference": "Standard approaches for generating adversarial patches lead to noisy conspicuous patterns, easily recognizable by humans. Furthermore, most of the current research focuses on suppressing a single large bounding box by directly overlapping it with the patch. Generating naturalistic patches with generative adversarial networks (GANs) for object detection is less explored.",
                "distance": 0.0218
            },
            {
                "reference": "Generating a novel image by manipulating two input images is a research problem in the study of generative adversarial networks (GANs).",
                "distance": 0.0219
            },
            {
                "reference": "The deep generative adversarial networks (GANs) have been shown to be promising for various computer vision applications such as image editing, synthesizing high-resolution images, generating videos, etc.",
                "distance": 0.0246
            },
            {
                "reference": "Noise injection has been effectively used to enhance generalization and avoid overfitting in machine learning and in generating high-fidelity images in Generative Adversarial Networks (GANs), specifically in models like StyleGAN, though its workings within GANs are unclear.",
                "distance": 0.0259
            },
            {
                "reference": "Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in an unsupervised manner. However, most of these GANs cannot properly cluster attributes of unlabeled data in their latent spaces, as they assume uniform distributions of the attributes.",
                "distance": 0.0261
            },
            {
                "reference": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. GAN-based models, common for non-autoregressive waveform synthesis, produce artifacts when performing mel-spectrogram inversion due to an inability for the generator to learn accurate pitch and periodicity.",
                "distance": 0.0273
            },
            {
                "reference": "Generative Adversarial Networks (GANs) have progressed in 3D-aware image synthesis from 2D data, but they fail to simulate indoor scenes due to the vast diversity in room layouts and objects inside. Also, indoor scenes don't have a shared intrinsic structure, and 2D images fail to provide enough guidance for the 3D geometry.",
                "distance": 0.028
            }
        ]
    },
    {
        "target": "Convolutional Neural Network (CNN) based methods that tackle hyperspectral image (HSI) denoising come with limitations of its efficiency and flexibility in capturing global and local spatial-spectral correlations.",
        "prediction": "Hyperspectral image (HSI) denoising has been used to improve HSI qualities, with learning-based methods showing their effectiveness. However, these methods are typically based on synthetic datasets, limiting their generalization capability on real testing HSI. Furthermore, no public paired real HSI denoising dataset exists to learn HSI denoising network and quantitatively evaluate HSI methods.",
        "queries": [
            22.329817,
            -8.181746
        ],
        "log": [
            {
                "reference": "Hyperspectral image (HSI) denoising has been used to improve HSI qualities, with learning-based methods showing their effectiveness. However, these methods are typically based on synthetic datasets, limiting their generalization capability on real testing HSI. Furthermore, no public paired real HSI denoising dataset exists to learn HSI denoising network and quantitatively evaluate HSI methods.",
                "distance": 0.0373
            },
            {
                "reference": "While deep learning has greatly improved hyperspectral image (HSI) reconstruction, current methods have deficiencies. Self-attention networks often compromise internal resolution in an attempt to balance performance and complexity, losing fine-grained, high-resolution features. Existing optimization methods focusing on spatial-spectral domain learning still result in a significant visual discrepancy between the reconstructed HSI and the true image.",
                "distance": 0.0597
            },
            {
                "reference": "The advancement of deep learning techniques has enhanced the spatial resolution of hyperspectral image super-resolution (HSI-SR), but developing unsupervised deep networks for this task remains a challenge.",
                "distance": 0.0656
            },
            {
                "reference": "Panchromatic (Pan) and multi-spectral (MS) images provided by satellites have high spatial and high spectral resolutions, respectively. The task is to create pan-sharpened images with high spatial and spectral resolutions using these images. Current methods use techniques like the additive wavelet luminance proportional (AWLP) method and context-based decision (CBD) method.",
                "distance": 0.0694
            },
            {
                "reference": "Non-local low-rank tensor approximation has evolved as a superior approach for hyperspectral image (HSI) denoising. However, with an increase in spectral bands, these methods' runtime significantly increases while offering little improvement in denoising performance.",
                "distance": 0.0769
            },
            {
                "reference": "A conventional camera reconstructs an image from a raw Bayer image through various signal processing steps performed sequentially. When these steps are performed in multiple stages, the residual error from each stage accumulates in the image and degrades the quality of the final reconstructed image.",
                "distance": 0.0793
            },
            {
                "reference": "Pan-sharpening, which generates high-resolution multi-spectral (MS) images by fusing PAN images and low-resolution MS images, has seen great advances. However, most existing pan-sharpening methods only work in the spatial domain and rarely explore potential solutions in the frequency domain.",
                "distance": 0.0841
            },
            {
                "reference": "Pansharpening, which aims to obtain a high-resolution image by fusing multispectral and panchromatic images, is commonly performed using CNNs with standard convolution operations. Yet, few approaches employ context-adaptive/dynamic convolution, which has shown impressive results in high-level vision tasks.",
                "distance": 0.0947
            },
            {
                "reference": "Existing pansharpening approaches, which fuse a high-resolution panchromatic image (PAN) with a low-resolution hyper-spectral image (LR-HSI) to generate an enhanced HSI with high spectral and spatial resolution, neglect using an attention mechanism for transferring HR texture features from PAN to LR-HSI features. This leads to spatial and spectral distortions.",
                "distance": 0.0951
            }
        ]
    },
    {
        "target": "Existing methods for differentiating through optimal trajectories arising from non-convex, constrained discrete-time optimal control (COC) problems achieve this efficiency by solving an auxiliary Linear Quadratic Regulator (LQR) problem. There are also claims that computing trajectory derivatives using the implicit function theorem (IFT) scales quadratically with the number of timesteps.",
        "prediction": "The problem of nonstochastic control with a sequence of quadratic losses, i.e., LQR control, has not been optimally solved yet, and the best known rate of dynamic regret was $ ilde{O}(\\sqrt{n (\\mathcal{TV}(M_{1:n})+1)})$ for general convex losses.",
        "queries": [
            6.995774,
            -20.40789
        ],
        "log": [
            {
                "reference": "The problem of nonstochastic control with a sequence of quadratic losses, i.e., LQR control, has not been optimally solved yet, and the best known rate of dynamic regret was $ ilde{O}(\\sqrt{n (\\mathcal{TV}(M_{1:n})+1)})$ for general convex losses.",
                "distance": 0.0468
            },
            {
                "reference": "The problem of learning in Linear Quadratic Control systems with unknown transition parameters has seen the development of efficient learning algorithms with regret growing with the square root of the number of decision steps.",
                "distance": 0.0678
            },
            {
                "reference": "Online adaptive control of the linear quadratic regulator is a well studied problem where the true system parameters are unknown and optimal regret is sought to scale favorably.",
                "distance": 0.0763
            },
            {
                "reference": "Adaptive control of high dimensional linear quadratic (LQ) systems has been previously studied, with established asymptotic convergence to an optimal controller. However, previous work's regret bounds were found to scale exponentially with the dimension of the state space, thus proving to be inefficient for large dimensions.",
                "distance": 0.0768
            },
            {
                "reference": "In solving the Linear Quadratic Regulator (LQR) problem of learning to control a linear dynamical system under fixed quadratic costs, so far, only model-based methods that depend on costly system identification were able to achieve regret scaling optimally with the time horizon T.",
                "distance": 0.0827
            },
            {
                "reference": "Adaptive control of the Linear Quadratic Regulator (LQR) has been a subject of interest where an unknown linear system is controlled subject to quadratic costs, but no polynomial time algorithm with provably sub-linear regret existed.",
                "distance": 0.0843
            },
            {
                "reference": "Current Linear Quadratic (LQ) control problems with unknown transition dynamics face sub-optimality issues, and existing work lacks sub-optimality guarantees in the partially observed Linear Quadratic Gaussian (LQG) setting.",
                "distance": 0.0873
            },
            {
                "reference": "Current state of the art methods for control in linear dynamical systems under adversarially changing strongly convex cost functions, given the knowledge of transition dynamics, such as the Kalman filter and the linear quadratic regulator, achieve regret which scales as T^0.5, where T is the time horizon.",
                "distance": 0.0905
            }
        ]
    },
    {
        "target": "Current research is primarily dedicated to improving the accuracy of camera-only 3D object detectors (apprentice) through knowledge transferred from LiDAR- or multi-modal-based models (expert). However, the domain gap between LiDAR and camera features, along with the inherent incompatibility in temporal fusion, significantly hinders the effectiveness of distillation-based improvements for apprentices.",
        "prediction": "Monocular 3D scene understanding tasks are challenging, with successful modern-day methods requiring the use of a 3D sensor, while single image-based methods perform significantly worse.",
        "queries": [
            17.783215,
            -1.069658
        ],
        "log": [
            {
                "reference": "Monocular 3D scene understanding tasks are challenging, with successful modern-day methods requiring the use of a 3D sensor, while single image-based methods perform significantly worse.",
                "distance": 0.0316
            },
            {
                "reference": "3D object detection is a crucial task in autonomous driving and virtual reality. LiDAR-based approaches perform well but are expensive, sometimes making them unsuitable for general scenes where LiDAR data might not be available.",
                "distance": 0.0319
            },
            {
                "reference": "Monocular 3D object detection presents significant challenges due to unreliable depth, causing a noticeable performance difference between monocular and LiDAR-based approaches.",
                "distance": 0.0386
            },
            {
                "reference": "LiDAR-based 3D object detection has become critical in long-range autonomous driving perception tasks. However, mainstream 3D object detectors rely on dense feature maps in their network backbone and prediction head. This creates issues as the computational and spatial costs on the dense feature map increase quadratically with the perception range, making them hardly scalable in long-range settings.",
                "distance": 0.0432
            },
            {
                "reference": "Detecting objects from LiDAR point clouds is significant for autonomous driving, but accurate and reliable 3D detection is challenging due to the sparsity and irregularity of LiDAR point clouds. Multi-view methods leveraging comprehensive information from bird's eye view (BEV) and range view (RV) have shown promise, but existing strategies either refine single-view predictions or fuse features without considering the global spatial context, limiting their performance.",
                "distance": 0.0452
            },
            {
                "reference": "In the field of 3D object detection, there exists a performance discrepancy between single-modality (LiDAR) detectors and multi-modality (LiDAR-image) detectors.",
                "distance": 0.0487
            },
            {
                "reference": "Monocular 3D object detection presents several challenges inherent to monocular imagery. Current methods heavily depend on labor-intensive and expensive manual annotation of 3D box labels on LiDAR point clouds.",
                "distance": 0.0502
            },
            {
                "reference": "LiDAR-based 3D object detection serves as an essential task in advanced autonomous driving systems. Though 3D detectors have achieved impressive results, they suffer performance degradation in unseen domains such as different LiDAR configurations, cities, or weather conditions. Current approaches employ unsupervised domain adaptation (UDA) techniques to address these challenges, but these yield unsatisfactory 3D detection results when facing severe domain shift.",
                "distance": 0.0527
            },
            {
                "reference": "Current 3D object detection systems use cameras and LiDAR sensors and their combination has been a challenge due to the differences in the signals, characteristics, and distributions they produce, making it difficult to accurately mix these spatial feature maps without losing information.",
                "distance": 0.0533
            },
            {
                "reference": "3D object detection is an important task for 3D scene understanding. While monocular-based methods can serve as an economical alternative to stereo-based or LiDAR-based methods, accurately detecting objects in 3D space from a single image is very challenging due to the lack of spatial cues.",
                "distance": 0.059
            },
            {
                "reference": "There is a need for high-accuracy 3D object detection in the autonomous driving scenario using a sensory-fusion framework that can take both LIDAR point cloud and RGB images as input and predict oriented 3D bounding boxes.",
                "distance": 0.0637
            },
            {
                "reference": "Monocular 3D object detection, which aims to extract the 3D position and properties of objects from a 2D input image, is a challenging problem due to the information loss by depth-agnostic cameras. Traditional approaches sample 3D bounding boxes from space and infer the relationship between the target object and each of them but the efficiency of this method is low due to the sparsity of effective samples in 3D space.",
                "distance": 0.0661
            },
            {
                "reference": "Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors and there remains a large gap in terms of performance between image-based and LiDAR-based methods. This is caused by inappropriate representation for the prediction in 3D scenarios.",
                "distance": 0.0662
            },
            {
                "reference": "Monocular 3D object parsing is challenging in various scenarios including occlusion reasoning and holistic scene interpretation, and acquiring training data with ground truth 3D shape and relevant concepts for these tasks is difficult.",
                "distance": 0.0664
            },
            {
                "reference": "Stereo-based 3D detection is aimed at detecting 3D object bounding boxes using either intermediate depth maps or implicit 3D geometry representations. However, its performance is currently inferior to LiDAR-based detection algorithms, particularly in accuracy of 3D bounding boxes due to erroneous depth features from stereo matching.",
                "distance": 0.0729
            },
            {
                "reference": "Deep learning-based road detection in unmanned driving using 3D Lidar data requires considerable data for training the DL model. The KITTI dataset provides sufficient 3D lidar and camera image data for road detection, but only contains manual annotations in camera images.",
                "distance": 0.0762
            },
            {
                "reference": "Existing 3D object detection methods for autonomous driving struggle to fully exploit the sparse and dense, semantic and geometry information in stereo imagery.",
                "distance": 0.0763
            },
            {
                "reference": "LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector, had been focusing only on object detection using LiDAR data.",
                "distance": 0.0779
            },
            {
                "reference": "In autonomous driving, LiDAR-based object detection is often affected by deteriorating point cloud quality due to variations in geographic locations and weather conditions. Modern detectors are found to perform inadequately when used in domains different from their training environments.",
                "distance": 0.0803
            },
            {
                "reference": "Recent voxel-based 3D object detectors for autonomous vehicles learn point cloud representations either from bird eye view (BEV) or range view (RV, also known as perspective view). Each view has its strengths and weaknesses, and these models typically use cuboid-shaped voxels in Cartesian coordinate system which only benefits learning BEV feature map.",
                "distance": 0.0858
            }
        ]
    },
    {
        "target": "The prevalent practice in Gaussian Process (GP) models involves using additive mixtures of Mat\u00e9rn kernels in single-output GP models and exploring the properties of multiplicative mixtures of Mat\u00e9rn kernels for multi-output GP models.",
        "prediction": "Previous Gaussian process (GP) regressions, like those proposed by Goldberg et al., use a separate GP to model the noise variance and utilize a Markov chain Monte Carlo method to approximate the posterior noise variance. However, these methods prove complicated to use and restrictive in combination with current GP-like sparse approximations.",
        "queries": [
            3.329553,
            -15.406754
        ],
        "log": [
            {
                "reference": "Previous Gaussian process (GP) regressions, like those proposed by Goldberg et al., use a separate GP to model the noise variance and utilize a Markov chain Monte Carlo method to approximate the posterior noise variance. However, these methods prove complicated to use and restrictive in combination with current GP-like sparse approximations.",
                "distance": 0.0057
            },
            {
                "reference": "Gaussian Processes serve as effective models for spatial data, however, their inherent smoothness can be a limiting factor, especially in physical scenarios where discontinuities occur along bounding surfaces such as in near-surface wind fields.",
                "distance": 0.0104
            },
            {
                "reference": "Gaussian processes are a type of function approximator that utilizes a covariance kernel to control inductive biases and representational learning, with kernel learning being essential for optimal predictive performance.",
                "distance": 0.0123
            },
            {
                "reference": "Early approaches to multiple-output Gaussian processes (MOGPs) struggled to provide a satisfactory parametric interpretation for cross covariance functions, conflicting with the interpretation abilities of single-output GPs. Current approaches have made progress by modelling the cross-covariances as a spectral mixture kernel with a phase shift.",
                "distance": 0.0207
            },
            {
                "reference": "Gaussian processes can be conditioned on gradient observations but this functionality is not often utilized due to the prohibitive computational cost of O((ND^3)), where N is the number of data points and D is the dimension.",
                "distance": 0.0218
            },
            {
                "reference": "Gaussian process (GP) regression is a popular method for non-linear prediction. However, when the input data is represented as a set of features like bag-of-words, calculating desirable kernel values becomes challenging because the co-occurrence of different but relevant words cannot be reflected in the kernel calculation.",
                "distance": 0.0247
            },
            {
                "reference": "Gaussian Process (GP) models are non-parametric models with rich representational power. By using a Gaussian process with an additive structure, complex responses can be modeled while retaining interpretability. However, previous work shows that additive Gaussian process models require high-dimensional interaction terms.",
                "distance": 0.0255
            },
            {
                "reference": "Gaussian processes (GPs) offer flexible function distributions, but their performance suffers in high-dimension applications due to the curse of dimensionality. This difficulty can be alleviated with low-dimensional projections, but these add numerous trainable hyperparameters, posing challenges in small data scenarios.",
                "distance": 0.0272
            },
            {
                "reference": "Gaussian processes (GPs) are probabilistic models of functions, but most GP kernels require $O((n+m)n^2)$ computational time, with $n$ being the number of data points and $m$ the number of predictive locations.",
                "distance": 0.0279
            },
            {
                "reference": "Gaussian Processes (GPs) have been extensively used in the modeling of random processes, but creating GPs with spectra that have compact support can present challenges which have not been thoroughly addressed.",
                "distance": 0.0281
            },
            {
                "reference": "Existing Gaussian Process (GP) models include GP regression, classification, and counting. Each GP model has a specific observation likelihood.",
                "distance": 0.0282
            },
            {
                "reference": "Currently, Gaussian process (GP) models are used for their property of introducing Matern family of kernels. In deep learning, activation functions do not mimic this property. Problematic uncertainty calibration in deep learning models particularly for out-of-distribution (OOD) data is a general issue.",
                "distance": 0.0285
            },
            {
                "reference": "The sparse pseudo-input Gaussian process (SPGP) model developed by Snelson and Ghahramani [1] provides a particular way to reduce computational complexity, but application to more complex problems such as binary classification was not fully explored.",
                "distance": 0.0285
            },
            {
                "reference": "Gaussian Processes (GPs) offer accurate uncertainty estimates but have a cubic cost in the number of data instances. Typically, sparse GP approximations with learned inducing points and stochastic variational inference are used to reduce this cost. However, the inducing points determine the flexibility of the model, and a large number may be required for optimal results in some tasks.",
                "distance": 0.0293
            },
            {
                "reference": "Gaussian processes (GPs) provide top performance in spatial statistics applications, but they require significant computations and storage, limiting their scalability. Moreover, existing fast Kronecker methods for GPs are confined to Gaussian likelihoods.",
                "distance": 0.0305
            },
            {
                "reference": "Gaussian Processes (GPs) are recognised as state-of-the-art for many machine learning tasks but have seen few applications in natural language processing (NLP). Commonly used machine learning frameworks in NLP are often linear models (logistic regression, least squares regression) and support vector machines.",
                "distance": 0.0305
            },
            {
                "reference": "Gaussian Processes (GPs) are commonly used for supervised learning applications; however, their use in safety-critical applications is restricted due to the lack of robust performance guarantees.",
                "distance": 0.0323
            },
            {
                "reference": "Gaussian Processes (GPs) define distributions over functions and their generalization capabilities depend heavily on the choice of kernels. The modeling of graph-structured data with GPs, however, presents challenges.",
                "distance": 0.0336
            },
            {
                "reference": "Current Warped Gaussian processes (WGP) use a parametric nonlinear transformation of a Gaussian process to model output observations in regression tasks. The parameters are typically learned using maximum likelihood, which may fail in certain scenarios such as low data regime, data with censored values, classification etc.",
                "distance": 0.0357
            },
            {
                "reference": "The use of Gaussian processes (GPs) on large datasets is often restricted due to heavy memory and computational requirements. Current methods that exploit structure in the kernel matrix struggle with mixtures of non-stationary processes, and separation of nonstationary sources is commonly a problem in spatio-temporal biomedical datasets.",
                "distance": 0.0365
            }
        ]
    },
    {
        "target": "The process of finding approximately-optimal preconditioners for solving linear systems has been problematic, typically involving high runtime and less optimized solution.",
        "prediction": "Nonlinear filtering can solve very complex problems, but typically involves very time-consuming calculations.",
        "queries": [
            6.735737,
            -14.059758
        ],
        "log": [
            {
                "reference": "Nonlinear filtering can solve very complex problems, but typically involves very time-consuming calculations.",
                "distance": 0.181
            },
            {
                "reference": "Solving systems of nonlinear equations is a challenging task in the field of parallel distributed computation.",
                "distance": 0.192
            }
        ]
    },
    {
        "target": "Improved architectures and better representation learning frameworks are driving significant advancements in visual recognition. Particularly, ConvNets like ConvNeXt have shown strong performance, but their combination with masked autoencoders (MAE) for self-supervised learning has produced subpar results.",
        "prediction": "Masked Autoencoders (MAE) are an established method for representation learning. However, its application on spatiotemporal representation learning from videos has not been fully explored.",
        "queries": [
            13.217628,
            15.033186
        ],
        "log": [
            {
                "reference": "Masked Autoencoders (MAE) are an established method for representation learning. However, its application on spatiotemporal representation learning from videos has not been fully explored.",
                "distance": 0.0222
            },
            {
                "reference": "Masked autoencoders (MAE) are commonly used for vision BERT pretraining.",
                "distance": 0.0266
            },
            {
                "reference": "Despite recent progress in masked image modeling, there is a gap due to the lack of semantic decomposition of images, making masked autoencoding (MAE) different between vision and language. Current methods mainly use random masking.",
                "distance": 0.0335
            },
            {
                "reference": "Masked Autoencoders (MAE) have been applied to image-based self-supervised representation learning. Recent models for audio and speech classification tasks have leveraged external supervised pre-training.",
                "distance": 0.0482
            },
            {
                "reference": "Masked Autoencoders (MAE) based on a reconstruction task have shown promising results for self-supervised learning (SSL) across different benchmark datasets. Despite the empirical success, there is limited theoretical understanding of how masking influences the performance of MAE.",
                "distance": 0.0632
            },
            {
                "reference": "Autoencoders (AE) aim to reproduce the output from the input, leading to a potential issue of overfitting towards learning the identity-function between the input and output. This becomes non-beneficial when AEs are used for prediction tasks, especially in the presence of noise in the data.",
                "distance": 0.0694
            },
            {
                "reference": "Existing methods for anomaly detection based on memory-augmented autoencoder (AE) require additional memory space for memory bank establishment and have a fixed number of prototypes based on subjective assumptions, which does not take into account data feature differences and diversity.",
                "distance": 0.0723
            },
            {
                "reference": "Deep Auto-Encoder (DAE) has been known for its capability in high-level representation learning, however, it lacks the ability to retain local geometric structure.",
                "distance": 0.0772
            },
            {
                "reference": "Autoencoder (AE)-based embedding approaches have achieved state-of-the-art performance in many tasks, but they struggle to handle real-world data that follow the power-law distribution, resulting in a polarization problem where dense inputs move away from sparse inputs in the embedding space even when they are highly correlated.",
                "distance": 0.0815
            },
            {
                "reference": "Autoencoder networks convert an input vector into a code vector using recognition weights and then use generative weights to transform the code vector into an approximate reconstruction of the input vector. Existing computational approach with distributed code vector representations is exponentially expensive.",
                "distance": 0.0861
            },
            {
                "reference": "Autoencoders are used to reconstruct input signals via a feature representation of latent variables. The representational capacity limit of the model is defined by the number of latent variables. In signal reconstruction tasks, autoencoders often exhaust their capacity on noisy signals, which is inefficient.",
                "distance": 0.0887
            },
            {
                "reference": "An autoencoder is a type of neural network that reproduces its input while learning key lower dimensional features, which encode the intrinsic data structure. However, recovering noisy motion data collected by a depth sensor using standard autoencoders can be challenging.",
                "distance": 0.0905
            },
            {
                "reference": "Autoencoders have been successful in learning meaningful representations from image datasets. Their performance on text datasets, which are confounding due to properties such as high-dimensionality, sparsity and power-law word distributions, hasn't been widely studied.",
                "distance": 0.0917
            },
            {
                "reference": "Autoencoder networks are unsupervised approaches combining generative and representational properties. However, the issues of whether they possess the same generative power as GANs or learn disentangled representations have not been fully established yet.",
                "distance": 0.095
            },
            {
                "reference": "Previous autoencoder models have been unable to learn representations in the multivariate Bernoulli latent space in an end-to-end fashion and perform well when compared with state-of-the-art variational methods.",
                "distance": 0.0967
            },
            {
                "reference": "Current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations.",
                "distance": 0.0972
            },
            {
                "reference": "Recurrent autoencoders are popular for time series anomaly detection, where outliers or abnormal segments are identified by high reconstruction errors. However, these autoencoders can often suffer from overfitting and error accumulation due to sequential decoding.",
                "distance": 0.0977
            },
            {
                "reference": "The task of learning representations of neural network weights from a model zoo, for applications like model inspection, neural architecture search, or knowledge distillation, is challenging and emerging. Autoencoders trained on a model zoo have been successful in capturing intrinsic and extrinsic properties of the models, but this work extends their use in generating new samples.",
                "distance": 0.0985
            },
            {
                "reference": "Autoencoders have been used for interpolating between data points by decoding convex combinations of latent vectors. However, this often results in artifacts or unrealistic reconstructions, largely because interpolated latent vectors can deviate from the data manifold.",
                "distance": 0.0989
            }
        ]
    },
    {
        "target": "Multilingual Neural Machine Translation (NMT) models can provide zero-shot translation, directly translating between language pairs unseen during training. However, for better transfer performance, these models need to learn universal representations across different languages which is a challenge.",
        "prediction": "In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches make use of subword-level tokens, but this increases sequence lengths and complicates the utilization of word-level information like POS tags or semantic dependencies.",
        "queries": [
            1.144918,
            14.855834
        ],
        "log": [
            {
                "reference": "In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches make use of subword-level tokens, but this increases sequence lengths and complicates the utilization of word-level information like POS tags or semantic dependencies.",
                "distance": 0.0311
            },
            {
                "reference": "Transfer learning is used in low-resource neural machine translation (NMT) to boost model performance. However, existing transfer learning methods for NMT are static, transferring knowledge from a parent model to a child model only once through parameter initialization.",
                "distance": 0.053
            },
            {
                "reference": "The standard neural machine translation model can only decode with the same depth configuration as training. Different hardware conditions on different terminal devices (e.g., mobile phones) require models of various sizes to maintain the same translation latency, leading to increased model maintenance costs and slower model iterations.",
                "distance": 0.0804
            }
        ]
    },
    {
        "target": "In dialogue, speakers need to adapt their utterances taking into account their audience's varying levels of knowledge about the discussed topic, and this audience-aware adaptation is a challenging problem in the computational domain.",
        "prediction": "Current turn-taking approaches for spoken dialogue systems restrict interactions and can lead to inefficient dialogues since they rely on the speaker releasing the turn before the other can take it.",
        "queries": [
            -7.183087,
            12.931269
        ],
        "log": [
            {
                "reference": "Current turn-taking approaches for spoken dialogue systems restrict interactions and can lead to inefficient dialogues since they rely on the speaker releasing the turn before the other can take it.",
                "distance": 0.0232
            },
            {
                "reference": "Incremental processing in interactive systems can address discourse phenomena like backchannels or barge-ins, enhancing system responsiveness and naturalness. However, prior work largely focused on deterministic incremental decision making, limiting system flexibility and adaptivity.",
                "distance": 0.0269
            },
            {
                "reference": "The existing architectural models for discourse processing in spoken dialogue systems are unable to efficiently manage dialogue, context tracking, and pragmatic adaptation in complex, near-future scenarios with multiple humans and computers in simultaneous dialogue exchanges.",
                "distance": 0.0466
            },
            {
                "reference": "The ongoing research or practices on dialogue systems that identify objects lack an approach that combines understanding, generation, semantics, and collaborative problem solving.",
                "distance": 0.0484
            },
            {
                "reference": "Response generation in collaborative consultation dialogues, particularly in cases where there is disagreement between the system (consultant) and user (executing agent), is a challenge that existing systems struggle to effectively address.",
                "distance": 0.0489
            },
            {
                "reference": "The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consist of multi-turn conversations on multiple topics and with knowledge annotations.",
                "distance": 0.053
            },
            {
                "reference": "Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains.",
                "distance": 0.0537
            },
            {
                "reference": "Existing dialogue learning systems do not adequately incorporate real-time human feedback during the fine-tuning process of language models for conditioned dialog generation tasks.",
                "distance": 0.0545
            },
            {
                "reference": "Current customer service dialogue generation research tends to generate generic seller responses by leveraging only the current dialogue information.",
                "distance": 0.0674
            },
            {
                "reference": "Accurate skill retrieval is a key factor for the success of modern conversational AI agents, but the major challenges lie in the ambiguity of human spoken language and the wide spectrum of candidate skills.",
                "distance": 0.0677
            },
            {
                "reference": "Most existing dialogue systems fail to respond properly to problematic content by either ignoring or passively agreeing with them.",
                "distance": 0.0692
            },
            {
                "reference": "The use of dialogue processing concepts in educational technology has been explored previously. In a previous corpus analysis of peer learning dialogues, initiative and initiative shifts were found to be indicative of learning.",
                "distance": 0.0751
            },
            {
                "reference": "Prior to this study, the issue of real-time detection of communication problems in spoken dialogue systems has been a challenge. Most existing methods either have significant computational overhead or are not universally applicable.",
                "distance": 0.0751
            },
            {
                "reference": "Deep models of dialog and communicative intent typically rely on modeling the internal state of the speakers\u2014states that are unobservable by a learning robot.",
                "distance": 0.0793
            },
            {
                "reference": "In Conversational AI, accurately tagging a query as out-of-domain is difficult, especially when the chatbot is not equipped to handle a topic which has semantic overlap with an existing topic it is trained on.",
                "distance": 0.085
            },
            {
                "reference": "Current conversational agents lack a dynamic model for controlling the turn-taking behavior which is key part of dialog systems.",
                "distance": 0.0858
            },
            {
                "reference": "Current task-oriented dialog systems do not model dialogs on a dialogue session level, causing a potential performance gap.",
                "distance": 0.0904
            },
            {
                "reference": "Participants in a persuasion dialogue use strategies to select locutions most likely to achieve their objective of persuading their opponent. Such strategies often presume the participant has a model of its opponents, which could be built based on a participant's accumulated dialogue experience. However, the potential for an agent's experience to encode additional information, which could enhance a strategy's efficiency, is often overlooked.",
                "distance": 0.0941
            },
            {
                "reference": "In the practice of Conversational Domain Exploration (CODEX), systems focus on enriching a user's knowledge of a given domain via an informative bot. It's challenging to make the conversations engaging, well-grounded in high-quality domain knowledge, and to allow the bot to pivot the conversation effectively without explicit user prompts.",
                "distance": 0.0973
            },
            {
                "reference": "Existing dialogue systems lack a general model and conceptual framework for specifying architectures for incremental processing.",
                "distance": 0.0996
            }
        ]
    },
    {
        "target": "Different knowledge graphs (KGs) contain complementary information, which can be used to improve KG embeddings and downstream tasks. However, current methods do not effectively make use of this across different KGs.",
        "prediction": "Knowledge graph completion (KGC) involves predicting missing edges in a knowledge graph, with recent research involving embeddings of these graphs into hyperbolic space. However, these methods, which emphasize either Euclidean or hyperbolic space, are unable to accurately capture the inherent heterogeneous structures of these multi-relational graphs.",
        "queries": [
            -6.473952,
            -16.612494
        ],
        "log": [
            {
                "reference": "Knowledge graph completion (KGC) involves predicting missing edges in a knowledge graph, with recent research involving embeddings of these graphs into hyperbolic space. However, these methods, which emphasize either Euclidean or hyperbolic space, are unable to accurately capture the inherent heterogeneous structures of these multi-relational graphs.",
                "distance": 0.0143
            },
            {
                "reference": "Most previous works on multi-hop knowledge graph (KG) reasoning use reinforcement learning (RL) based methods to navigate toward the target entity, but these methods suffer from slow and poor convergence and may fail when there is a missing edge along the path.",
                "distance": 0.0213
            },
            {
                "reference": "Knowledge graphs capture structured information and relations between a set of entities or items, and they have the potential to improve recommender systems. However, existing approaches rely on manual feature engineering and do not allow for end-to-end training.",
                "distance": 0.024
            },
            {
                "reference": "In search engines, Knowledge Graphs are used for knowledge discovery. When a user searches for an entity, related entities are usually provided without explanations of how they are related. Existing methods for dealing with large complex relatedness graphs between two related entities have limitations.",
                "distance": 0.0278
            },
            {
                "reference": "Knowledge Graphs (KGs) are used in Natural Language Processing tasks requiring real world data. However, creating embeddings for these large KGs poses challenges due to memory cost issues.",
                "distance": 0.0279
            },
            {
                "reference": "Knowledge Graph (KG) embeddings have been critical in predicting missing facts in KGs for knowledge base construction and reasoning. However, existing KG embedding approaches mainly focus on learning and predicting facts within single language-specific KGs, with no effective method of transferring knowledge between multiple KGs due to insufficiency of alignment information and inconsistency of facts.",
                "distance": 0.0287
            },
            {
                "reference": "Knowledge Graph (KG) completion aims to tackle the incompleteness issue in modern KGs. Existing work mostly leans on link prediction and relation prediction tasks, which have their own limitations and assumptions.",
                "distance": 0.0301
            },
            {
                "reference": "Knowledge Graphs (KGs) have been used to infer missing facts and entity types based on existing facts. However, there is still a challenge for the inference of fine-grained entity types due to the relatively fewer observed instances of these types present in KGs.",
                "distance": 0.0312
            },
            {
                "reference": "Estimating the accuracy of an automatically constructed knowledge graph (KG) is challenging due to its large number of entities and triples. Current methods focus on evaluating the triple accuracy that indicates information extraction quality, but ignore the entity accuracy.",
                "distance": 0.0325
            },
            {
                "reference": "Predicting missing facts in a knowledge graph (KG) is crucial but greatly hindered by incomplete modern KGs and the labor-intensive process of human labeling. This issue becomes more pronounced when dealing with multilingual knowledge. Existing approaches for handling this problem have limitations such as treating all alignment pairs equally and scarce seed alignment.",
                "distance": 0.0333
            },
            {
                "reference": "Knowledge Graphs are used for integrating data from heterogeneous sources at organizations like JPMorgan Chase (JPMC). A core problem in this field is linking mentions in textual sources to entities in the knowledge graph. Existing techniques cater mainly to entities existing in Wikipedia, proving less effective when applied to bespoke entities within an enterprise.",
                "distance": 0.0363
            },
            {
                "reference": "Pretraining a language model on text has been shown to help various downstream NLP tasks. Although recent works show that a knowledge graph (KG) can complement text data, these works don't pretrain to learn a deep fusion of the two modalities at scale.",
                "distance": 0.0363
            },
            {
                "reference": "Querying Knowledge Graphs (KGs) is a complex task due to their size and complexity. Traditional approaches of embedding KG units in Euclidean space often fail to capture the hierarchical nature and semantic information of entities in the graph, and they usually only utilize multi-hop queries, ignoring more complex operations like intersection and union of simpler queries.",
                "distance": 0.038
            },
            {
                "reference": "Knowledge Graphs (KGs) are complex and understanding the relatedness between entities in KGs can be challenging. Querying these KGs can also be complex due to the large and diverse nature of entities.",
                "distance": 0.039
            },
            {
                "reference": "Knowledge-graph (KG) embeddings have been used in biomedical research to address the gap between therapeutic needs and available treatments. However, the ability of the KG embeddings to capture the semantics of a node can depend on the data, as nodes can have a number of relationships that do not indicate their properties. Additionally, real-life KG embedding tools present a runtime performance challenge.",
                "distance": 0.0404
            },
            {
                "reference": "Knowledge graphs are widely used in information retrieval as they can enhance semantic understanding of queries and documents. These are used to consider entities and entity relationships as side information, and have been used in retrieval models, but have not been leveraged sufficiently in understanding users' search behavior.",
                "distance": 0.0421
            },
            {
                "reference": "Multimodal Knowledge Graphs (MKGs), organizing visual-text factual knowledge, have been utilized in several applications like information retrieval and recommendation systems. However, the incompleteness of most MKGs has prompted various studies focusing on knowledge graph completion tasks such as entity, relation extraction and link prediction. Such works often require model changes depending on the tasks and modalities, and also, not all images/objects are relevant to text input, limiting their applicability.",
                "distance": 0.0432
            },
            {
                "reference": "Models for Knowledge Graph Completion (KGC) tasks, such as link prediction, are typically evaluated using averaged metrics on a held-out test set. However, these metrics cannot reveal specific strengths and weaknesses of a model.",
                "distance": 0.0476
            },
            {
                "reference": "Knowledge graphs are often incomplete, and the evaluation of models for knowledge graph completion frequently ignores the incompleteness, treating all unknown triplets as false. This is known as the closed-world assumption.",
                "distance": 0.0489
            },
            {
                "reference": "Knowledge graphs have become essential for AI-related applications, yet modern ones are far from complete. Existing knowledge graph completion (KGC) models treat the triples in KGs independently, not leveraging the valuable information from the local neighborhood around an entity.",
                "distance": 0.0497
            }
        ]
    },
    {
        "target": "Graphs are often used to represent real-world structures in various fields, with high connectivity and robustness being desirable. While adding edges and anchoring nodes have been extensively studied to enhance the connectivity and robustness of graphs, the operation of merging nodes has been overlooked despite its practical relevance in scenarios like bus station reorganization or multiple team formation.",
        "prediction": "Many practical graph problems require to handle multi-relational graphs, which is often challenging due to their evolving nature, where new entities (nodes) can emerge over time. Moreover, newly emerged entities often have few links, which makes the learning even more difficult.",
        "queries": [
            -8.398656,
            -13.37897
        ],
        "log": [
            {
                "reference": "Many practical graph problems require to handle multi-relational graphs, which is often challenging due to their evolving nature, where new entities (nodes) can emerge over time. Moreover, newly emerged entities often have few links, which makes the learning even more difficult.",
                "distance": 0.0176
            },
            {
                "reference": "Large graph structures often focus on global structure and run algorithms that utilize the entire graph's size. There has been a shift towards focusing on the microscopic structure especially as graphs are dealing with billions of vertices and finding communities of hundreds.",
                "distance": 0.0377
            },
            {
                "reference": "In many real-world domains, link graphs are used to model relationships between objects and measuring the similarity of objects in a link graph has been widely studied but an efficient and effective method is still desired.",
                "distance": 0.0389
            },
            {
                "reference": "Graphs are often used to represent relationships between entities, but many times these relations are heterogenous and are simplified to single edges between pairs of nodes, which can conceal deeper, intrinsic connections.",
                "distance": 0.0392
            },
            {
                "reference": "Given two sets of vertices in a graph, identifying how the vertices are interconnected, especially the vertices of high prominence based on the topological structure is challenging. Additionally, many real-world applications using this concept are constructed on graphs that are too large to fit in a single server and the evaluation of such interconnections is NP-hard.",
                "distance": 0.0576
            },
            {
                "reference": "Graph represents a natural way of encoding data sample features and their relationships among them. Many techniques have been proposed in the past. However, recent years have seen the rapid development of data mining and knowledge discovery leading to novel graph analytics algorithms, which have been applied in various areas, including healthcare.",
                "distance": 0.0693
            },
            {
                "reference": "Graph-based knowledge representation has been a hot topic for some years, with significant potential for further research, particularly in its application to the biomedical domain. The use of graphs as a powerful tool to map structures within datasets and recognize relationships between specific data objects represents the current state of the field.",
                "distance": 0.083
            },
            {
                "reference": "The representation of knowledge has become graph-centric, with the computing community creating methods for supporting graph management via digital technology. Graph databases and knowledge graphs are among the most successful solutions.",
                "distance": 0.0852
            },
            {
                "reference": "In recent years, large-scale knowledge graphs such as Freebase, YAGO, Google's Knowledge Graph, and Microsoft's Satori have proliferated. Despite a large body of research on mining homogeneous graphs, these new information networks are highly heterogeneous, featuring thousands of entity and relation types and billions of instances of vertices and edges.",
                "distance": 0.0916
            },
            {
                "reference": "Recent years have seen an increase in large-scale knowledge graphs, from purely academic projects to major commercial projects, but these new information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances, representing a challenge to traditional graph mining methods.",
                "distance": 0.0996
            }
        ]
    },
    {
        "target": "Prior methods for Natural Language Querying (NLQ) on databases typically rely on text-to-SQL semantic parsing, which is a challenging and complex task.",
        "prediction": "Existing systems that allow users to access databases through natural language questions require encoding of knowledge about the application domain in complex data structures. This process often requires significant effort by computer professionals with special training in computational linguistics and databases.",
        "queries": [
            -5.042791,
            10.998064
        ],
        "log": [
            {
                "reference": "Existing systems that allow users to access databases through natural language questions require encoding of knowledge about the application domain in complex data structures. This process often requires significant effort by computer professionals with special training in computational linguistics and databases.",
                "distance": 0.0466
            },
            {
                "reference": "Previous research has developed natural language interfaces for formulating database queries, but there is a need for a way to present detailed, understandable explanations for these query results.",
                "distance": 0.0535
            },
            {
                "reference": "The construction of a natural-language interface for database queries is typically a manual process, and previous systems focused mainly on syntactic representations.",
                "distance": 0.0593
            },
            {
                "reference": "Currently, database query languages can be intimidating to non-experts, leading to high popularity for keyword-based search despite its limitations. There is a need for a natural language query interface.",
                "distance": 0.081
            },
            {
                "reference": "Earlier work on PLANES, a natural language question answering system interfacing users with relational databases, motivated efforts to extend its conceptual capabilities.",
                "distance": 0.0942
            },
            {
                "reference": "Translating relational queries expressed in SQL into natural language has been a challenging problem.",
                "distance": 0.0942
            }
        ]
    },
    {
        "target": "Task 9 (Multilingual Tweet Intimacy Analysis, MTIA) of the SemEval 2023 competition aims to quantitatively analyze tweets in 6 languages for intimacy, posing challenge in extracting semantic information from code-mixed texts.",
        "prediction": "The paper is addressing SemEval2018 task2, which deals with Multilingual Emoji Prediction.",
        "queries": [
            -22.87534,
            0.998731
        ],
        "log": [
            {
                "reference": "The paper is addressing SemEval2018 task2, which deals with Multilingual Emoji Prediction.",
                "distance": 0.0224
            },
            {
                "reference": "The task at hand is the SemEval 2018 task on emoji prediction in tweets.",
                "distance": 0.0602
            },
            {
                "reference": "The SemEval 2018 shared task involved building systems capable of predicting the occurrence of a language's most frequently used emojis in Tweets.",
                "distance": 0.0728
            }
        ]
    },
    {
        "target": "The Visual Word Sense Disambiguation (VWSD) shared task aims at selecting the image among candidates that best interprets the semantics of a target word with a short-length phrase for English, Italian, and Farsi. The limited phrase context and the visual label requires image-text matching performance across different modalities, which challenges the model's understanding ability.",
        "prediction": "The task of all-words Word Sense Disambiguation (WSD) on a specific domain warrants the development of high performance systems.",
        "queries": [
            17.773281,
            -17.373753
        ],
        "log": [
            {
                "reference": "The task of all-words Word Sense Disambiguation (WSD) on a specific domain warrants the development of high performance systems.",
                "distance": 0.0493
            },
            {
                "reference": "Word Sense Disambiguation (WSD) for the Chinese language is a significant task for which the optimal feature set was yet to be determined.",
                "distance": 0.0575
            },
            {
                "reference": "The topic of a document can provide useful information for Word Sense Disambiguation (WSD) as certain meanings tend to be associated with particular topics. However, current WSD systems may not effectively leverage this relationship.",
                "distance": 0.0599
            },
            {
                "reference": "Automatic interpretation of documents is hindered by ambiguous terms that can have multiple meanings, an issue also present in a specialized field like biomedicine. Existing Word Sense Disambiguation (WSD) systems are typically limited, only being able to identify the meanings for a small set of ambiguous terms.",
                "distance": 0.0638
            },
            {
                "reference": "Word Sense Disambiguation (WSD) is the task of identifying meaning in given context and lies at the base of Natural Language Processing. The knowledge acquisition bottleneck problem, which refers to the challenge of semantically annotating texts at a large scale and in different languages, is a pervasive issue that limits the creation of multilingual knowledge bases and manually-curated training sets.",
                "distance": 0.072
            },
            {
                "reference": "Word Sense Disambiguation (WSD) in the Chinese language is a challenging issue not adequately addressed by existing methods.",
                "distance": 0.084
            },
            {
                "reference": "Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field. They both address the lexical ambiguity of language, but they use different resources for their solutions: EL uses encyclopedic knowledge and WSD uses lexicographic information.",
                "distance": 0.0921
            },
            {
                "reference": "Word Sense Disambiguation (WSD) is an ongoing challenge in Natural Language Processing. Existing WSD systems and methods struggle when applied to specific domains, and the effectiveness of these systems in domain-specific applications is still an open research question.",
                "distance": 0.0936
            },
            {
                "reference": "Word Sense Disambiguation (WSD) is the task of associating a word in context with one of its meanings, which is typically performed using WordNet as the sense inventory. However, the fine granularity of WordNet makes it difficult to differentiate between similar senses, limiting WSD performance with peak F-scores noticeably below the 80% range.",
                "distance": 0.097
            }
        ]
    },
    {
        "target": "Task 9 requires the textual intimacy analysis of tweets in 10 languages. Severe class imbalance is observed in the official dataset, which can weaken the model's performance and impede convergence.",
        "prediction": "The SemEval 2018 shared task involved building systems capable of predicting the occurrence of a language's most frequently used emojis in Tweets.",
        "queries": [
            -22.800552,
            1.024383
        ],
        "log": [
            {
                "reference": "The SemEval 2018 shared task involved building systems capable of predicting the occurrence of a language's most frequently used emojis in Tweets.",
                "distance": 0.0148
            },
            {
                "reference": "SemEval 2018 Task 1: Affect in Tweets is a task for detecting and quantifying the intensity of emotions in Spanish.",
                "distance": 0.047
            },
            {
                "reference": "The task at hand is the SemEval 2018 task on emoji prediction in tweets.",
                "distance": 0.0485
            },
            {
                "reference": "The task of predicting emojis in tweets in both English and Spanish, particularly as part of the SemEval-2018 task on Multilingual Emoji Prediction, is a significant area of interest.",
                "distance": 0.0632
            },
            {
                "reference": "Task 11 of SemEval2015 aims to identify the sentiment intensity of figurative language in tweets, which could be a challenging task due to the use of irony and sarcasm.",
                "distance": 0.0845
            },
            {
                "reference": "IRISA was participating in the four tasks of the SMM4H 2018 challenge, which involves tweet classification.",
                "distance": 0.0865
            },
            {
                "reference": "The paper is addressing SemEval2018 task2, which deals with Multilingual Emoji Prediction.",
                "distance": 0.0897
            }
        ]
    },
    {
        "target": "Classic content moderation approaches often utilize a rule-based heuristic method, which while easily interpreted by humans, can lack robustness and flexibility to handle the variety of undesirable content online. Recent developments in deep learning offer a solution, but they lack transparency and explainability, issues which often result in mistrust and limited adoption.",
        "prediction": "Commercial content moderators, estimated to number 100,000, are often exposed to disturbing content, which can lead to lasting psychological and emotional distress.",
        "queries": [
            -22.09417,
            4.543485
        ],
        "log": [
            {
                "reference": "Commercial content moderators, estimated to number 100,000, are often exposed to disturbing content, which can lead to lasting psychological and emotional distress.",
                "distance": 0.1562
            },
            {
                "reference": "Online social media plays a vital role during real-world events, particularly crisis events. While it can be used effectively by authorities for disaster management, it could also be exploited by malicious entities to spread fake news and rumors, particularly fake images.",
                "distance": 0.162
            }
        ]
    },
    {
        "target": "Apache Hive is widely used for large-scale data analysis applications in many organizations with various visual analytical tools developed to help users identify the performance bottleneck of executed queries. Existing tools focus on showing the time usage of query sub-components but fail to provide sufficient evidence to analyze the root causes of slow execution progress.",
        "prediction": "Hadoop is a widely-used platform for large-scale data analytics, but it has a significant performance bottleneck due to its inability to colocate related data on the same set of nodes.",
        "queries": [
            -13.576861,
            -8.758125
        ],
        "log": [
            {
                "reference": "Hadoop is a widely-used platform for large-scale data analytics, but it has a significant performance bottleneck due to its inability to colocate related data on the same set of nodes.",
                "distance": 0.0022
            },
            {
                "reference": "Apache Hive is a widely used distributed, fault-tolerant data warehouse system for large-scale data analytics. Currently, it uses CPU for data processing, while it is a common practice to exploit the parallelism of GPU to improve the performance of Online Analytical Processing (OLAP) in database systems.",
                "distance": 0.0079
            },
            {
                "reference": "MapReduce is commonly used as a way of big data analysis in many fields. However, shuffling, the inter-node data exchange phase of MapReduce, has been reported as the major bottleneck of the framework. Efforts to accelerate shuffling exist but two questions remain open: the effect of Remote Direct Memory Access (RDMA) on the performance of shuffling, and whether the data transfer algorithm affects the shuffle performance.",
                "distance": 0.0088
            },
            {
                "reference": "Hadoop Distributed File System (HDFS) is widely used, but there is room for improvement in terms of efficiency.",
                "distance": 0.0115
            },
            {
                "reference": "Hadoop and the surrounding projects & vendors are maturing, and their impact on the data management sector is growing.",
                "distance": 0.0139
            },
            {
                "reference": "The adoption of technologies like Hadoop for large-scale data mining and machine learning has become popular due to its simple programming semantics and active ecosystem. However, providing a rich developer ecosystem, including easy data ingress to and egress from online systems and managing workflows, is not straightforward.",
                "distance": 0.0211
            },
            {
                "reference": "Enterprises are increasingly using Apache Hadoop as a central repository for data from a variety of sources. However, many enterprise data management tools, such as SAP ERP, SAS to Tableau, still rely heavily on SQL.",
                "distance": 0.0213
            },
            {
                "reference": "Hadapt, a company commercializing the Yale University research project HadoopDB, aims to maximize performance for processing data warehousing queries over very large datasets without sacrificing fault tolerance and scalability.",
                "distance": 0.0251
            },
            {
                "reference": "Hadoop is being increasingly used in data science workloads, however, there is a lack of understanding from a user-centric perspective about the manner in which data scientists use the system and the extent to which its use  aligns with its design.",
                "distance": 0.0292
            },
            {
                "reference": "Hadoop is a massively scalable parallel computation platform capable of running hundreds of jobs concurrently, and many thousands of jobs per day, which requires a scalable, secure, multi-tenant, and operable workflow management system. Existing workflow management tools are found lacking in at least one of these qualities.",
                "distance": 0.0313
            },
            {
                "reference": "Apache Hive is a widely used data warehouse system for Apache Hadoop. It is used for various big data analytics applications. However, there are several identified shortcomings in Hive's file formats, query planning, and query execution which affect its performance.",
                "distance": 0.0317
            },
            {
                "reference": "The Hadoop Distributed File System (HDFS) is the common system for handling large datasets but may have inefficiencies.",
                "distance": 0.0347
            },
            {
                "reference": "Apache Hive started as an open-source relational database system primarily designed for batch processing of big data workloads.",
                "distance": 0.0381
            },
            {
                "reference": "The Resilient Distributed Dataset (RDD) is the core memory abstraction behind the popular Apache Spark data-analytic framework. However, there are efficiencies to be found in how datasets relate to one another.",
                "distance": 0.0404
            },
            {
                "reference": "Enterprises are increasingly using Apache Hadoop, specifically HDFS, as a central repository for all their data from various sources. However, many enterprise data management tools rely on SQL and many users are familiar with it. Therefore, SQL processing over Hadoop data has gained traction, leading to a surge in systems providing such capabilities.",
                "distance": 0.042
            },
            {
                "reference": "The size of data sets being collected and analyzed in the industry for business intelligence is rapidly growing, making traditional warehousing solutions expensive. Hadoop, a popular open-source map-reduce implementation, is used as an alternative, but its programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse.",
                "distance": 0.0429
            },
            {
                "reference": "There is a growing interest in integrating relational DBMSs and MapReduce systems, but finding a balance, specifically leveraging the superior indexing and query processing power of a relational DBMS for data stored in Hadoop/HDFS, is a technical challenge.",
                "distance": 0.044
            },
            {
                "reference": "Spark is a widely used in-memory computing framework in big data applications that accelerates computations by storing data as resilient distributed datasets (RDDs) in main memory. However, main memory space is limited and Spark does not have an intelligent mechanism to optimize RDD storage.",
                "distance": 0.0459
            },
            {
                "reference": "Enterprises face the challenge of executing analytical queries over large datasets, and while Hadoop has proven to scale well, it struggles with I/O performance. The proposed solutions aiming to improve Hadoop's query performance often make deep modifications in Hadoop or HDFS, limiting their adoption and compatibility with future Hadoop versions.",
                "distance": 0.0475
            },
            {
                "reference": "The growth of Hadoop has facilitated a variety of application engines building on the YARN resource management layer, leading to fragmentation and repeated efforts as each engine re-implements basic functional features such as fault-tolerance and security.",
                "distance": 0.0483
            }
        ]
    },
    {
        "target": "The task of low-light image enhancement can be challenging due to the loss of certain information in images that are poorly lit. Existing solutions may fail to fully restore the details and color accuracy in degraded images.",
        "prediction": "Extremely low-light image enhancement aims to enhance image brightness and disclose hidden information in dark areas. Current methods face three main problems, namely failure to restore image details in extremely dark/bright areas, inability to precisely correct the color of low-light images and production of blurry images when the object edges are unclear.",
        "queries": [
            22.609179,
            -7.436757
        ],
        "log": [
            {
                "reference": "Extremely low-light image enhancement aims to enhance image brightness and disclose hidden information in dark areas. Current methods face three main problems, namely failure to restore image details in extremely dark/bright areas, inability to precisely correct the color of low-light images and production of blurry images when the object edges are unclear.",
                "distance": 0.0621
            },
            {
                "reference": "Enhancing low-light images to normally-exposed images is highly ill-posed because the mapping relationship between them is one-to-many. Previous works based on pixel-wise reconstruction losses and deterministic processes fail to capture the complex conditional distribution of normally exposed images, which leads to improper brightness, residual noise, and artifacts.",
                "distance": 0.0635
            },
            {
                "reference": "Underexposed image amplification is a challenging problem in image processing which usually need dual acquisition, containing a normally exposed image and a low-light one.",
                "distance": 0.0665
            },
            {
                "reference": "Typically, high resolution video capture does not include both high spectral and spatial resolution, which can limit performance in certain video analysis tasks.",
                "distance": 0.0718
            },
            {
                "reference": "Low-light image enhancement (LLE) remains challenging due to low-contrast and weak-visibility problems of single RGB images. Current LLE methods do not effectively utilize accessible unpaired over/underexposed images and high-level semantic guidance.",
                "distance": 0.0728
            },
            {
                "reference": "Low-light image enhancement is an ill-posed problem with multiple possible enhanced versions for a given image. However, existing studies mostly aim to build a deterministic mapping from the input to a single enhanced version.",
                "distance": 0.0865
            },
            {
                "reference": "Currently, estimating the Normalized Difference Vegetation Index (NDVI) usually requires imaging from the visible spectral band together with actual near infrared (NIR) images, which may be not available or practical for all applications.",
                "distance": 0.0884
            }
        ]
    },
    {
        "target": "Recent methods use fusion of RGB images and point clouds method to jointly estimate 2D optical flow and 3D scene flow. Existing approaches face limitations due to the low sampling rates of conventional RGB cameras and LiDAR sensors, especially in highly-dynamic scenes.",
        "prediction": "The existing methods employed for analyzing movements of objects and mobile robots from a sequence of stereo frames present challenges when used in unknown environments with multiple moving rigid objects.",
        "queries": [
            21.084019,
            0.341552
        ],
        "log": [
            {
                "reference": "The existing methods employed for analyzing movements of objects and mobile robots from a sequence of stereo frames present challenges when used in unknown environments with multiple moving rigid objects.",
                "distance": 0.0407
            },
            {
                "reference": "Current methods for automated 3D navigation rely on optical flow information, 3D reconstruction, segmentation, feature tracking or pre-processing, and are heavily dependent on the texture of the 3D surface.",
                "distance": 0.0519
            },
            {
                "reference": "The task of estimating surface and motion parameters of a free-flying object in a telerobotics experiment using multi-sensor range data is considered, yet the existing systems may not be efficient in handling sparse data.",
                "distance": 0.0608
            },
            {
                "reference": "The study examines the inconsistencies and failures in human perception of structure from motion, a topic that has not previously been explored.",
                "distance": 0.0706
            },
            {
                "reference": "The need for effective compression and indexing of image sequences had not been efficiently addressed.",
                "distance": 0.0752
            },
            {
                "reference": "The current work on multi-body factorization separates between objects whose motions are independent, but often merges objects with different 3D motions into a single object.",
                "distance": 0.0767
            },
            {
                "reference": "Estimating the motion and underlying physical parameters of a rigid body in free flight from a video is a complex task.",
                "distance": 0.0786
            },
            {
                "reference": "Prior work in structure from motion analysis often requires certain assumptions about the objects and their motions.",
                "distance": 0.0863
            }
        ]
    },
    {
        "target": "In visual-based Reinforcement Learning (RL), agents often struggle to generalize well to environmental variations in the state space that were not observed during training. Due to the inherent correlations among features in the state space, the associations between features and decisions become entangled, making it difficult for the policy to distinguish them.",
        "prediction": "In recent years, extensive research has been carried out in reinforcement learning with the aim of teaching computer programs to perform tasks such as playing video games. The Arcade Learning Environment (ALE) has become a commonly used benchmark for allowing algorithms to train on various Atari 2600 games.",
        "queries": [
            8.570192,
            -22.78142
        ],
        "log": [
            {
                "reference": "In recent years, extensive research has been carried out in reinforcement learning with the aim of teaching computer programs to perform tasks such as playing video games. The Arcade Learning Environment (ALE) has become a commonly used benchmark for allowing algorithms to train on various Atari 2600 games.",
                "distance": 0.0056
            },
            {
                "reference": "Deep Reinforcement Learning (DRL) methods have shown proficiency in ATARI games, outperforming humans in many. However, understanding why DRL performs better on some games than others is not fully understood.",
                "distance": 0.0095
            },
            {
                "reference": "Deep reinforcement learning has been successful in decision-making for sophisticated games, such as Atari and Go. However, real-world decision-making often requires reasoning with partial information extracted from complex visual observations. Current reinforcement learning has a limitation when it comes to dealing with complex partial observations.",
                "distance": 0.0146
            },
            {
                "reference": "The sample-complexity of Reinforcement Learning (RL) techniques still represents a challenge for scaling up RL to unsolved domains. Limited advice is usually used to help learn faster from a demonstrator's policy.",
                "distance": 0.025
            },
            {
                "reference": "While reinforcement learning (RL) has seen significant advances in solving a single problem in a given environment, learning policies that generalize to unseen variations of the same problem remains a challenge.",
                "distance": 0.0379
            },
            {
                "reference": "Reinforcement-learning (RL) algorithms are traditionally fine-tuned for specific environments, highlighting an issue about their real autonomy.",
                "distance": 0.0696
            },
            {
                "reference": "Reinforcement learning (RL) has been successfully applied to recommender systems, but these existing RL-based recommendation methods are limited by their unstructured state/action representations.",
                "distance": 0.0751
            },
            {
                "reference": "While self-play algorithms where the agent learns by playing against itself have proven to be effective in the realm of practical reinforcement learning, their effectiveness remains largely unexplored in theoretical analysis. This observation is particularly valid in scenarios requiring to manage the tradeoff between exploration and exploitation, which predominantly apply principles from reinforcement learning.",
                "distance": 0.0758
            },
            {
                "reference": "Reinforcement learning algorithms are often tested in relatively simple, static environments with rigid objects, and their performance in dynamic environments with deformable and topologically complex objects is not well studied.",
                "distance": 0.0766
            },
            {
                "reference": "The problem of predicting and controlling the future state distribution of an autonomous agent is a reframing of goal-conditioned reinforcement learning (RL), which is usually centered around learning a conditional probability density function over future states.",
                "distance": 0.0784
            },
            {
                "reference": "Existing deep reinforcement learning-based methods for discovering dynamic treatment regimes suffer from limitations such as compounding errors in supervised methods, self-defined reward signals that are either too sparse or need clinical guidance, and overemphasis on positive trajectories leaving negative trajectories largely ignored.",
                "distance": 0.0805
            },
            {
                "reference": "Deep reinforcement learning has demonstrated potential for improving system performance, primarily in static environments. However, performance can degrade significantly in dynamic environments due to high-variance and biased estimation of reward.",
                "distance": 0.0846
            },
            {
                "reference": "Reinforcement learning (RL) from visual observations is a challenging issue due to problems with sample efficiency of learning and generalization to new environments.",
                "distance": 0.0848
            },
            {
                "reference": "Most recent advancements in deep reinforcement learning take an RL-centric perspective, focusing on the refinement of training objectives.",
                "distance": 0.0854
            },
            {
                "reference": "While there are environments for assessing particular open problems in reinforcement learning (RL), it can be challenging to extend these in more complex environments. Existing benchmarks widely used by the community are not typically designed for evaluating specific capabilities of RL methodologies.",
                "distance": 0.0868
            },
            {
                "reference": "While Deep Reinforcement Learning (DRL) has emerged as a promising approach to many complex tasks, it remains challenging to train a single DRL agent that is capable of undertaking multiple different continuous control tasks.",
                "distance": 0.0901
            },
            {
                "reference": "Reinforcement Learning algorithm advancements heavily rely on the development of challenging environments that test the limits of existing techniques. Current RL environments are either sufficiently complex or based on swift simulation, but seldom both.",
                "distance": 0.0955
            },
            {
                "reference": "Applications of Reinforcement Learning (RL) in robotics are limited by high data demand. Furthermore, model-based planning approaches in many robotics scenarios often suffer in performance if the model is imprecise or wrong.",
                "distance": 0.0984
            }
        ]
    },
    {
        "target": "Previous works on traffic scene understanding have achieved great success at the low-level perception stage, such as road segmentation and lane detection; however, few concern high-level understanding.",
        "prediction": "Lane detection is a crucial aspect of visual perception in autonomous driving, and currently, heavily relies on purely CNN-based architectures. These architectures often fail in incorporating relations of long-range lane points and global contexts induced by surrounding objects, such as pedestrians and vehicles.",
        "queries": [
            11.738741,
            1.86325
        ],
        "log": [
            {
                "reference": "Lane detection is a crucial aspect of visual perception in autonomous driving, and currently, heavily relies on purely CNN-based architectures. These architectures often fail in incorporating relations of long-range lane points and global contexts induced by surrounding objects, such as pedestrians and vehicles.",
                "distance": 0.0298
            },
            {
                "reference": "Self-driving cars are required to accurately detect vehicles, pedestrians, and other traffic participants to operate safely. However, detecting small, far-away, or highly occluded objects is a challenge due to limited information in the LiDAR point clouds for these objects.",
                "distance": 0.0325
            },
            {
                "reference": "Training deep models for lane detection is a challenge due to the subtlety and sparsity of supervisory signals in lane annotations. These models often fail in difficult scenarios like severe occlusion, ambiguous lanes, and poor light conditions, as learning from a richer context is required.",
                "distance": 0.0337
            },
            {
                "reference": "Vehicle detection with visual sensors, such as lidar and camera, is vital for autonomous driving. However, these sensors fail in adverse weather conditions like fog, where opaque particles distort lights and significantly reduce visibility.",
                "distance": 0.0445
            },
            {
                "reference": "Lane detection, a key task in self-driving, is challenging due to complex scenarios like severe occlusion and ambiguous lanes, coupled with sparse supervisory signals inherent in lane annotations. Ordinary convolutional neural networks (CNNs) struggle to train in general scenes and capture subtle lane features from raw images.",
                "distance": 0.0463
            },
            {
                "reference": "Camera-based lane detection algorithms are crucial for semi-autonomous and fully autonomous systems like self-driving cars. However, the lane detection process is typically encumbered with tedious preprocessing and computational effort.",
                "distance": 0.0485
            },
            {
                "reference": "Current object detection methods in autonomous driving assume common traffic categories and struggle to detect uncommon objects and corner cases, which can lead to accidents. The development of reliable self-driving systems is impeded due to a lack of public datasets for evaluating detector performance on these corner cases.",
                "distance": 0.0516
            },
            {
                "reference": "The motivation for this study is the problem of lane detection in the Lane Departure Warning (LDW) system.",
                "distance": 0.0573
            },
            {
                "reference": "Autonomously backing up a vehicle towards a trailer using a self-estimating computer vision system for 3D trailer coupler position alignment is a challenging task that has not been efficiently solved.",
                "distance": 0.0607
            },
            {
                "reference": "Self-driving cars use multi-camera systems for motion estimation, but extracting the full relative motion including metric scale has been a challenge with existing methods.",
                "distance": 0.0611
            },
            {
                "reference": "Driver assistance systems based on camera are strongly disturbed by the presence of foggy weather, undermining the performance of these systems.",
                "distance": 0.0688
            },
            {
                "reference": "Lane extraction is a crucial task for autonomous driving and while there have been major advances with deep learning models, they target ordinary RGB images generated by frame-based cameras, limiting their performance.",
                "distance": 0.069
            },
            {
                "reference": "Existing approaches for lane detection, which are key in advanced driver assistance systems, are categorized as lane area segmentation and lane boundary detection. However, most of these methods do not utilize a large amount of complementary information such as geometric priors, which are discarded in the alternative use of lane area and lane boundaries.",
                "distance": 0.0704
            },
            {
                "reference": "Overtaking accidents often occur due to limited visibility, often caused by occlusion from the vehicle in front. With the proliferation of digital cameras in vehicles, there's potential to mitigate these incidences.",
                "distance": 0.0718
            },
            {
                "reference": "Lane detection is a critical and challenging task in autonomous driving which requires accurately predicting lane lines and distinguishing between different types of lanes. Recently, some works frame lane detection as a keypoint estimation problem which needs to group adjacent keypoints belonging to the same lane line in a point-by-point manner, which results in inefficient and time-consuming post-processing.",
                "distance": 0.0718
            },
            {
                "reference": "After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation based on accuracy and F1 score have become the standard to measure the performance of lane detection methods. However, the validity of this evaluation method in down-stream tasks has not been adequately researched.",
                "distance": 0.0749
            },
            {
                "reference": "Existing road detection techniques for autonomous navigation can be limited by illumination conditions, requiring day and night capabilities.",
                "distance": 0.077
            },
            {
                "reference": "Vehicle detection plays an important role in traffic control at signalised intersections, and a solution based on vision-based sensors is needed.",
                "distance": 0.0811
            },
            {
                "reference": "There's a challenge in bridging domain discrepancy in lane detection among different scenarios to reduce additional annotation and re-training costs for autonomous driving. Conventional methods only focus on pixel-wise loss while ignoring shape and position priors of lanes, which hinders performance improvement of cross-domain lane detection.",
                "distance": 0.0898
            },
            {
                "reference": "Lane detection is a critical method for autonomous systems. Numerous approaches have been presented, however, the challenge is not fully resolved as most techniques either consider lane detection as a dense prediction or a detection task, disregarding the distinctive lane marker topologies.",
                "distance": 0.0914
            }
        ]
    },
    {
        "target": "Modern detection transformers (DETRs) rely on confidence scores for predicting bounding boxes and selecting the top-ranked predictions, but they often struggle due to misalignment between classification scores and localization accuracy.",
        "prediction": "DETR (DEtection TRansformer) models typically struggle with slow training convergence and the process of improving the query-to-feature similarity can be quite complex.",
        "queries": [
            15.199677,
            -1.209645
        ],
        "log": [
            {
                "reference": "DETR (DEtection TRansformer) models typically struggle with slow training convergence and the process of improving the query-to-feature similarity can be quite complex.",
                "distance": 0.0025
            },
            {
                "reference": "DETR, translating the image feature map directly into the object detection result, has pioneered the solution of vision tasks using transformers. However, it can be costly because of redundant computation on certain areas of the image like the background.",
                "distance": 0.0134
            },
            {
                "reference": "State-of-art detection transformer (DETR) models processes comprehensive object detection tasks but suffer from complexity, making them slow and not suitable for real-time applications where inference speed is critical.",
                "distance": 0.028
            },
            {
                "reference": "The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. However, the existing approach suffers from slow training convergence due to its high dependency on content embeddings for localizing objects and predicting box locations.",
                "distance": 0.0304
            },
            {
                "reference": "Detection with Transformer (DETR) is becoming popular but its global attention modeling requires a very long training period to optimize and achieve good detection performance. Existing studies mainly develop advanced feature or embedding designs to tackle the training issue.",
                "distance": 0.0311
            },
            {
                "reference": "In the domain of object detection, DETR poses limitations including small feature resolution and slow training convergence, due to the quadratic computational complexity of the self-attention module in Transformer encoders and difficulty of learning.",
                "distance": 0.0373
            },
            {
                "reference": "DETR, a Transformer-based method, reimagines object detection as a set prediction problem. Despite achieving state-of-the-art performance, DETR is plagued by slow convergence due to a long training time.",
                "distance": 0.0423
            },
            {
                "reference": "DEtection TRansformer (DETR) is a method that simplifies the object detection pipeline by eliminating complex anchor generation and post-processing. However, DETR models have many redundant parameters, making them computationally and storage intensive and unsuitable for deployment on resource-constrained devices.",
                "distance": 0.0515
            },
            {
                "reference": "The recently proposed Detection Transformer (DETR) model successfully applies the Transformer to object detection but suffers from slow convergence, requiring 500 epochs to achieve high accuracy.",
                "distance": 0.0541
            },
            {
                "reference": "DETR has been recently proposed for object detection, eliminating the need for many hand-designed components while demonstrating good performance. However, its limitations include slow convergence and limited feature spatial resolution because of the limitations of Transformer attention modules in processing image features maps.",
                "distance": 0.0566
            },
            {
                "reference": "Transformer-based detectors such as DETR have been developed for object detection, composed of a CNN-based backbone and paired transformer encoder-decoder for local feature extraction and global context capture respectively. However, these models are not concise and still rely on prior knowledge driven components like anchors, proposals, and the NMS.",
                "distance": 0.0578
            },
            {
                "reference": "DETR demonstrates competitive performance but low computational efficiency on high resolution feature maps. Deformable DETR enhances the efficiency of DETR using deformable attention, but the number of encoder tokens increases significantly, and the computation cost of the encoder attention remains a bottleneck.",
                "distance": 0.0725
            },
            {
                "reference": "The recently proposed DEtection TRansformer (DETR) has shown promising performance in end-to-end object detection, but has limitations with detection of small objects and slow convergence.",
                "distance": 0.0777
            }
        ]
    },
    {
        "target": "A supervised embedding for a machine learning task should ideally be sensitive to changes in the label of interest and invariant to other confounding factors.",
        "prediction": "Existing approaches to the zero-shot multi-label text classification (ZS-MTC) problem leverage label hierarchies. Despite the success of pretrained models like BERT in transforming classification tasks into textual entailment tasks, they are not frequently used in this context as they don't generate individual vector representations for text or labels, making it difficult to combine them with conventional graph encoding methods.",
        "queries": [
            -1.401745,
            -9.68199
        ],
        "log": [
            {
                "reference": "Existing approaches to the zero-shot multi-label text classification (ZS-MTC) problem leverage label hierarchies. Despite the success of pretrained models like BERT in transforming classification tasks into textual entailment tasks, they are not frequently used in this context as they don't generate individual vector representations for text or labels, making it difficult to combine them with conventional graph encoding methods.",
                "distance": 0.064
            },
            {
                "reference": "Large multi-label datasets often contain labels that occur frequently, infrequently (few-shot group), or never appear (zero-shot group). Prior work has not extensively explored multi-label few- and zero-shot label prediction on datasets with large label spaces, especially for text classification.",
                "distance": 0.1234
            }
        ]
    },
    {
        "target": "Dataset distillation aims to create a smaller dataset that can yield comparable test performance to a full dataset using the same model. Current state-of-the-art methods optimize synthetic datasets mainly by matching heuristic indicators extracted from two networks, one trained on real data and the other on synthetic data. However, these methods lack well-defined metrics to measure the amount of shared information between the synthetic and the real datasets.",
        "prediction": "Dataset distillation, which aims to learn a small synthetic dataset preserving most of the information from the original dataset, is a challenging task due to significant computation and memory costs introduced by differentiating through the inner loop learning procedure.",
        "queries": [
            3.077193,
            -8.386568
        ],
        "log": [
            {
                "reference": "Dataset distillation, which aims to learn a small synthetic dataset preserving most of the information from the original dataset, is a challenging task due to significant computation and memory costs introduced by differentiating through the inner loop learning procedure.",
                "distance": 0.0448
            },
            {
                "reference": "A major problem when developing neural networks or machine diagnostics situations is that no data or very little data is available for training on fault conditions, yet domain experts often have good knowledge on expected input and output parameter values.",
                "distance": 0.0572
            },
            {
                "reference": "Dataset condensation aims at reducing the network training effort by condensing a large training set into a compact synthetic one. State-of-the-art approaches mostly learn the synthetic data by matching the gradients between the real and synthetic data batches, but these methods often overfit to a biased set of samples that produce dominant gradients, leading to a lack of global supervision of data distribution.",
                "distance": 0.0733
            },
            {
                "reference": "Current transfer learning methods could be improved in efficiency and effectiveness.",
                "distance": 0.074
            },
            {
                "reference": "Neural trees strive to integrate the benefits of deep neural networks and decision trees, including representation learning and faster inference respectively. However, existing approaches either use a predefined structure or grow hierarchical layers progressively.",
                "distance": 0.0775
            },
            {
                "reference": "Recent methods for Dataset Distillation can condense a large set of images of a specific class into a single image that can be used to train a classifier with similar performance to the original dataset.",
                "distance": 0.0905
            }
        ]
    },
    {
        "target": "The authors study indiscriminate poisoning for linear learners, where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. The authors observe that linear learners on some datasets can resist the best known attacks even without any defenses.",
        "prediction": "Most data mining algorithms assume that the data generation process is independent of the data miner's activities. However, in many domains like spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, the data is manipulated by an adversary seeking to make the classifier produce false negatives which often results in rapid degradation of a classifier's performance after it is deployed.",
        "queries": [
            -3.541205,
            -21.611933
        ],
        "log": [
            {
                "reference": "Most data mining algorithms assume that the data generation process is independent of the data miner's activities. However, in many domains like spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, the data is manipulated by an adversary seeking to make the classifier produce false negatives which often results in rapid degradation of a classifier's performance after it is deployed.",
                "distance": 0.0091
            },
            {
                "reference": "Data poisoning attacks modify a subset of training examples by small adversarial perturbations, changing the prediction of certain test-time data. Existing defense mechanisms are often ineffective in practice; they either significantly harm generalization performance, are attack-specific, or are too slow to apply.",
                "distance": 0.0347
            },
            {
                "reference": "Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems where an adversary injects few correctly-labeled, minimally-perturbed samples into the training data, leading to model misclassification. While defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated.",
                "distance": 0.0503
            },
            {
                "reference": "Availability attacks poison the training data with perturbations, making the data less exploitable by machine learning algorithms in an effort to safeguard unauthorized data use. Understanding why these perturbations function effectively remains unexplored.",
                "distance": 0.0556
            },
            {
                "reference": "The rise of intelligent agents equipped with machine learning capabilities, such as spam filters and robots, has opened up possibilities for a new type of security threat, called training-set attacks. Such attacks occur when an attacker manipulates the training data, generating a model profitable for the attacker.",
                "distance": 0.0638
            },
            {
                "reference": "Machine Learning (ML) models are increasingly being used for various tasks in real-world applications. An area of relevance that has not been extensively explored is the ability for an adversary to steal the functionality of these models solely based on black-box interactions.",
                "distance": 0.0646
            },
            {
                "reference": "Existing data poisoning techniques in machine learning models, where adversarial goals are achieved at test time through data injection during training, rely on certain assumptions about adversary knowledge and capability, which may not always be practical.",
                "distance": 0.0783
            },
            {
                "reference": "Automated learning and decision making systems used in public-facing applications are vulnerable to malicious attacks. Many of these systems can be manipulated by attackers to alter predictions to their own benefit, a risk that is even more pronounced when financial stakes are high.",
                "distance": 0.0951
            },
            {
                "reference": "Although there has been extensive research on the robustness of machine learning systems to malicious data attacks when attackers have complete knowledge, less is known about the potential effects of 'blind attackers' who have very limited information about the learner's training set or the distribution from which it is drawn.",
                "distance": 0.0972
            }
        ]
    },
    {
        "target": "Nonconvex-nonconcave minimax optimization is important in machine learning, and most existing algorithms depend on regularity conditions such as the convexity (resp. concavity) of the primal (resp. dual) functions, or other structures, including the Polyak-\u0141ojasiewicz (P\u0141) and Kurdyka-\u0141ojasiewicz (K\u0141) conditions. These conditions can be difficult to verify in practice.",
        "prediction": "Optimization problems that are parameterized concavely in a single parameter are common. However, there is a need for solutions to efficiently approximate these problem types.",
        "queries": [
            5.906266,
            -14.049829
        ],
        "log": [
            {
                "reference": "Optimization problems that are parameterized concavely in a single parameter are common. However, there is a need for solutions to efficiently approximate these problem types.",
                "distance": 0.004
            },
            {
                "reference": "Previous studies have been trying to find the optimal high-order algorithm for solving smooth convex minimization problems. Those state-of-the-art high-order methods achieved the oracle complexity that didn't match the lower bound as they relied on a complex binary search procedure, making them sub-optimal and impractical.",
                "distance": 0.0194
            },
            {
                "reference": "Existing approaches to rank-constrained convex optimization have limitations when dealing with sparse convex optimization and rank-constraint convex optimization for smooth functions.",
                "distance": 0.0252
            },
            {
                "reference": "The best known iteration complexity for solving a non-smooth, linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex, is O(e\u22121).",
                "distance": 0.0254
            },
            {
                "reference": "Classical algorithms for optimizing approximately convex functions can be slow, and efforts to improve their speed and efficiency have so far not achieved desired results.",
                "distance": 0.035
            },
            {
                "reference": "Current methods to solve non-smooth optimization problems with a non-smooth term having explicit max-structure and a smooth term or a simple non-smooth term have a best known iteration complexity of O(1/e) without assuming strong convexity.",
                "distance": 0.0364
            },
            {
                "reference": "Function fitting problems are common in convex analysis, but the specific problem referred to as first-order convex fitting (FCF) has been overlooked in previous literature.",
                "distance": 0.0387
            },
            {
                "reference": "Minimizing a smooth function with bounded constraints has been a challenge, often leading to a situation where workers have to idle and wait for other cores when executed in a parallel setting.",
                "distance": 0.0403
            },
            {
                "reference": "Convex optimization has emerged as a useful tool for various applications such as data analysis, model fitting, resource allocation, engineering design, network design and optimization, finance, and control and signal processing.",
                "distance": 0.0436
            },
            {
                "reference": "Prior optimization algorithms were synchronous, focused on individual coordinate-wise constraints or smooth and strongly-convex objectives, and lacked finite-time data-dependent convergence guarantees for generic convex constraints.",
                "distance": 0.0462
            },
            {
                "reference": "Existing methods for minimizing smooth unconstrained or separably constrained functions often have limitations in their parallelism and efficiency.",
                "distance": 0.0481
            },
            {
                "reference": "Minimizing a non-convex function that can be split into a sum of simple functions is a common problem in computer vision, for example, the non-convex problem of binary image segmentation based on Euler's Elastica.",
                "distance": 0.049
            },
            {
                "reference": "Composite convex minimization is a broadly applicable problem template, but achieving optimal convergence rates can be challenging, particularly for certain types of complex problems such as semidefinite programming.",
                "distance": 0.0537
            },
            {
                "reference": "Convex optimization is an important tool in machine learning and data mining. However, scalability remains a challenge for general convex optimization solvers, and specialized solvers only address a narrow range of problems, necessitating the need for simple and scalable algorithms for various optimization problems.",
                "distance": 0.0542
            },
            {
                "reference": "Existing algorithms for linearly constrained convex programming, where neither the primal nor the dual function has to be smooth or strongly convex, achieve a non-smooth problem-solving iteration complexity of $\\mathcal{O}(eps^{-1})$.",
                "distance": 0.0557
            },
            {
                "reference": "Nonconvex optimization with nonlinear constraints is a prevalent challenge in computational mathematics, and existing complexity results are based on specific conditions like the Polyak-Lojasiewicz and Mangasarian-Fromowitz conditions.",
                "distance": 0.0587
            },
            {
                "reference": "Previous optimization algorithms struggle to minimize the sum of two non-necessarily convex functions; one of which is proper lower semi-continuous, and the other is differentiable with a Lipschitz continuous gradient.",
                "distance": 0.0606
            },
            {
                "reference": "The need to solve nonconvex, nonsmooth optimization problems in both parallel and asynchronous environments remains a challenge.",
                "distance": 0.061
            },
            {
                "reference": "Stochastic first-order algorithms for minimax optimization under Polyak-{\\L}ojasiewicz (PL) conditions are usually solved by state-of-the-art methods with stochastic first-order oracle (SFO) upper bound as ${\\mathcal O}\big((n + n^{2/3}\\kappa_x\\kappa_y^2)\\log (1/\\epsilon)\big)$, but they may not be efficient.",
                "distance": 0.0614
            },
            {
                "reference": "The task of approximating an arbitrary convex function arises in several learning problems and current state-of-the-art approaches for solving this task are relatively slow.",
                "distance": 0.0655
            }
        ]
    },
    {
        "target": "Distant supervision has been widely used in NLP tasks such as named entity recognition, but the noisy distant labels tagged unsupervisedly pose a major obstacle. Some previous works have addressed this problem by using a self-training framework with a sample-selection mechanism, but existing methods have tended to overlook certain types of biases.",
        "prediction": "Distant supervision methods typically require centralized training; that is, collecting texts from different sources and storing on a single machine. This setup is hindered by problems associated with data barriers and privacy protection, thereby making it difficult or cost-prohibitive to centralize data from multiple platforms.",
        "queries": [
            -3.166848,
            3.470782
        ],
        "log": [
            {
                "reference": "Distant supervision methods typically require centralized training; that is, collecting texts from different sources and storing on a single machine. This setup is hindered by problems associated with data barriers and privacy protection, thereby making it difficult or cost-prohibitive to centralize data from multiple platforms.",
                "distance": 0.0367
            },
            {
                "reference": "Distant labeling for information extraction (IE) suffers from noisy training data.",
                "distance": 0.0371
            },
            {
                "reference": "Distant supervision is used for creating relation classifiers in the absence of labelled data. These techniques are often evaluated using a held-out portion of the distantly labelled data, thereby avoiding the need for labelled data entirely. However, it is difficult to determine the true accuracy of these systems as they are tested against noisy data.",
                "distance": 0.0464
            },
            {
                "reference": "Distant Supervision is widely used for training relation extraction models. It generates noisy training samples by heuristically labeling a corpus using an existing knowledge base. Previous noise reduction methods for distant supervision fail to utilize information such as data credibility and sample confidence.",
                "distance": 0.057
            },
            {
                "reference": "Distant supervision (DS) is a learning method used for extracting relational facts from a text corpus. However, the accuracy of current approaches is not satisfactory and certain key factors, such as valid entity type detection, negative training examples construction and ensembles, greatly influence accuracy.",
                "distance": 0.0684
            },
            {
                "reference": "The fundamental problem of distant supervision is the noisy training corpus problem.",
                "distance": 0.0711
            },
            {
                "reference": "Distant supervision (DS) is widely used for generating labeled data for information extraction (IE) tasks, but it suffers from noisy label problems as labels are extracted from the knowledge base (KB) without considering the input context. Existing denoising mechanisms are usually designed for specific tasks and cannot be directly adapted to other tasks.",
                "distance": 0.0801
            },
            {
                "reference": "Distant supervision reduces human efforts in building training data for many classification tasks, but this technique often introduces noise to the generated training data, which can severely affect the model performance.",
                "distance": 0.0849
            },
            {
                "reference": "Distant supervision is a training method for information extraction systems that uses existing knowledge bases to heuristically label a training corpus. However, previous work has not addressed the issue of false negative training examples mislabeled due to the incompleteness of these knowledge bases.",
                "distance": 0.0864
            },
            {
                "reference": "Distant Supervision has become an effective way to train supervised relation extractors by using external databases to generate training data. However, this approach inherently suffers from noise in the training data.",
                "distance": 0.0928
            },
            {
                "reference": "In many disciplines, the PART-WHOLE relationship is imperative, and current algorithmic works primarily focus on predicting outcomes of the whole and parts using separate or linear joint models, assuming that the outcome of the parts has an independent effect on that of the whole.",
                "distance": 0.0976
            },
            {
                "reference": "Existing bootstrapping methods like co-training and scoped-learning for combining labelled and unlabelled data suffer from low precision and recall in information extraction tasks, especially when few labelled items are used as seed data.",
                "distance": 0.0989
            }
        ]
    },
    {
        "target": "The current state of re-identification (ReID) models poses limitations, as they are primarily designed and trained for specific categories such as person or vehicle. Existing models cannot address tracking wildlife populations and migration patterns.",
        "prediction": "Existing unsupervised person re-identification (Re-ID) methods use clustering to create pseudo labels for training. However, clustering can mix up different identities or split the same identity into multiple sub clusters, negatively affecting Re-ID accuracy.",
        "queries": [
            20.75655,
            10.852251
        ],
        "log": [
            {
                "reference": "Existing unsupervised person re-identification (Re-ID) methods use clustering to create pseudo labels for training. However, clustering can mix up different identities or split the same identity into multiple sub clusters, negatively affecting Re-ID accuracy.",
                "distance": 0.0376
            },
            {
                "reference": "Existing unsupervised methods for person re-identification (re-ID) often rely on the use of pseudo labels obtained from video tracklets or clustering. This can result in accumulated errors and challenges in estimating the number of pseudo IDs.",
                "distance": 0.0435
            },
            {
                "reference": "Most person re-identification (Re-ID) approaches require a large amount of pre-labelled data to be put into training all at once, which is not applicable in most real-world deployment of the Re-ID task.",
                "distance": 0.0457
            },
            {
                "reference": "Most person re-identification (re-ID) approaches are based on supervised learning, which requires intensive manual annotation for training data, making it resource-intensive and impractical for large-scale real-world data.",
                "distance": 0.0466
            },
            {
                "reference": "Person re-identification (re-id) is often solved using learning procedures. However, most databases used for re-id are small, leading to the potential for over-fitting.",
                "distance": 0.0475
            },
            {
                "reference": "Unsupervised person re-identification (re-ID) relies on label estimation, specifically focusing on cross-camera label estimation which is used in feature learning for creating robust re-ID models. However, labels generated by existing graph matching methods can be inaccurate and noisy due to significant cross-camera variations.",
                "distance": 0.0475
            },
            {
                "reference": "State-of-the-art methods for person re-identification (ReID) typically use complex network structures and concatenate multi-branch features. Some training tricks have been outlined only briefly in several papers or source codes but have not been gathered or evaluated systematically.",
                "distance": 0.0495
            },
            {
                "reference": "Previous methods for the purely unsupervised person re-identification (Re-ID) problem use clustering techniques to generate pseudo labels and train Re-ID models progressively. But, these methods neglect the large intra-ID variance caused mainly by the change of camera views.",
                "distance": 0.0502
            },
            {
                "reference": "Person re-identification (reID) retrieval problems typically involve convolutional neuron networks (CNN) where weight vectors within a fully connected (FC) layer act as a projection basis. However, these weight vectors are usually highly correlated, leading to correlations among entries of the FC descriptor, which can compromise retrieval performance based on the Euclidean distance.",
                "distance": 0.0509
            },
            {
                "reference": "Person re-identification (re-id) is challenging due to significant intra-class variations across different cameras. Traditional methods use generative models to augment training data, but the generative pipelines remain separate from the discriminative re-id learning stages.",
                "distance": 0.0531
            },
            {
                "reference": "Unsupervised person re-identification (Re-ID) is challenging due to the lack of ground-truth labels. Existing methods often rely on estimated pseudo labels via iterative clustering and classification, but they are susceptible to performance penalties incurred by the inaccurate estimated number of clusters.",
                "distance": 0.0537
            },
            {
                "reference": "Existing person re-identification (Re-ID) methods require all training data in advance, whereas real-world Re-ID data are inherently captured over time or from different locations, requiring a model to incrementally learn from new data without forgetting what it has already learned.",
                "distance": 0.0539
            },
            {
                "reference": "Existing person re-identification (ReID) methods use hashing algorithms to learn compact binary codes and perform fast Hamming distance and counting sort. However, these algorithms need a very long code to achieve high accuracy, which compromises search speed.",
                "distance": 0.0541
            },
            {
                "reference": "Person re-identification (re-id) has attracted a lot of attention in recent years, but the enormous number of possible triplet data among the large number of training samples makes the training impossible.",
                "distance": 0.0551
            },
            {
                "reference": "Unsupervised person re-identification (RE-ID) is increasingly being researched due to its potential to address the scalability problem of supervised RE-ID models. However, it is challenging to learn discriminative information without pairwise labels across disjoint camera views.",
                "distance": 0.0551
            },
            {
                "reference": "Person re-identification (re-ID) is critical in retrieval processes, where re-ranking is a necessary step for accuracy improvement. However, there is limited effort in the re-ID community to improve re-ranking, especially with fully automatic, unsupervised solutions.",
                "distance": 0.0566
            },
            {
                "reference": "Unsupervised person re-identification (re-ID) is relevant in real-world systems like public security and intelligent transportation systems, however it's challenged by a data distribution discrepancy across cameras and lack of label information.",
                "distance": 0.0574
            },
            {
                "reference": "In person re-identification (re-ID), feature representation through convolutional neural networks is key, although single-path and single-loss networks have limited efficacy due to the learning objective only achieving one of multiple minima.",
                "distance": 0.0593
            },
            {
                "reference": "Person re-identification (ReID) is a challenging problem addressed by many metric learning approaches, among which triplet loss is one of the state-of-the-art methods. However, the exploration of the margin between positive and negative pairs of triplets has not been sufficiently addressed.",
                "distance": 0.0598
            },
            {
                "reference": "Despite the great advances made in person re-identification (ReID) using Convolutional Neural Networks, current ReID models only output a scalar distance between two persons and provide little in the way of a semantic understanding or explanation as to why two individuals are deemed the same or not.",
                "distance": 0.061
            }
        ]
    },
    {
        "target": "The Proximal Policy Optimization (PPO) algorithm, a popular policy learning method, typically does not incorporate analytical gradients from differentiable environments.",
        "prediction": "Actor-Critic (AC) methods are extensively used in reinforcement learning and are known to be closely related to policy gradient (PG) methods, but the exact relationship and differences between these two approaches have not been fully explored or characterized.",
        "queries": [
            9.516253,
            -21.257833
        ],
        "log": [
            {
                "reference": "Actor-Critic (AC) methods are extensively used in reinforcement learning and are known to be closely related to policy gradient (PG) methods, but the exact relationship and differences between these two approaches have not been fully explored or characterized.",
                "distance": 0.0048
            },
            {
                "reference": "Modern policy gradient algorithms, such as Proximal Policy Optimization (PPO), use a number of heuristics including loss clipping and gradient clipping to ensure successful learning. These techniques, however, often resemble methods from robust statistics which are used in outlier-rich environments.",
                "distance": 0.0136
            },
            {
                "reference": "Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. Existing actor-critic methods only use values or gradients of the critic to update the policy parameter.",
                "distance": 0.0248
            },
            {
                "reference": "Policy gradient methods are widely used in control and reinforcement learning. However, existing convergence analysis relies on non-intuitive, impractical, and often opaque conditions and is achieved under strict regularity conditions in limited settings.",
                "distance": 0.0283
            },
            {
                "reference": "Policy Optimization (PO) is a widely used approach to address continuous control tasks, but traditional methods do not reuse samples generated by one policy to estimate the performance of other policies.",
                "distance": 0.0349
            },
            {
                "reference": "Policy gradient methods are a prevalent class of model-free reinforcement learning algorithms, and a state-dependent baseline is used to reduce gradient estimator variance. Some recent research has proposed extending the baseline to depend on both the state and action, arguing that it reduces variance and boosts sample efficiency without introducing bias into the gradient estimates.",
                "distance": 0.0409
            },
            {
                "reference": "Policy gradient-based methods used in continuous control are limited by the need for the agent's underlying probability distribution which limits policy representation to parametric distribution classes. This limitation results in local movement in the action space and can lead to the convergence of sub-optimal solutions.",
                "distance": 0.0414
            },
            {
                "reference": "Actor-critic is one of the most popular families of reinforcement learning algorithms. However, existing works on actor-critic primarily focus on two-timescale updates where the actor and critic are updated in separate steps.",
                "distance": 0.0443
            },
            {
                "reference": "Policy gradient methods are reinforcement learning algorithms that update by following a performance gradient estimate. Current methodologies use Monte Carlo techniques for this estimation. These methods, however, tend to have high variance and therefore require a large number of samples which leads to slow convergence.",
                "distance": 0.0451
            },
            {
                "reference": "The problem of temporal credit assignment in reinforcement learning is typically resolved using Q-learning and other algorithms based on the methods of temporal differences (TD(\u03bb)). However, using TD-based algorithms with \u03bb > 0 often allows faster credit propagation, it also involves certain implementational problems such as lack of generality and computational inefficiency.",
                "distance": 0.0485
            },
            {
                "reference": "The hierarchical interaction between the actor and critic in actor-critic based reinforcement learning algorithms naturally lends itself to a game-theoretic interpretation, but the usual individual gradient approach may lead to issues with cycling and slower convergence.",
                "distance": 0.0492
            },
            {
                "reference": "In reinforcement learning, assuring convergence when using a compact hypothesis class for value function approximation is a lingering issue. The standard temporal-difference learning algorithm converges when the hypothesis class is a linear compilation of fixed basis functions, but may diverge with a non-linear (general) hypothesis class.",
                "distance": 0.0509
            },
            {
                "reference": "Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. The statistically efficient estimation of policy gradients from off-policy data is a challenging task.",
                "distance": 0.052
            },
            {
                "reference": "Policy gradient methods are among the most effective methods for large-scale reinforcement learning. Their empirical success has prompted several works that develop the foundation of their global convergence theory. However, prior works have either required exact gradients or state-action visitation measure based mini-batch stochastic gradients with a diverging batch size.",
                "distance": 0.055
            },
            {
                "reference": "Policy gradient methods are widely used to solve complex control problems in Reinforcement Learning (RL). Most methods focus on choosing the step size while ignoring the batch size, which is the number of samples used to estimate the gradient direction for each update of the policy parameters.",
                "distance": 0.0553
            },
            {
                "reference": "Policy gradient methods have grown in popularity in motor control and robotic applications in the reinforcement learning field over the last decade. These methods have been optimized through effective gradient directions and efficient estimation algorithms. However, little attention has been given to the automatic selection of step size, which greatly influences the convergence properties of policy gradient methods.",
                "distance": 0.0576
            },
            {
                "reference": "Actor-critic algorithms for reinforcement learning have been re-popularized due to their good convergence properties in situations where other approaches tend to fail, such as when function approximation is involved. Additionally, there is a growing body of evidence that phasic dopamine signals-based actor-critic approaches play a key role in biological learning.",
                "distance": 0.0587
            },
            {
                "reference": "Actor-critic (AC) methods have shown great empirical success in reinforcement learning. Despite the well-studied asymptotic convergence of AC under two time-scale learning rate schedules, the non-asymptotic convergence and finite sample complexity of actor-critic methods remain largely unresolved.",
                "distance": 0.0594
            },
            {
                "reference": "Policy gradient (PG) reinforcement learning algorithms, despite their strong (local) convergence guarantees, are plagued by a high variance in the estimation of the gradient that limits their learning performance.",
                "distance": 0.06
            },
            {
                "reference": "Policy gradient methods have performed well in solving complex reinforcement learning challenges but still suffer from large variance issues on policy gradient estimation, which leads to poor sample efficiency during training.",
                "distance": 0.0624
            }
        ]
    },
    {
        "target": "Modern integrated circuit (IC) designs require analyses of complex phenomena such as timing, noise, and power for tens of billions of electronic components; there is a need to balance between accuracy and speed. Recently, AI has started to be explored to improve these analyses, in particular for timing analysis.",
        "prediction": "When discovery systems face the task of revising an initially held theory due to new information, the process often involves a hill-climbing search for a consistent solution. This process was typically executed using domain-independent heuristics.",
        "queries": [
            -2.498453,
            -2.825123
        ],
        "log": [
            {
                "reference": "When discovery systems face the task of revising an initially held theory due to new information, the process often involves a hill-climbing search for a consistent solution. This process was typically executed using domain-independent heuristics.",
                "distance": 0.0489
            },
            {
                "reference": "Current methods for designing application-specific hardware accelerators require considerable effort, a large number of time-consuming simulations, and must be re-run every time the set of target applications or design constraints change.",
                "distance": 0.0558
            },
            {
                "reference": "Designing analog circuits using simulation offers advantages in exploring alternatives without added cost or risk of hardware faults. However, it's difficult to use simulation for interactive debugging of physical circuits due to the challenges in comparing simulated analyses with hardware measurements and continually configuring simulations to match the physical circuit's state.",
                "distance": 0.0715
            }
        ]
    },
    {
        "target": "BERT-based models are known for their strong performance in various applications, but they often exhibit poor generalizability in real-world settings, especially when the amount of training data is limited.",
        "prediction": "Fine-tuned variants of BERT achieve state-of-the-art accuracy on many natural language processing tasks, but come with significant computational costs.",
        "queries": [
            -1.018418,
            15.343117
        ],
        "log": [
            {
                "reference": "Fine-tuned variants of BERT achieve state-of-the-art accuracy on many natural language processing tasks, but come with significant computational costs.",
                "distance": 0.0195
            },
            {
                "reference": "Large-scale pretrained language models have shown effectiveness in many NLP tasks, but there still remain real-world contextual aspects of language that these models do not capture. Language often depends heavily on the geographical, temporal, and other social contexts of the speaker, but these elements are not incorporated in modern transformer-based language models.",
                "distance": 0.0277
            },
            {
                "reference": "BERT is a dominant and powerful language model, however, its complexity leads to the need for efficient model compression techniques. Current techniques including knowledge distillation come with their own limitations including the introduction of an additional loss function.",
                "distance": 0.0278
            },
            {
                "reference": "The state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders which have a huge memory footprint. Their deployment on edge devices with limited memory budgets, such as Amazon Alexa and Google Assistant, is challenging.",
                "distance": 0.0291
            },
            {
                "reference": "Current transformer language models demonstrate notable performance, but there is room for optimization particularly during inference time. Existing dynamic evaluation methods only update model parameters, not cached hidden states.",
                "distance": 0.038
            },
            {
                "reference": "Pre-trained language models such as BERT and RoBERTa are powerful for many natural language processing tasks, but they are computational and memory expensive. Previous efforts to compress these models have resulted in fixed smaller sizes, which do not fully satisfy the requirements of different edge devices with various hardware performances.",
                "distance": 0.0393
            },
            {
                "reference": "Pre-training models like BERT have greatly improved performance in natural language processing tasks, but their large parameter sizes make deployment on edge devices problematic. Existing distillation methods don't consider the angular distance aspect when fitting the output of the intermediate layer.",
                "distance": 0.0402
            },
            {
                "reference": "Pre-trained language models of the BERT family have been known to perform exceedingly well in a range of NLP tasks yet their large number of parameters make them difficult to deploy in resource-limited scenarios. Previous work on compressing BERT has primarily focused on a single kind of compression technique and little attention has been paid to the combination of different methods.",
                "distance": 0.0438
            },
            {
                "reference": "Pre-trained language models like BERT have achieved impressive results on various Natural Language Processing tasks. However, they have a large number of parameters and require significant computational and memory resources, making them difficult to deploy in the real world. As a result, there is a need to compress these models to reduce their computational and memory footprint.",
                "distance": 0.049
            },
            {
                "reference": "Pre-trained big models like BERT, ERNIE, XLNet, GPT3 etc., though effective, have their deployments in real-world applications impeded by excessive computation and memory demand. For named entity recognition (NER) and other applications, achieving state-of-the-art results within resource constraints has been a significant focus.",
                "distance": 0.0503
            },
            {
                "reference": "Transformer-based pre-trained models like BERT have achieved great success on Semantic Sentence Matching. Dependency prior knowledge has shown general benefits in multiple NLP tasks, but integrating this structure into pre-trained models to better model complex semantic matching relations remains unresolved.",
                "distance": 0.0536
            },
            {
                "reference": "Pre-trained language models like BERT have been effective in various natural language processing tasks but their vast number of parameters makes them impractical for deployment on resource-constrained devices. Current model compression methods such as Knowledge distillation, Weight pruning, and Quantization have limitations in terms of accuracy loss, small compression ratio or high error rate.",
                "distance": 0.0556
            },
            {
                "reference": "Large pretrained models like BERT integrate a range of features into comprehensive vectors, thereby providing strong predictive accuracy across many tasks. The drawback of this method, however, is that all aspects of a text are entangled in a single representation, which can limit the interpretability and usability of the model for specific tasks.",
                "distance": 0.0785
            },
            {
                "reference": "Lightweight speech recognition models have exploded in demand due to the increasing amount of speech-interactive features on mobile devices. Typically, these systems are created by compressing large, pre-trained speech models.",
                "distance": 0.0799
            },
            {
                "reference": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but it is also computation and memory expensive. Though binarization, a powerful compression approach, reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations, full binarization of BERT usually results in a significant performance drop, and there are few studies addressing this problem.",
                "distance": 0.0835
            },
            {
                "reference": "Pre-trained Transformers like BERT and RoBERTa are now commonly used in natural language processing for tasks such as natural language inference, paraphrase detection, and commonsense reasoning. However, little is known empirically about whether these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example, both in-domain and out-of-domain.",
                "distance": 0.0921
            },
            {
                "reference": "The BERT model, despite its popularity, is known for its high computation cost during inference, which makes it inefficiency for real-time applications.",
                "distance": 0.0925
            },
            {
                "reference": "Large pre-trained language models such as BERT show effective results in various natural language processing tasks. However, their massive size makes them difficult to deploy in real-time applications that need quick inference with limited resources. Existing methods compress BERT into smaller models, but the compression is insensitively conducted regardless of the downstream tasks.",
                "distance": 0.0927
            },
            {
                "reference": "Pre-trained language models such as BERT yield exceptional performance in natural language processing tasks but are expensive to deploy in real-time applications. Existing solutions use knowledge distillation to compress large teacher models to smaller student models but perform poorly when training data is scarce in the target domain.",
                "distance": 0.0974
            },
            {
                "reference": "While over-parameterized pretrained language models like BERTBASE have greatly contributed for the success of natural language processing, their complexity renders them unsuitable for deployment on low-capacity devices.",
                "distance": 0.099
            }
        ]
    },
    {
        "target": "Over-parametrized shallow networks are known to achieve global convergence under gradient descent. However, narrow networks often get stuck in badly generalizing local minima.",
        "prediction": "The structure and optimization of neural networks, especially their potential as convex regularizers, are complex and not fully understood.",
        "queries": [
            5.287877,
            -10.115346
        ],
        "log": [
            {
                "reference": "The structure and optimization of neural networks, especially their potential as convex regularizers, are complex and not fully understood.",
                "distance": 0.0568
            },
            {
                "reference": "Previous studies have indicated that overparametrization helps explain global convergence of gradient descent for neural networks in the absence of convexity. Previous analyses have primarily focused on the lazy regime or used infinite width (mean field) for shallow networks.",
                "distance": 0.0618
            },
            {
                "reference": "Training neural networks often involves differentiable loss functions, but they do not necessarily align with the target evaluation metric, which might be non-differentiable.",
                "distance": 0.064
            },
            {
                "reference": "Previous studies have developed kernel functions for multi-layer random neural networks, but they are not framed in a Bayesian context, and the correspondence between using these kernels as covariance functions for a Gaussian process (GP) and performing fully Bayesian prediction with a deep neural network has not been identified.",
                "distance": 0.0731
            }
        ]
    },
    {
        "target": "Learning in deep neural networks is known to depend critically on the knowledge embedded in the initial network weights, but few theoretical results have precisely linked prior knowledge to learning dynamics.",
        "prediction": "Neural Networks (NNs) are widely used across various domains in computer science. However, they lack formal guarantees on their behavior, raising doubts about their reliability in safety and security critical applications.",
        "queries": [
            5.477496,
            -8.515601
        ],
        "log": [
            {
                "reference": "Neural Networks (NNs) are widely used across various domains in computer science. However, they lack formal guarantees on their behavior, raising doubts about their reliability in safety and security critical applications.",
                "distance": 0.0255
            },
            {
                "reference": "Most related work on learning dynamics of neural networks focus on the behavior of effective learning rate in 'equilibrium' state, assuming weight norm remains unchanged. However, the explanation of why this equilibrium can be reached has been absent or less convincing.",
                "distance": 0.0366
            },
            {
                "reference": "Most deep learning model architectures are designed empirically and with a certain level of arbitrariness, with few methods or mathematical tools available to design or evaluate new structures and their feature representation capabilities.",
                "distance": 0.0383
            },
            {
                "reference": "It is widely believed that the depth of deep neural networks (DNN) plays a crucial role in their success, but this belief lacks concrete theoretical backing.",
                "distance": 0.0738
            },
            {
                "reference": "Deep multitask networks, which produce multiple predictive outputs, have scalability and regularization advantages over single-task networks. However, their training is challenging due to the need to find the right balance between tasks.",
                "distance": 0.0772
            },
            {
                "reference": "Neural Network (NN) models have gained a reputation as black boxes and are theoretically complex to prove their properties. Despite these challenges, some models have been verified by exploiting their piecewise linear structure and insights from Satisfiability Modulo Theory. However, these methods do not yet scale well to realistic neural networks.",
                "distance": 0.0779
            },
            {
                "reference": "Existing theoretical frameworks have limitations analyzing deep neural networks, failing to evaluate networks with more than two layers without encountering complexity bounds that surge exponentially with each increase in depth.",
                "distance": 0.0824
            }
        ]
    },
    {
        "target": "Current arbitrary style transfer models are limited to either image or video domains, requiring separate models with different training processes for achieving satisfying image and video style transfers.",
        "prediction": "Current methods for neural image style transfer lack an explicit representation for style, limiting their flexibility and understanding.",
        "queries": [
            24.921173,
            3.422395
        ],
        "log": [
            {
                "reference": "Current methods for neural image style transfer lack an explicit representation for style, limiting their flexibility and understanding.",
                "distance": 0.0107
            },
            {
                "reference": "Existing approaches to artistic style transfer, which separates style from content via neural networks, are not suitable for photorealistic style transfer as they often result in distortions that resemble paintings, even when both input and reference images are photographs.",
                "distance": 0.0119
            },
            {
                "reference": "Previous research has demonstrated the potential of using feed-forward convolutional neural networks for fast style transfer in images.",
                "distance": 0.0156
            },
            {
                "reference": "Existing style transfer methods face challenges such as distortions in local style patterns with neural parametric methods, and wash-out artifacts with neural non-parametric methods.",
                "distance": 0.0181
            },
            {
                "reference": "In the field of artistic style transfer, models aim to reflect changes and variations in artistic styles. Existing works have neglected to address these observations.",
                "distance": 0.0195
            },
            {
                "reference": "Neural style transfer has significantly advanced for image style transfer. However, consistent style transfer for videos remains a challenging problem, as existing strategies either use substantial video data with optical flows or employ single-frame regularizers, which have limited performance on real videos.",
                "distance": 0.0204
            },
            {
                "reference": "Previous single-frame style transfer methods which maintain temporal consistency impose a strong constraint on the whole image, an assumption that can often be violated. There is a need for a versatile style transfer method capable of performing artistic, photo-realistic, and video style transfer jointly, without needing videos during training.",
                "distance": 0.0215
            },
            {
                "reference": "Video style transfer, which has numerous applications such as augmented reality and animation production, is gaining attention in the artificial intelligence community. Traditional image style transfer methods face challenges when applied to video, particularly generating satisfactory stylized results for any specified style while maintaining temporal coherence across frames.",
                "distance": 0.0221
            },
            {
                "reference": "Current style transfer methods struggle from a lack of holistic understanding of artistic styles, often using a single image or artist for reference, and relying on direct comparisons within the domain of RGB images or on CNNs pre-trained on ImageNet. These methods can introduce extra biases, as they often lack an in-depth artistic perspective.",
                "distance": 0.024
            },
            {
                "reference": "Current deep convolutional neural networks can solve the artistic style transfer problem initially proposed by Gatys et al. However, these models lack flexibility in adapting to different tasks or style constraints without undergoing retraining.",
                "distance": 0.0248
            },
            {
                "reference": "Existing universal style transfer methods based on second-order statistics are either computationally expensive or prone to generate artifacts. These methods face a trade-off between image quality and runtime performance.",
                "distance": 0.026
            },
            {
                "reference": "Recent work in style transfer has improved significantly in the representation of color and texture, computational speed, and image resolution. However, the explicit transformation of image content, such as deforming, adding or removing content details, has been mostly neglected.",
                "distance": 0.0261
            },
            {
                "reference": "Photorealistic style transfer involves transferring the style of a reference image to another image so the result looks like a likely photo. The existing models are slow due to their large sizes, creating a need for more efficient solutions.",
                "distance": 0.0262
            },
            {
                "reference": "Recent solutions for photorealistic style transfer that employ deep convolutional neural networks (CNNs) require intensive training from large-scale datasets, which results in their limited applicability and poor generalization ability to unseen images or styles.",
                "distance": 0.027
            },
            {
                "reference": "Existing neural style transfer methods require reference style images to transfer texture information from style images to content images.",
                "distance": 0.0281
            },
            {
                "reference": "Fast Style Transfer methods, which aim to transfer a photograph to an artistic style in real-time, have been trending recently. However, controlling the stroke size in the stylized results remains an open problem.",
                "distance": 0.0285
            },
            {
                "reference": "Numerous attempts have been made at arbitrary style transfer since the seminal work of Gatys et al. Existing state-of-the-art approaches, however, often generate results that are insufficiently stylized, especially in challenging cases.",
                "distance": 0.0311
            },
            {
                "reference": "Neural style transfer, an emerging technique endowing images with artistic styles, has succeeded with convolutional neural networks (CNNs) for monocular images or videos. However, stereoscopic image style transfer, requiring consistency between views to provide a comfortable visual experience, remains unexplored.",
                "distance": 0.0319
            },
            {
                "reference": "Existing artistic style transfer methods with deep neural networks have significantly improved, but they still suffer from artifacts like disharmonious colors and repetitive patterns.",
                "distance": 0.0335
            },
            {
                "reference": "Transferring artistic styles onto everyday photographs is a popular task. Recent approaches enable nearly real-time stylization through offline training. However, when applied to high-resolution images, the style of localized regions often appears less similar to the desired artistic style, failing to capture small, intricate textures and maintain correct texture scales of the artworks.",
                "distance": 0.0336
            }
        ]
    },
    {
        "target": "Encryption traffic classification is critical and widely researched. Existing methods focus on flow-level features, which often fail in short flows due to unreliable statistical properties. They also treat headers and payloads equally, missing potential correlations among bytes.",
        "prediction": "Traditionally, network traffic analysis focuses on protocol identification and application classification. However, as many general protocols and legal applications could be misused to hide and transmit data of different content types, this conventional approach may allow illegal content to bypass the traffic analysis system, leading to inefficient network management and potential risks.",
        "queries": [
            -15.895291,
            -0.752678
        ],
        "log": [
            {
                "reference": "Traditionally, network traffic analysis focuses on protocol identification and application classification. However, as many general protocols and legal applications could be misused to hide and transmit data of different content types, this conventional approach may allow illegal content to bypass the traffic analysis system, leading to inefficient network management and potential risks.",
                "distance": 0.0285
            },
            {
                "reference": "Network security studies have been focused on methods for discovering anomalies and recognizing malignant patterns, while network traffic studies have been investigating methods for traffic classification mainly for traffic engineering.",
                "distance": 0.0318
            },
            {
                "reference": "While encryption techniques enhance personal network metadata protection, they pose challenges for encrypted network traffic monitoring and analysis. Current deep learning-based methods, leveraging statistical features for encrypted traffic classification, face limitations due to lack of accounting for correlations in flow sequences and the need for substantial manually-labeled data for training.",
                "distance": 0.0537
            },
            {
                "reference": "The increase in the size and source of network traffic brings the challenge of monitoring and analyzing network traffic. Classifying encrypted traffic is particularly challenging due to the imbalanced nature of network data, the need for generalization on unseen datasets, and the dependence on data size.",
                "distance": 0.0645
            },
            {
                "reference": "A fair contract signing protocol allows potentially mistrusted parties to exchange their commitments to an agreed contract in a fair way. Existing protocols that utilize the RSA signature scheme, although fair and optimistic, have a loophole that allows possible mistreatments if the protocol is executed unsuccessfully.",
                "distance": 0.0654
            },
            {
                "reference": "Encrypted traffic classification is crucial for network security and network management. Current solutions rely heavily on deep features, which depend on data size and are difficult to generalize on unseen data. Leveraging the open-domain unlabeled traffic data to learn strong generalizable representations is a key challenge.",
                "distance": 0.068
            },
            {
                "reference": "Internet traffic classification is typically done using statistics such as packet sizes and inter-packet delays. Additionally, there is a need for effective methods of detecting botnets and malware on social platforms such as Facebook.",
                "distance": 0.0694
            },
            {
                "reference": "Mobile network traffic is mostly carried over HTTP/HTTPS, which presents new challenges to traditional traffic identification methods. Existing methods often rely on predefined statistical features and side-channel traffic information, which are not always effective.",
                "distance": 0.0808
            },
            {
                "reference": "Internet traffic is largely dominated by P2P downloads facilitated by BitTorrent, with an increasing research focus being placed on tools for automatic classification of Internet traffic by application. While standard supervised machine learning methods based on parameters such as packet size and interarrival time have been applied for traffic classification, they can't be used for subclassification of P2P traffic because P2P transfers sharing the same BitTorrent protocol have similar parameter profiles.",
                "distance": 0.0927
            },
            {
                "reference": "Boundary Protection Devices (BPDs) are used to regulate information across networks of various security levels and are critical especially in national security and infrastructure environments. However, their design as one-way traffic control systems presents challenges for dynamic load adaptation techniques, and a rigorous approval process results in often inefficient or ineffective manual configuration management.",
                "distance": 0.0951
            }
        ]
    },
    {
        "target": "Web vulnerability scanners (WVS) are crucial tools for detection of vulnerabilities. However, they are also used by malicious actors to exploit vulnerabilities in third-party websites. Existing research is more focused on identifying the number of vulnerabilities these tools can discover, rather than identifying the illicit usage of these tools.",
        "prediction": "Black-box web scanners have been a prevalent tool for penetration testing to find reflected cross-site scripting (XSS) vulnerabilities, but they suffer from unscalable testing and false negatives that result from a testing strategy that employs fixed attack payloads, failing to exploit contextual information to trigger vulnerabilities.",
        "queries": [
            -15.972213,
            0.187561
        ],
        "log": [
            {
                "reference": "Black-box web scanners have been a prevalent tool for penetration testing to find reflected cross-site scripting (XSS) vulnerabilities, but they suffer from unscalable testing and false negatives that result from a testing strategy that employs fixed attack payloads, failing to exploit contextual information to trigger vulnerabilities.",
                "distance": 0.0336
            },
            {
                "reference": "Effective improvement in the accuracy of cross-site scripting (XSS) attack detection remains a challenge.",
                "distance": 0.0472
            }
        ]
    },
    {
        "target": "Conversational Question Generation (CQG) has two main challenges: what-to-ask and how-to-ask, particularly in answer-unaware settings which are more realistic and gaining attention. Existing methods select sequential sentences from context as rationales and implicitly decide the question type (boolean/span-based) which may result in conversations that are not natural.",
        "prediction": "Existing computational models for interpreting and generating indirect answers to Yes-No questions are limited in handling different types of indirect answers.",
        "queries": [
            -7.32976,
            10.884404
        ],
        "log": [
            {
                "reference": "Existing computational models for interpreting and generating indirect answers to Yes-No questions are limited in handling different types of indirect answers.",
                "distance": 0.0897
            },
            {
                "reference": "Generating diverse and human-like natural language questions through existing models is challenging and lacks direct capture of variability in possible questions.",
                "distance": 0.1038
            }
        ]
    },
    {
        "target": "In knowledge distillation, a higher-performing teacher model doesn't necessarily lead to a stronger student model, emphasizing a mismatch between teacher training practices and effective knowledge transfer.",
        "prediction": "Distillation efforts already exist for language models that aim for more compact and efficient models without any significant drops in performance. The standard approach uses a dual-objective method - a task-specific objective and an imitation objective.",
        "queries": [
            9.139605,
            20.876684
        ],
        "log": [
            {
                "reference": "Distillation efforts already exist for language models that aim for more compact and efficient models without any significant drops in performance. The standard approach uses a dual-objective method - a task-specific objective and an imitation objective.",
                "distance": 0.0122
            },
            {
                "reference": "Traditional knowledge distillation (KD) methods fix the teacher model during training, which may limit the ability to transfer knowledge to the student network.",
                "distance": 0.0148
            },
            {
                "reference": "Knowledge distillation transfers knowledge from a larger teacher network to a smaller student network. However, because of the capacity mismatch, better teachers do not always result in better students.",
                "distance": 0.0159
            },
            {
                "reference": "Most teacher-student frameworks based on knowledge distillation (KD) heavily rely on instance level congruent constraints, but they often overlook the correlation between multiple instances, which could be valuable for knowledge transfer.",
                "distance": 0.0162
            },
            {
                "reference": "The literature on knowledge distillation, a technique used to improve the performance of compact student networks by leveraging knowledge from a more robust teacher, has been limited to scenarios where the student and teacher work on the same task.",
                "distance": 0.0172
            },
            {
                "reference": "Existing two-stage knowledge distillation approaches pre-train a DNN with large capacity as the 'teacher' and then transfer the teacher's knowledge to another 'student' DNN in a one-way, unidirectional manner.",
                "distance": 0.0177
            },
            {
                "reference": "Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. Various complex approaches with elaborately designed knowledge representations have been proposed to achieve this which increase the difficulty of model development and interpretation.",
                "distance": 0.0177
            },
            {
                "reference": "Knowledge distillation is an important model compression technique that distills knowledge from larger networks to smaller ones. However, prior distillation methods tend to overlook the functional properties of neural networks, making their application to new tasks unreliable and non-trivial.",
                "distance": 0.0201
            },
            {
                "reference": "Knowledge distillation is used to compress large pre-trained convolutional neural networks (CNNs) or their ensembles into models suitable for mobile and embedded devices. However, existing methods are hand-crafted and require a lot of effort to explore a powerful student model from a large design space.",
                "distance": 0.0208
            },
            {
                "reference": "Knowledge distillation can be separated into offline and online categories based on whether a pre-trained, persistent teacher model is used during the process. Offline distillation makes use of existing models but generally underperforms online ones.",
                "distance": 0.021
            },
            {
                "reference": "There have been some recent attempts at Gaussian noise-based data-free knowledge distillation for the knowledge transfer from one network (teacher) to the other (student). However, none of them offer a consistent or reliable solution, and this method can potentially eliminate privacy concerns such as in healthcare applications.",
                "distance": 0.0211
            },
            {
                "reference": "Knowledge distillation is a technique that transfers knowledge from a large, cumbersome teacher model to a smaller student model. Recent results suggest that a student-friendly, pruned teacher model might be more suitable for knowledge transfer.",
                "distance": 0.0211
            },
            {
                "reference": "Knowledge distillation transfers knowledge from a teacher network to a student network using training data. However, data is not always accessible due to large sizes, privacy, or confidentiality. There have been many attempts to address this issue for Convolutional Neural Networks (CNNs) that handle grid data like images and videos, but the problem remains largely overlooked for Graph Neural Networks (GNNs) which handle non-grid data with differing topology structures within a discrete space, making these CNN-based methods unsuitable for GNNs.",
                "distance": 0.0213
            },
            {
                "reference": "Existing knowledge distillation frameworks usually focus on getting the student model to mimic the output of the teacher model for every sample and transfer cross-sample relations from the teacher to the student. However, the structured relations at a category level are often overlooked.",
                "distance": 0.0214
            },
            {
                "reference": "Knowledge distillation addresses model compression by transferring knowledge from a large model to a smaller one. Existing approaches focus on aligning sample representations but fail to transfer class representations, leading to potential biases. They also ignore the imperfections of the teacher model.",
                "distance": 0.0221
            },
            {
                "reference": "Recommender systems have begun to use knowledge distillation, a model compression technique in which a more compact model (the student) is trained with transferred knowledge from a complex model (the teacher). Existing methods rely on unidirectional distillation, with the assumption that the teacher is always superior to the student.",
                "distance": 0.0223
            },
            {
                "reference": "Teacher-student knowledge distillation is a widely used technique for compressing large language models into smaller sizes suitable for low-latency applications. The quality of the student model depends on both the teacher model and the choice of transfer set used for distillation. However, there's a difference between the generic corpora used for pretraining the teacher model and the domain-specific corpora of downstream tasks.",
                "distance": 0.0224
            },
            {
                "reference": "Cross-modal knowledge distillation involves transferring knowledge from a model trained with superior modalities (Teacher) to another model trained with weaker modalities (Student). However, existing approaches require paired training examples in both modalities, and getting data from superior modalities may not always be feasible.",
                "distance": 0.0225
            },
            {
                "reference": "Distillation with unlabeled examples is a popular method for training deep neural networks when the amount of labeled data is limited. A large 'teacher' neural network is trained on labeled data, which is then used to generate labels on an unlabeled dataset, which are then used to train a smaller 'student' model. The success of this approach depends on the quality of the teacher's labels, as the student could get confused if trained on inaccurate data.",
                "distance": 0.0234
            },
            {
                "reference": "Existing knowledge distillation methods focus on baseline settings, where teacher models and training strategies are not as strong and advanced as state-of-the-art approaches",
                "distance": 0.0235
            }
        ]
    },
    {
        "target": "Existing works benchmark automatic movie narrating systems as a normal video captioning task, simplifying the process by removing role names and evaluating narrations with ngram-based metrics. These oversimplifications, however, make it difficult for automatic systems to meet the needs of real application scenarios.",
        "prediction": "Current methods for video captioning often use static fusion concepts such as concatenation and summation. These methods do not effectively manage different visual entities like actions and objects, leading to ambiguity in video descriptions.",
        "queries": [
            11.017399,
            6.649009
        ],
        "log": [
            {
                "reference": "Current methods for video captioning often use static fusion concepts such as concatenation and summation. These methods do not effectively manage different visual entities like actions and objects, leading to ambiguity in video descriptions.",
                "distance": 0.0132
            },
            {
                "reference": "Video captioning models generate natural language descriptions of video content with representations learning playing a crucial role. Current methods function within the supervised learning framework and compare the generated caption with the ground-truth text word-by-word without fully implementing linguistic semantics.",
                "distance": 0.0161
            },
            {
                "reference": "Video captioning combines video understanding and language generation, usually considering a sequence of frames and biases towards focused objects. Detecting and properly accommodating focused objects is critical in video captioning, but existing methods do not effectively enforce the description of focused objects.",
                "distance": 0.0168
            },
            {
                "reference": "The conventional approach to video captioning is to learn from offline-extracted dense video features usually obtained at a fixed frame rate and often trained on image/video understanding tasks without adaptation to video captioning data.",
                "distance": 0.0172
            },
            {
                "reference": "Existing techniques for video captioning typically follow the encoder-decorator framework which processes a single source video at a time, potentially limiting multi-contextual understanding of words appearing in multiple relevant videos.",
                "distance": 0.0577
            },
            {
                "reference": "Dense captioning is a new computer vision topic and faces two challenges: overlapping target regions make accurate localization of each visual concept challenging, and the large amount of visual concepts makes recognition difficult.",
                "distance": 0.0681
            },
            {
                "reference": "Dense video captioning involves generating multiple associated captions with their temporal locations from a video. Previous methods follow a 'localize-then-describe' scheme, which relies heavily on numerous hand-crafted components and often employ a two-stage scheme.",
                "distance": 0.0682
            },
            {
                "reference": "Current methods for video description are based on encoder-decoder sentence generation using recurrent neural networks (RNNs), where the decoder network uses temporal attention mechanisms to give more weight to encoded features from specific time frames, typically using image and motion features.",
                "distance": 0.0759
            },
            {
                "reference": "3D dense captioning is a recently introduced task, presenting challenges due to the high complexity and variety of inter-object relations in point clouds. Current methods view these relations as mere by-products of object feature learning in graphs, leading to sub-optimal results.",
                "distance": 0.0775
            },
            {
                "reference": "Previous works on video captioning have primarily focused on video clips, treating entire videos as a whole and generating captions based on a single embedding, which fails to capture rich temporal structures in videos.",
                "distance": 0.0805
            },
            {
                "reference": "Video captioning, which involves generating one or multiple sentences to describe a realistic video, has been a problem in the field of computer science.",
                "distance": 0.0809
            },
            {
                "reference": "In the field of video captioning, previous frameworks employed a sequence-to-sequence approach and struggled to handle multiple channels of video representation.",
                "distance": 0.0838
            },
            {
                "reference": "Dense video captioning involves detecting and describing events in an untrimmed video which is typically tackled with two separate models for event proposal and captioning. These models are either trained alone or alternately, inhibiting a direct link between language descriptions and event proposals which could improve description accuracy.",
                "distance": 0.0879
            },
            {
                "reference": "Dense video captioning, which involves localizing events in a video and producing text descriptions for them, has traditionally relied solely on visual information. The audio track has been widely ignored in previous works despite being a crucial aspect in humans\u2019 understanding of an environment.",
                "distance": 0.0904
            },
            {
                "reference": "Dense video captioning is challenging due to its need for holistic knowledge of the video and contextual reasoning of individual events. Existing approaches propose event boundaries from a video and then caption a subset, which can be considerably resource-consuming, especially with long videos.",
                "distance": 0.0915
            },
            {
                "reference": "Video captioning, which generates natural language descriptions of video content, has gained attention. Accurate video captioning needs to understand the global content and detailed object information in videos, with the quality heavily reliant on video representations. There has been a need to capture salient objects and their detailed temporal dynamics and represent them using discriminative spatio-temporal representations.",
                "distance": 0.0928
            }
        ]
    },
    {
        "target": "Language identification (LID) is an essential part of many natural language processing pipelines, but current systems often struggle with lower-resource languages.",
        "prediction": "Multilingual sentence and document representations are emerging as important components in natural language processing tasks, but there is a need to study these advances further, focusing on efficiency and large-scale applicability.",
        "queries": [
            -0.566655,
            11.591786
        ],
        "log": [
            {
                "reference": "Multilingual sentence and document representations are emerging as important components in natural language processing tasks, but there is a need to study these advances further, focusing on efficiency and large-scale applicability.",
                "distance": 0.0797
            },
            {
                "reference": "Existing language models do not efficiently utilize contextual information.",
                "distance": 0.0843
            }
        ]
    },
    {
        "target": "In adaptive data analysis, it is computationally hard to answer more than \u0398(n^2) adaptive queries with a mechanism that gets n i.i.d. samples from an unknown distribution and is required to provide accurate estimations to statistical queries. However, these negative results strongly rely on an adversarial model strongly favoring the adversarial analyst who chooses the adaptive queries and the underlying distribution.",
        "prediction": "The need to store usage statistics, event logs, and other time-series data from Cisco Meraki customers' devices drove the creation of LittleTable.",
        "queries": [
            -8.672142,
            -5.898225
        ],
        "log": [
            {
                "reference": "The need to store usage statistics, event logs, and other time-series data from Cisco Meraki customers' devices drove the creation of LittleTable.",
                "distance": 0.1361
            },
            {
                "reference": "There are challenges in applying the MapReduce framework to reason over temporal information, especially in the context of large-scale reasoning under temporal RDFS semantics.",
                "distance": 0.1895
            }
        ]
    },
    {
        "target": "Previous research on relation extraction emphasizes the utilization of natural language explanations as inductive biases to guide models, improving their efficiency and generalization ability.",
        "prediction": "Open relation extraction models typically rely on language-specific knowledge or tools like translators or Part-of-Speech (PoS) taggers. However, these methods may face difficulties when extending to new languages or dealing with languages with scarce resources.",
        "queries": [
            -3.416738,
            4.117673
        ],
        "log": [
            {
                "reference": "Open relation extraction models typically rely on language-specific knowledge or tools like translators or Part-of-Speech (PoS) taggers. However, these methods may face difficulties when extending to new languages or dealing with languages with scarce resources.",
                "distance": 0.0056
            },
            {
                "reference": "Relation extraction is a challenging task in natural language processing and syntactic features have been shown to be quite effective for this task. The state of the art involves using a syntactic convolution tree kernel, as introduced by Collins and Duffy.",
                "distance": 0.0255
            },
            {
                "reference": "Current neural models for relation extraction typically map each sentence to a single relation label, leading to problems when trying to handle multiple, overlapping relations. The overlapped features of these relations are either ignored or very difficult to identify.",
                "distance": 0.039
            },
            {
                "reference": "Existing relation extraction methods require a substantial amount of training examples to perform well, which can be costly and time-consuming to gather.",
                "distance": 0.0391
            },
            {
                "reference": "Current relation extraction methods aim to extract semantic relationships from unstructured text, mainly written language. However, essential data sources such as spoken language have been largely ignored mainly due to error propagation introduced in automatic speech recognition (ASR). Conversely, approaches exploring end-to-end speech-based relation extraction are rare.",
                "distance": 0.0466
            },
            {
                "reference": "Automatic extraction techniques for semantic relations between entities in natural language documents need to identify different versions of the same relation, which could be expressed in various ways. These techniques can benefit from considering a broad range of syntactic and semantic features, particularly those parse trees produced by automatic sentence parsers.",
                "distance": 0.0481
            },
            {
                "reference": "Tree kernel is an effective technique for relation extraction. However, the traditional syntactic tree representation is often too coarse or ambiguous to accurately capture the semantic relation information between two entities.",
                "distance": 0.0485
            },
            {
                "reference": "State-of-the-art relation extraction systems often source knowledge from patterns extracted from dependency parses of sentences. However, these can be of low quality in distantly supervised settings.",
                "distance": 0.0653
            },
            {
                "reference": "Previous research into extracting relations of varying complexity often required considerable supervision and lacked the ability to assign exact target argument roles.",
                "distance": 0.0722
            },
            {
                "reference": "Existing methods for relation classification in sentences treat all relations as the candidate relations for the two entities in a sentence, neglecting the restrictions on candidate relations by entity types that could lead to inappropriate candidate relations being considered.",
                "distance": 0.0737
            },
            {
                "reference": "Extracting semantic relationships between entities is a complex task. Prior methods typically approach this by using diverse lexical, syntactic, and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVM), but it's still a challenge to effectively capture useful information for relation extraction.",
                "distance": 0.0743
            },
            {
                "reference": "In the field of natural language processing, relation extraction in text data is important. However, current approach - distant supervision often lacks semantic information, and may cause wrong labeling.",
                "distance": 0.0743
            },
            {
                "reference": "Current relation extraction systems heavily rely on features generated by linguistic analysis modules. Errors in these features lead to issues in the detection and classification of relations.",
                "distance": 0.0815
            },
            {
                "reference": "Current approaches to relation extraction between two named entities, like semantic bornIn relation between a person and location entity, generally require supervision.",
                "distance": 0.0838
            },
            {
                "reference": "In previous work, the discovery of relationships of a specified concept is mostly focused on generic known relationships, not a wide range of relationships centered on the given concept.",
                "distance": 0.0867
            },
            {
                "reference": "The extraction of relation instances from unlabeled movie articles is a challenging task that requires a complex approach.",
                "distance": 0.0881
            },
            {
                "reference": "A large number of Open Relation Extraction approaches have been proposed that range from using 'shallow' (e.g., part-of-speech tagging) to 'deep' (e.g., semantic role labeling\u2010SRL) NLP machinery.",
                "distance": 0.0904
            },
            {
                "reference": "Building relation extraction systems in low-resource settings is a challenging task, and the pre-existing methods inadequately address this problem.",
                "distance": 0.0906
            },
            {
                "reference": "Previous approaches to relation extraction typically isolate the task and do not leverage information across documents. They also rely heavily on labelled data, which is resource-intensive to collect.",
                "distance": 0.0911
            },
            {
                "reference": "State-of-the-art methods for relation extraction, which is the task of finding semantic relations between entities from text, are mostly based on statistical learning. These methods have to deal with feature selection, which can significantly affect the classification performance.",
                "distance": 0.092
            }
        ]
    },
    {
        "target": "Large language models pose potential social harms such as generating offensive content and reinforcing biases. Existing works focus on ill-intentioned users, like those who explicitly make hate speech or elicit harmful responses, but discussions on sensitive issues can become toxic even if the users are well-intentioned.",
        "prediction": "Large pretrained language models can produce toxic or biased content, making their practical use problematic. Existing methods to detect these issues rely on templates, real-world data extraction, crowdsourcing, or automatic generation to construct adversarial contexts that could lead to toxic generations. However, research about the type of context that is more likely to induce unsafe responses is still under-explored.",
        "queries": [
            -3.470795,
            14.195266
        ],
        "log": [
            {
                "reference": "Large pretrained language models can produce toxic or biased content, making their practical use problematic. Existing methods to detect these issues rely on templates, real-world data extraction, crowdsourcing, or automatic generation to construct adversarial contexts that could lead to toxic generations. However, research about the type of context that is more likely to induce unsafe responses is still under-explored.",
                "distance": 0.0225
            },
            {
                "reference": "Pre-trained language models are known for easily generating toxic language.",
                "distance": 0.0596
            },
            {
                "reference": "Pre-trained language models encode undesirable social biases, which are often further exacerbated in downstream uses.",
                "distance": 0.0608
            },
            {
                "reference": "As powerful natural language generation, representation, and understanding models are increasingly developed and deployed across different applications, potential adverse impacts have become apparent. These impacts may include disparities in service quality, unequal resource distribution, erasure, stereotyping, misrepresentation of groups or individuals, and impacts on individual agency or wellbeing. Foreseeing and mitigating such harms in complex, open-ended language tasks has been a challenge.",
                "distance": 0.0615
            },
            {
                "reference": "Pre-trained language models are shown to capture social biases from the large amounts of text they are trained on, which has led to the development of techniques for mitigating such biases.",
                "distance": 0.0641
            },
            {
                "reference": "Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context.",
                "distance": 0.0776
            },
            {
                "reference": "Large, pre-trained language models can often contain harmful stereotypes learned from the internet. The bias transfer hypothesis suggests that these social biases internalized by these models during pre-training transfer into harmful task-specific behavior after fine-tuning.",
                "distance": 0.0801
            },
            {
                "reference": "Pre-trained language models are known to learn socially harmful biases from their training corpora and may repeat these biases when generating content. Existing studies have focused on uncovering explicit biases in such models.",
                "distance": 0.0829
            },
            {
                "reference": "Human-like biases and undesired social stereotypes are existent in large pre-trained language models. With these models being widely adopted in real-world applications, mitigating such biases has become important.",
                "distance": 0.0934
            }
        ]
    },
    {
        "target": "Dialogue state tracking (DST) research has focused on methods that allow few- and zero-shot transfer to new domains or schemas, but these performance gains rely heavily on aggressive data augmentation and fine-tuning of larger language model-based architectures.",
        "prediction": "Dialogue state tracking (DST) aims to extract essential information from multi-turn dialogue situations and take actions. The existing evaluation metrics, such as joint goal accuracy and slot accuracy, have limitations when evaluating accumulated belief states, particularly in the common MultiWOZ dataset.",
        "queries": [
            -6.31535,
            14.153126
        ],
        "log": [
            {
                "reference": "Dialogue state tracking (DST) aims to extract essential information from multi-turn dialogue situations and take actions. The existing evaluation metrics, such as joint goal accuracy and slot accuracy, have limitations when evaluating accumulated belief states, particularly in the common MultiWOZ dataset.",
                "distance": 0.0101
            },
            {
                "reference": "The arduous task of accumulating and annotating task-oriented conversations has created a pressing need for zero and few-shot learning in dialogue state tracking (DST).",
                "distance": 0.0161
            },
            {
                "reference": "Dialogue State Tracking (DST) is evaluated using Joint Goal Accuracy (JGA), which measures the fraction of turns where the ground-truth dialogue state matches the prediction. However, because of the cumulative nature of the belief state, correct predictions become difficult after a misprediction, making this metric potentially harsh and underestimating a DST model's potential.",
                "distance": 0.0187
            },
            {
                "reference": "Dialogue state tracking (DST) research has been accelerated by the MultiWOZ 2.0 dataset, however, considerable noise in the state annotations presents substantial challenges for model training. Although newer versions of the dataset contain fewer errors, noisy labels persist, particularly in the training set, and correcting problematic annotations is burdensome.",
                "distance": 0.0187
            },
            {
                "reference": "The vulnerability of dialogue state tracking (DST) models to distributional shifts has been revealed by recent works, highlighting the need for comprehensive diagnostics to understand relative performance.",
                "distance": 0.0212
            },
            {
                "reference": "Collecting dialogue data with domain-slot-value labels for dialogue state tracking (DST) is a costly process. Existing methods for few-shot cross-domain DST face challenges in performance.",
                "distance": 0.0213
            },
            {
                "reference": "Existing dialogue state tracking (DST) models require plenty of labeled data, which is costly to collect, especially when the number of domains increases.",
                "distance": 0.0225
            },
            {
                "reference": "Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are typically trained offline requiring a fixed dataset prepared in advance. This can be problematic in real-world scenarios since online dialogue systems usually involve constantly emerging new data and domains. However, these models struggle with the continual learning of new domains and the catastrophic forgetting of previously learned domains.",
                "distance": 0.0241
            },
            {
                "reference": "Current dialogue state tracking (DST) methodologies use historical information to establish states, typically represented as slot-value pairs. These methods generally struggle with effectively using relevant context due to an absence of a robust mechanism to model interactions between the slot and dialogue history. Furthermore, they don't address the slot imbalance problem, which hampers the learning of difficult slots and overall performance.",
                "distance": 0.0254
            },
            {
                "reference": "Dialogue state tracking (DST) is an essential component of a spoken dialogue system. However, existing DST models either overlook temporal feature dependencies across dialogue turns or do not effectively model temporal state dependencies within the dialogue.",
                "distance": 0.0261
            },
            {
                "reference": "Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, there are major challenges, such as the data sparsity problem, due to increased numbers of state candidates and dialogue lengths.",
                "distance": 0.0299
            },
            {
                "reference": "Dialogue State Tracking (DST) models help estimate user intentions and requests in dialogue systems. However, most existing approaches train DST on a single domain, ignoring information across multiple domains, which is a major challenge for constructing robust DST models.",
                "distance": 0.031
            },
            {
                "reference": "Existing dialogue datasets contain lots of noise in their state annotations, which can negatively impact model training and generalization performance. A framework named ASSIST has been proposed to train robust dialogue state tracking (DST) models, but tuning of its weighting parameter, which is shared by all slots and instances, can be challenging.",
                "distance": 0.0318
            },
            {
                "reference": "Recently proposed dialogue state tracking (DST) approaches predict the dialogue state of a target turn sequentially based on the previous dialogue state. However, this process may lead to error propagation as mistakes made by the model in the current turn are likely to be carried over to the following ones.",
                "distance": 0.0318
            },
            {
                "reference": "Recent efforts in Dialogue State Tracking (DST) have progressed towards open-vocabulary or generation-based approaches. However, these existing models do not allow the tracker to learn signals across domains and slots to detect dependencies and follow an auto-regressive approach that incurs a high cost when the dialogue evolves over multiple domains and turns.",
                "distance": 0.0337
            },
            {
                "reference": "Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user's goal. There are two prevalent strategies for DST: predicting from scratch and updating from the previous state. However, each strategy comes with its own drawbacks and the impact of the granularity of context information on DST is not well understood.",
                "distance": 0.0441
            },
            {
                "reference": "Task-based Virtual Personal Assistants (VPAs) depend on multi-domain Dialogue State Tracking (DST) models to monitor goals throughout a conversation. However, previously proposed models struggle adapting to unseen domains due to domain-specific parameters in their model structures.",
                "distance": 0.0448
            },
            {
                "reference": "Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. Typical DST models use the dialogue state of the last turn as the input for predicting the current state, but these models tend to maintain the predicted slot values unchanged. This creates problems in updating slot values that need to be changed and correcting wrongly predicted slot values.",
                "distance": 0.045
            },
            {
                "reference": "In multi-domain Dialogue State Tracking (DST), data sparsity is a major issue because of the increased number of state candidates. Existing DST approaches predict the value for each slot independently and do not take into account slot relationships, exacerbating the data sparsity problem.",
                "distance": 0.047
            },
            {
                "reference": "Dialogue State Tracking (DST) models aim to track users' intentions during conversations, and existing models lack efficient handling of relations among domains and slots. They also typically fail to fuse prior slot-domain membership relations and dialogue-aware dynamic slot relations explicitly and have trouble generalizing to unseen domains.",
                "distance": 0.048
            }
        ]
    },
    {
        "target": "The problem of influence maximization with fairness is to select influential nodes in a given network in order to maximize information spread while ensuring fair treatment of user attributes. The challenge lies in achieving a scalable solution for larger graphs with millions or billions of nodes as most studies so far focussed only on smaller networks.",
        "prediction": "Influence Maximization is an active topic, but previous works have always assumed full knowledge of the social network graph. However, there are scenarios where the network graph may not be fully known beforehand.",
        "queries": [
            0.145974,
            22.066202
        ],
        "log": [
            {
                "reference": "Influence Maximization is an active topic, but previous works have always assumed full knowledge of the social network graph. However, there are scenarios where the network graph may not be fully known beforehand.",
                "distance": 0.0015
            },
            {
                "reference": "Influence propagation and maximization is a well-studied problem in social network mining, but most existing works focus only on homogeneous social networks where nodes and links are of single type.",
                "distance": 0.0022
            },
            {
                "reference": "Previous methods for influence maximization in social networks, essential for viral marketing, fall into two paradigms: greedy algorithms, which select seed nodes one by one and ensure accuracy with high computational cost, and heuristic algorithms, which estimate influence spread using efficient heuristics but have unstable accuracy.",
                "distance": 0.0027
            },
            {
                "reference": "While the influence maximization problem, selecting users in a social network to maximize the expected number of users influenced by the selected users, has been extensively studied, existing works have overlooked the role of location information in this process.",
                "distance": 0.0039
            },
            {
                "reference": "Influence maximization on social networks has become a popular research topic with the rise of online social network services. Most current studies approximate a greedy-based sub-optimal solution by validating the submodular nature of the utility function.",
                "distance": 0.0042
            },
            {
                "reference": "The dominant approach in influence maximization in social networks had been disconnected, with one strand focusing on the generative model that produces cascades and the other on the algorithmic challenge of identifying influencers, assuming the generative model is known. Recent results indicate that no algorithm for influence maximization can yield a constant factor approximation guarantee using polynomially-many samples, drawn from any distribution, if the generative model is unknown but learned from training data.",
                "distance": 0.0054
            },
            {
                "reference": "The problem of influence maximization in social networks has been studied with two largely disjoint efforts: learning the generative model that produces cascades, and identifying a set of influencers assuming the generative model is known. Recent results suggest that when the generative model is not known but instead learned from training data, no algorithm can provide a constant factor approximation guarantee using polynomially-many samples.",
                "distance": 0.0085
            },
            {
                "reference": "Algorithms for social influence maximization are commonly used for strategically choosing an initial set of individuals in a social network from which information gets propagated. However, as human networks may encode historical biases, these algorithms might capture and reproduce such biases.",
                "distance": 0.009
            },
            {
                "reference": "Existing algorithms for seed minimization in social networks primarily operate in a non-adaptive setting where all seed nodes are selected in one batch, ignoring the potential influence these nodes may exert on other users.",
                "distance": 0.009
            },
            {
                "reference": "Influence maximization is a problem in social networks where the goal is to find a set of users who are most influential under specific propagation models. Previous approaches did not typically consider the importance of timing and delays in influence diffusion.",
                "distance": 0.0092
            },
            {
                "reference": "The influence maximization problem is widely studied for applications on viral marketing and product promotions. However, existing studies do not consider the monetary effect on the purchasing decision of individuals.",
                "distance": 0.0094
            },
            {
                "reference": "Influence estimation and maximization are crucial in areas like viral marketing, but most existing methods are non-adaptive, and may not suffice for scenarios where influence campaigns are run and observed over multiple rounds, or in situations where there is partial or no knowledge of the diffusion networks and how information spreads through them.",
                "distance": 0.0097
            },
            {
                "reference": "The influence maximization problem seeks to find a set of seed nodes in a social network to maximize their influence spread under certain propagation models. Existing algorithms have not accounted for the impact of novelty decay -- the idea that repeated exposures to a message results in diminishing influence.",
                "distance": 0.0097
            },
            {
                "reference": "Influence maximization is the problem of selecting $k$ nodes in a social network to maximize their influence spread. Most works focus on the submodular influence diffusion models.",
                "distance": 0.0102
            },
            {
                "reference": "Influence maximization has found applications in various fields such as sensor placement, viral marketing, and controlling rumor outbreak. However, traditional approaches do not account for the polarity of opinion in the network or the timeliness of the product adoption.",
                "distance": 0.0103
            },
            {
                "reference": "Influence maximization, which is defining a set of seed nodes in social networks to maximize influence spread, is conventionally seen as a stochastic process and formulated as a discrete optimization problem. Hence, most previous works have centered on developing effective heuristic algorithms within the greedy framework.",
                "distance": 0.0104
            },
            {
                "reference": "Influence maximization is a fundamental problem in social influence analysis, and numerous efficient algorithms have been developed to address this issue. However, identifying the superior algorithm has become a challenging task due to significant variations in their computational efficiency and solution quality, as demonstrated in a benchmark study by Arora, Galhotra, and Ranu in SIGMOD'17.",
                "distance": 0.0108
            },
            {
                "reference": "Traditional algorithms for influence maximization, designed to aid with the choice of 'peer leaders' or 'influencers' in various interventions, have not been designed with social, behavioral and public health interventions in mind. As a result, they may disproportionately exclude minority communities from the benefits of the intervention, prompting research on fair influence maximization. However, existing techniques have drawbacks such as requiring commitment to a single fairness measure and wastage of resources due to the imposition of strict constraints.",
                "distance": 0.0113
            },
            {
                "reference": "Influence maximization, the task of selecting a small number of seed nodes in a social network to maximize the spread of the influence from these seeds, has been widely investigated. The existing methods require the known social network and its diffusion parameters as input.",
                "distance": 0.0117
            },
            {
                "reference": "Influence maximization, the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models, has scalability issues. Prior solutions, such as the greedy algorithm, are slow, not scalable, and other heuristic algorithms don't provide consistently good performance.",
                "distance": 0.0122
            }
        ]
    },
    {
        "target": "In machine learning, classifiers tend to learn a false causal relationship between an over-represented concept and a label, leading to over-reliance on certain concepts, sparingly compromised classification accuracy. There isn't an efficient method to compare different models in terms of their over-reliance on specific concepts.",
        "prediction": "Prior to this work, there is a common belief that collecting more diverse and representative training data can help improve the machine learning predictor's performance across subpopulations. However, a precise framework for understanding how dataset properties like diversity affect learning outcomes is lacking.",
        "queries": [
            1.591925,
            -9.453688
        ],
        "log": [
            {
                "reference": "Prior to this work, there is a common belief that collecting more diverse and representative training data can help improve the machine learning predictor's performance across subpopulations. However, a precise framework for understanding how dataset properties like diversity affect learning outcomes is lacking.",
                "distance": 0.0253
            },
            {
                "reference": "Machine learning models are increasingly deployed as services where the training data distribution is hidden from clients and different clients may be interested in different regions of the data distribution. Existing methods assess the accuracy of a black-box classification model as a single aggregate on a given test data distribution.",
                "distance": 0.04
            },
            {
                "reference": "In many applications, training data is provided in the form of related datasets obtained from several sources, which typically affects the sample distribution. The learned classification models suffer due to bias introduced by 'spurious' samples - those due to source characteristics and not representative of any other part of the data. Standard outlier detection and robust classification usually fall short of determining groups of spurious samples.",
                "distance": 0.0741
            },
            {
                "reference": "The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time.",
                "distance": 0.0828
            },
            {
                "reference": "The presence of bias in model training data is a significant challenge in credit risk models for underwriting. Existing credit risk models, built using only applicants who had been funded for credit, may introduce sampling bias, which can affect default prediction on loan repayments when screening prospective borrowers.",
                "distance": 0.0918
            }
        ]
    },
    {
        "target": "The quality, workflow, and overall incentives of conference peer review in the NLP field were found to be in need of improvement, along with issues in paper-reviewer matching for non-mainstream NLP contexts.",
        "prediction": "Due to the rapidly increasing community, the myriad conferences related to Natural Language Processing, and easy access to conference proceedings, it has become essential to verify the integrity and novelty of new submissions.",
        "queries": [
            -8.473651,
            3.196469
        ],
        "log": [
            {
                "reference": "Due to the rapidly increasing community, the myriad conferences related to Natural Language Processing, and easy access to conference proceedings, it has become essential to verify the integrity and novelty of new submissions.",
                "distance": 0.0248
            },
            {
                "reference": "The rapid growth in the number of submissions to leading academic publication venues has identified a need for automation in parts of the peer-review process, but existing interfaces and technologies may exacerbate problems related to fairness and efficiency if not carefully designed.",
                "distance": 0.0342
            },
            {
                "reference": "The Conference Paper Assignment Problem (CPAP) involves the task of assigning reviewers to conference paper submissions with the aim to minimize dissatisfaction. Currently, papers are reviewed by members of a program committee (PC), who bid on papers before the assignment algorithm is applied.",
                "distance": 0.0397
            },
            {
                "reference": "Current automatic paper-reviewer matching algorithms for the peer review process often suffer from two major problems - that is, they often assign groups of reviewers that do not collectively have sufficient expertise, and they can result in highly skewed reviewer workloads.",
                "distance": 0.0406
            },
            {
                "reference": "The rapid increase of paper submissions in various conferences is challenging the effectiveness of the current scientific peer review process. The need to recruit a large number of reviewers with varying levels of expertise and the lack of conformity in reviews puts a burden on meta-reviewers tasked with reaching a final decision.",
                "distance": 0.0412
            },
            {
                "reference": "The increase in scientific submissions has posed challenges to the peer review process, necessitating the exploration of automated systems to assist human reviewers.",
                "distance": 0.0445
            },
            {
                "reference": "The quality of peer review, which is central to both scientific progress and individual researcher careers, heavily depends on the paper-reviewer matching process. However, this has been mostly viewed as an automated recommendation problem, without significant consideration for the accumulated experiences of different stakeholders.",
                "distance": 0.0448
            },
            {
                "reference": "The surge in the number of submissions received by leading AI conferences has challenged the sustainability of the review process because the pool of qualified reviewers is growing at a much slower rate.",
                "distance": 0.0483
            },
            {
                "reference": "Conflicts of interest (COIs) during the peer-review process often result in biased outcomes, and the current self-declared systems are both time consuming and potentially incomplete.",
                "distance": 0.054
            },
            {
                "reference": "The current peer review system employed in various computer science communities has several shortcomings, such as long review times, overloaded reviewers, and the fostering of niche topics. These flaws lower the quality and impact of studies, decelerate the innovation process, and cause frustration among authors, readers, and reviewers.",
                "distance": 0.0564
            },
            {
                "reference": "Peer reviewing is a central process in modern research and essential for ensuring the quality and reliability of published work. The increasing interest in emerging fields often results in a high review workload, particularly for senior researchers in these areas. Coping with high workload is a significant challenge and is a topic of active discussion across major conferences.",
                "distance": 0.0621
            },
            {
                "reference": "Evaluating the quality of research work is critical in the scientific process. However, managing increasing article submission rates and potential consistency and bias issues in the peer review process is challenging. Current works mainly focus on using machine learning models to predict whether a paper will be accepted, but these models often lack interpretability.",
                "distance": 0.0624
            },
            {
                "reference": "Peer review systems, such as conference paper reviews, often experience miscalibration. Current methods of calibrating peer reviews typically use ordinal information or assume simplistic reviewer scoring functions, like linear functions. Most real-world applications, such as academic conferences, still rely on manual methods for calibration.",
                "distance": 0.0629
            },
            {
                "reference": "In the conference peer review process, there exist a few key challenges: reviewers maliciously trying to review certain papers to provide positive reviews, often due to agreements with authors (quid-pro-quo); 'torpedo reviewing,' where certain reviewers intentionally attempt to reject specific papers; and the potential for reviewer de-anonymization when similarities and reviewer-assignment code are released.",
                "distance": 0.0635
            },
            {
                "reference": "Peer reviewing mechanism is crucial for assessing the quality of papers submitted to scientific conferences or journals. Typical Conference management systems (CMS) rely on paper bids entered by the reviewers and apply simple matching algorithms to assign papers. However, this process can be tedious and error-prone.",
                "distance": 0.0638
            },
            {
                "reference": "In typical conferences such as CVPR, there is often an emphasis on extensive experimental validation, and less room for open discussions and debate.",
                "distance": 0.0656
            },
            {
                "reference": "A recent report published in the Communications of the ACM outlines the existence and significant impact of collusion rings in academic peer reviewing, where a sequence of reviewers each give favorable reviews to papers authored by the subsequent reviewer in the sequence, thus creating a review cycle.",
                "distance": 0.0657
            },
            {
                "reference": "In the current process of scientific communications, papers and proposals are reviewed by multiple peers before final acceptance or rejection. The task of summarizing the reviewers' comments and making final decisions, termed as meta-review, is done manually by area/program chair/ editor. This procedure is often time-consuming, especially when dealing with large numbers of submissions",
                "distance": 0.0691
            },
            {
                "reference": "There is widespread discontent with the current machine learning publication process, with many attributing the flaws to the workflow of LaTeX to PDF to reviewers to camera ready PDF. This discontent has led to the desire for new forms of publications to increase inclusivity, accessibility, and pedagogical strength.",
                "distance": 0.0706
            },
            {
                "reference": "Most computer science conferences rely on paper bidding to assign reviewers to papers. This paper bidding process, while enabling high-quality assignments, is susceptible to manipulation by dishonest reviewers who may adversarially influence paper reviewing assignments.",
                "distance": 0.0742
            }
        ]
    },
    {
        "target": "I2R is participating in the offline speech translation track for IWSLT 2023, with a focus on translation from English audio to German text.",
        "prediction": "In end-to-end speech translation, knowledge distillation from a machine translation (MT) task is often used to improve the speech translation (ST) task. However, this allows only one way transfer of knowledge, which is deemed sub-optimal.",
        "queries": [
            4.210459,
            13.718916
        ],
        "log": [
            {
                "reference": "In end-to-end speech translation, knowledge distillation from a machine translation (MT) task is often used to improve the speech translation (ST) task. However, this allows only one way transfer of knowledge, which is deemed sub-optimal.",
                "distance": 0.0714
            },
            {
                "reference": "In the field of speech translation (ST), multitask learning has been used to improve ST performance. This typically involves a recognition decoder that generates the source language text and a translation decoder that forms translations based on this output.",
                "distance": 0.0979
            }
        ]
    },
    {
        "target": "Teaching machines to reason over texts, involving complex reasoning tasks such as compositional reasoning, knowledge retrieval, etc., has been a long-standing goal of NLP. The standard method of fine-tuning a pretrained language model (LM) on specific tasks has been demonstrated as often brittle and ineffective in recent research.",
        "prediction": "Large language models are being trained on ever-larger text corpora, often made by scraping significant portions of the internet. However, these corpora are frequently introduced with only minimal documentation. This is the case of the Colossal Clean Crawled Corpus (C4), a popular dataset used for language modeling tasks.",
        "queries": [
            -2.656057,
            14.091081
        ],
        "log": [
            {
                "reference": "Large language models are being trained on ever-larger text corpora, often made by scraping significant portions of the internet. However, these corpora are frequently introduced with only minimal documentation. This is the case of the Colossal Clean Crawled Corpus (C4), a popular dataset used for language modeling tasks.",
                "distance": 0.0661
            },
            {
                "reference": "Previous research has enabled language models to retrieve factual information from external sources as a complement to architecture scaling. However, most such models are general-purpose and rely on databases like Wikipedia or books, which may not cater to complex domains like biomedicine.",
                "distance": 0.0717
            }
        ]
    },
    {
        "target": "Prior to this work, there is lack of benchmark datasets for facilitating Natural Language Processing (NPL) research in the chemical industry.",
        "prediction": "Natural Language Processing (NLP) developers are facing increased challenges due to the growing interest in NLP applications in real-world systems, the dramatically increasing quantity of text data available for training and processing, and the expanding range of languages and tasks being researched.",
        "queries": [
            -2.983697,
            11.397224
        ],
        "log": [
            {
                "reference": "Natural Language Processing (NLP) developers are facing increased challenges due to the growing interest in NLP applications in real-world systems, the dramatically increasing quantity of text data available for training and processing, and the expanding range of languages and tasks being researched.",
                "distance": 0.0364
            },
            {
                "reference": "Natural language processing (NLP) systems have become essential in a multitude of fields, but the rapid progress experienced in NLP technologies over the last decade has been predominantly limited to a small subset of the world's 6,500 languages. ",
                "distance": 0.0408
            },
            {
                "reference": "Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that handle a variety of linguistic phenomena and nuances. Current solutions for complex problems are still far from perfect, identifying the need for systems that can quickly and incrementally learn from a minimum of training data.",
                "distance": 0.0414
            },
            {
                "reference": "Incremental learning in natural language processing encompasses techniques like projecting and integrating semantic constraints to learn word definitions, as implemented in systems such as POLITICS.",
                "distance": 0.044
            },
            {
                "reference": "Current developments in natural language processing and programming language processing have been largely independent, and there is a need for a model that can handle tasks related to both natural languages and programming languages.",
                "distance": 0.0442
            },
            {
                "reference": "Natural Language Processing has been gaining attention due to the vast amount of biological information stored in free text. A critical function most NLP systems need is to correctly identify biological entity terms and link them to respective database records, known as biological named entity tagging.",
                "distance": 0.0531
            },
            {
                "reference": "Most Natural Language Processing (NLP) datasets are manually labeled, which leads to issues such as inconsistent labeling or limited size.",
                "distance": 0.0531
            },
            {
                "reference": "Recent developments in Natural Language Processing (NLP) are heading towards knowledge rich resources and technology. Integration of linguistically sound grammars, sophisticated machine learning settings and world knowledge background is made possible due to resources like deep multilingual treebanks, and world knowledge information encoded within ontologies and Linked Open Data datasets (LOD).",
                "distance": 0.0555
            },
            {
                "reference": "The notion of 'in-domain data' in Natural Language Processing (NLP) is often vague as textual data varies in nuanced linguistic aspects such as topic, style, or level of formality. Moreover, domain labels are often not available, making it challenging to build domain-specific systems.",
                "distance": 0.0557
            },
            {
                "reference": "Undergraduate students can benefit from learning how natural language processing (NLP) relates to their understanding of formal natural language structures. However, resources that allow these students to interact with NLP visualizations and models may not be readily accessible or inclusive.",
                "distance": 0.0583
            },
            {
                "reference": "Novice writers often struggle with composition, and creating effective composition support tools requires a deep understanding of the writing process and products. Existing natural language processing (NLP) techniques face significant challenges in handling the variety of texts produced by middle school students.",
                "distance": 0.0628
            },
            {
                "reference": "Unfamiliar terminology and complex language can present barriers to understanding science. Considering this, natural language processing can help address these issues by automatically defining unfamiliar terms.",
                "distance": 0.0733
            },
            {
                "reference": "In NLP research, the increased emphasis on data quantity has made it challenging to assess the quality of data.",
                "distance": 0.0829
            },
            {
                "reference": "State-of-the-art natural language processing (NLP) models analyze the meaning and similarity of texts to perform tasks such as paraphrase identification, document retrieval, response suggestion, and extractive summarization.",
                "distance": 0.0851
            },
            {
                "reference": "The field of natural language processing (NLP) has seen significant growth, with an increase in publications and applications in a variety of products. However, the field has not realized its full potential due to certain practices.",
                "distance": 0.0871
            },
            {
                "reference": "Natural language processing (NLP) tools often exhibit a decrease in performance when applied to data that is linguistically different from the corpus used during development, posing challenges for developing NLP tools for domains lacking annotated corpora.",
                "distance": 0.0925
            },
            {
                "reference": "The Whiteboard project was initiated to integrate deep and shallow natural language processing components to take advantage of their synergies. This was done to increase robustness, guide the search space, and reduce processing time.",
                "distance": 0.094
            },
            {
                "reference": "Empirical natural language processing (NLP) systems in application domains involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation, and visualization.",
                "distance": 0.0974
            },
            {
                "reference": "There is a lack of large-scale, open, community-based language resources for natural language processing (NLP), with existing resources having data from limited genres.",
                "distance": 0.099
            }
        ]
    },
    {
        "target": "Existing learned metrics for evaluating text generation quality either perform unsatisfactorily across text generation tasks or require human ratings for training on specific tasks.",
        "prediction": "Current natural language (NL) interfaces to expert systems have a number of shortcomings, affecting the acceptability and effectiveness of expert systems.",
        "queries": [
            -4.511773,
            11.245086
        ],
        "log": [
            {
                "reference": "Current natural language (NL) interfaces to expert systems have a number of shortcomings, affecting the acceptability and effectiveness of expert systems.",
                "distance": 0.0201
            },
            {
                "reference": "Computer commands in the future office environment will often be formed from words of natural language, making them more accessible to users unaccustomed to computers. Till now, studies have primarily focused on evaluating existing languages, experimenting with variants based on linguistic or semantic variables, or researching languages designed by users themselves.",
                "distance": 0.0465
            },
            {
                "reference": "Previous research has investigated the effects of natural language on command languages for interacting with computer systems, specifically focusing on command arguments and command-argument interrelations.",
                "distance": 0.0595
            },
            {
                "reference": "Existing linguistic studies of human-computer communication have primarily focused on syntax and discourse structure, leaving the field of lexical semantics of command languages relatively unexplored.",
                "distance": 0.0768
            },
            {
                "reference": "The development of natural language interfaces (NLIs) for databases has been a challenging problem in natural language processing (NLP) since the 1970's, with the need for NLIs getting pronounced due to widespread access to complex databases online. Automatically acquiring NLIs from training examples is a particular challenge for empirical NLP.",
                "distance": 0.0974
            }
        ]
    },
    {
        "target": "In most spreadsheet platforms, users have to manually write data-dependent formatting rules, creating a frustrating and time-consuming process.",
        "prediction": "Spreadsheet users routinely read and misread others' spreadsheets, and only a high-level understanding of users' comprehension behaviors exists in the literature, which limits the ability to support these users.",
        "queries": [
            -9.432996,
            -1.941106
        ],
        "log": [
            {
                "reference": "Spreadsheet users routinely read and misread others' spreadsheets, and only a high-level understanding of users' comprehension behaviors exists in the literature, which limits the ability to support these users.",
                "distance": 0.0285
            },
            {
                "reference": "While spreadsheets allow users to blend calculations with layout and formatting, they offer limited support for reusing groups of formulae along with layout and formatting. Most users resort to the error-prone and tedious method of copy and paste.",
                "distance": 0.0422
            },
            {
                "reference": "The need for simple calculations arises often, especially when away from desktop computers. Major software vendors have optimized their spreadsheet applications for mobile environments but they are still quite heavy for such tasks.",
                "distance": 0.0631
            },
            {
                "reference": "Millions of computer end users need to perform tasks over tabular spreadsheet data, yet lack the programming knowledge to do such tasks automatically.",
                "distance": 0.0732
            },
            {
                "reference": "Spreadsheets are popular and widely used user interfaces with table manipulation, such as cell insertion and removal, typically being performed by menu commands or shortcut keys. However, the large number of shortcut keys can make them difficult to learn and remember.",
                "distance": 0.0761
            },
            {
                "reference": "Conventional spreadsheet programs offer a very convenient user interface for many quantitative tasks, but they are restricted to handling precisely-specified quantities and calculations.",
                "distance": 0.0763
            },
            {
                "reference": "Spreadsheet use and experience with textual programming languages is an area of interest, but there is a need for further study on user experience.",
                "distance": 0.0788
            },
            {
                "reference": "As the volumes of data continue to rise, manual inspection of this data is becoming increasingly difficult and is not feasible.",
                "distance": 0.0806
            },
            {
                "reference": "The NoPumpG prototype suggested that the spreadsheet model of computation could simplify the creation of interactive graphical application when compared with other computational models.",
                "distance": 0.0824
            },
            {
                "reference": "Current programming environments for end-users often lack interactivity and visual feedback in logic programming, and conventional spreadsheet programs are limited in their scope and functionality.",
                "distance": 0.0857
            },
            {
                "reference": "Spreadsheet systems have impacted the world's perception and interaction with computers, however, they come with limitations which need to be addressed.",
                "distance": 0.0975
            }
        ]
    }
]